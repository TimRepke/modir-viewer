{"docs":{"023db6fa5c87852885dcee9531a1c843063c2347":{"id":"023db6fa5c87852885dcee9531a1c843063c2347","date":"2004-01-01T12:12:00Z","text":"Many algorithms have been proposed to approximate holistic aggregates, such as quantiles and heavy hitters, over data streams. However, little work has been done to explore what techniques are required to incorporate these algorithms in a data stream query processor, and to make them useful in practice.In this paper, we study the performance implications of using user-defined aggregate functions (UDAFs) to incorporate selection-based and sketch-based algorithms for holistic aggregates into a data stream management system's query processing architecture. We identify key performance bottlenecks and tradeoffs, and propose novel techniques to make these holistic UDAFs fast and space-efficient for use in high-speed data stream applications. We evaluate performance using generated and actual IP packet data, focusing on approximating quantiles and heavy hitters. The best of our current implementations can process streaming queries at OC48 speeds (2x 2.4Gbps).","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","Approximation algorithm","Database","Holism","Network packet","Optical Carrier transmission rates"],"vec":[0.9768318466,0.0886391683],"nodes":["1709589","1685257","2096611","1711192","1704170","1704011"]},"1007e2bfb377757a75f51a0edaf745edeabf3757":{"id":"1007e2bfb377757a75f51a0edaf745edeabf3757","date":"2001-01-01T12:12:00Z","text":"This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the Penn Treebank with the Treebank\u2019s own CFG grammar. We show how performance is dramatically affected by rule representation and tree transformations, but little by top-down vs. bottom-up strategies. We discuss grammatical saturation, including analysis of the strongly connected components of the phrasal nonterminals in the Treebank, and model how, as sentence length increases, the effective grammar rule size increases as regions of the grammar are unlocked, yielding super-cubic observed time behavior in some configurations.","category_a":"ACL","category_b":"ComLing","keywords":["Chart parser","Context-free grammar","Cubic function","Parsing","Strongly connected component","Top-down and bottom-up design","Treebank"],"vec":[-0.3592027464,-0.5036065403],"nodes":["38666915","1812612"]},"35254a19bac666c291646e3640014bf0bb2ce11b":{"id":"35254a19bac666c291646e3640014bf0bb2ce11b","date":"2014-01-01T12:12:00Z","text":"Neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models, a recently-developed class of neural language model. We show that embeddings from translation models outperform those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexical-syntactic role. We further show that these effects hold when translating from both English to French and English to German, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. Finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degradation of embedding quality. Our embedding spaces can be queried in an online demo and downloaded from our web page. Overall, our analyses indicate that translation-based embeddings should be used in applications that require concepts to be organised according to similarity and\/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Download","Language model","Lexical function","Machine translation","Mathematical model","Neural machine translation","Vocabulary","Web page"],"vec":[-1.1772968952,0.1349992587],"nodes":["38909889","1979489","34564969","16284940","1751762"]},"225721565af848f8360a8b01f7cc6d28dcc8e240":{"id":"225721565af848f8360a8b01f7cc6d28dcc8e240","date":"2003-01-01T12:12:00Z","text":"Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors. This paper provides a unified framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering. This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel. Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Cluster analysis","Data dependency","Dimensionality reduction","Experiment","Isomap","Machine learning","Multidimensional scaling","Nonlinear dimensionality reduction","Numerical analysis","Spatial variability","Spectral clustering","Test set","Unified Framework","Unsupervised learning"],"vec":[-0.3080069609,0.4997345625],"nodes":["1751762","39651651","1724875","2460212","7245737","2120888"]},"69a80e4c13f94be768cd113d11890e62ec4f1596":{"id":"69a80e4c13f94be768cd113d11890e62ec4f1596","date":"2011-01-01T12:12:00Z","text":"This paper describes two applications of conditional restricted Boltzmann machines (CRBMs) to the task of autotagging music. The first consists of training a CRBM to predict tags that a user would apply to a clip of a song based on tags already applied by other users. By learning the relationships between tags, this model is able to pre-process training data to significantly improve the performance of a support vector machine (SVM) autotagging. The second is the use of a discriminative RBM, a type of CRBM, to autotag music. By simultaneously exploiting the relationships among tags and between tags and audio-based features, this model is able to significantly outperform SVMs, logistic regression, and multi-layer perceptrons. In order to be applied to this problem, the discriminative RBM was generalized to the multi-label setting and four different learning algorithms for it were evaluated, the first such in-depth analysis of which we are aware.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Layer (electronics)","Logistic regression","Machine learning","Multi-label classification","Multilayer perceptron","Perceptron","Preprocessor","Restricted Boltzmann machine","Support vector machine","Test set"],"vec":[-0.1657121549,0.2540420902],"nodes":["2035708","1996134","1777528","1751762"]},"46cd50a49397e37e5d83cb536f4b92a40293a467":{"id":"46cd50a49397e37e5d83cb536f4b92a40293a467","date":"2005-01-01T12:12:00Z","text":"The quality of the data residing in information repositories and databases gets degraded due to a multitude of reasons. Such reasons include typing mistakes during insertion (e.g., character transpositions), lack of standards for recording database fields (e.g., addresses), and various errors introduced by poor database design (e.g., missing integrity constraints). Data of poor quality can result in significant impediments to popular business practices: sending products or bills to incorrect addresses, inability to locate customer records during service calls, inability to correlate customers across multiple services, etc. In the presence of data quality errors, a problem central in this context is the ability to identify whether two entities (e.g., relational tuples) are approximately the same. Depending on the type of data under consideration, various \u201csimilarity metrics\u201d (approximate match predicates) have been defined to quantify the closeness of a pair of data entities in a way that common mistakes are captured. A key operation in this context is, given two large multi-attribute data sets, identify all pairs of entities (tuples) in the two sets that are approximately the same. This operation has been well studied through the years and it is known under various names, including record linkage, entity identification, entity reconciliation and approximate join, to name a few. Given the significance and the inherent difficulty of the approximate join problem, a plethora of techniques have been developed in various communities, including the statistics, pattern matching, and the database communities, deploying diverse approximate match predicates. The objective of this tutorial is to provide a comprehensive and cohesive overview of the key research results, techniques, and tools used for approximate joins. It complements recent data quality tutorials that present broad overviews of various aspects of data quality, and don\u2019t delve into the details of approximate join technology [2, 1].","category_a":"VLDB","category_b":"DB","keywords":["Approximation algorithm","Centrality","Chemical similarity","Data integrity","Data quality","Database","Database design","Entity","Insertion sort","Linkage (software)","Pattern matching","Typing"],"vec":[1.0074238054,0.107610091],"nodes":["1721062","1704011"]},"219309d6413a29656178f99186157ceeb0932020":{"id":"219309d6413a29656178f99186157ceeb0932020","date":"2007-01-01T12:12:00Z","text":"We investigate the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. This problem is important in many applications, such as data warehousing, mobile computing, global information systems, and maintaining physical data independence, where access to local or cached materialized views may be cheaper than access to the underlying database. In addition, this problem has obvious potential in optimizing query evaluation. The problem is formally stated as nding a rewriting of an SQL query Q where the materialized views occur in the FROM clause, and the rewritten query is multiset-equivalent to Q. First, we study the case where the query has grouping and aggregation but the views do not, and show that usability of a view in evaluating a query essentially requires an isomorphism between the view and a portion of the query. We present a rewriting algorithm that generates all possible rewritings of the query using the views (for the case of equality predicates); when using multiple views, considering the views iteratively in any order yields all possible rewritings. Second, we study the case where the query and the views both have grouping and aggregation, identify the conditions under which the aggregation information present in a view is suucient to perform the aggregate computations required in the query, and present a rewriting algorithm. Third, we outline how our techniques can be extended to take advantage of set-valued queries and views in the presence of keys or SELECT DISTINCT.","category_a":"","category_b":"Others","keywords":["Algorithm","Computation","Global information system","Information Systems","Information system","Materialized view","Mobile computing","Outline (list)","Question (inquiry)","Reasoning","Rewriting","SQL","Select (SQL)","Single-access key","Structured Query Language","Usability","algorithm"],"vec":[0.9920389136,0.0770798656],"nodes":["35612310","1735239","38326512","1704011"]},"146b3649b693ae591c8953c0aae5264512c26ea3":{"id":"146b3649b693ae591c8953c0aae5264512c26ea3","date":"2006-01-01T12:12:00Z","text":"The end-to-end performance of natural language processing systems for compound tasks, such as question answering and textual entailment, is often hampered by use of a greedy 1-best pipeline architecture, which causes errors to propagate and compound at each stage. We present a novel architecture, which models these pipelines as Bayesian networks, with each low level task corresponding to a variable in the network, and then we perform approximate inference to find the best labeling. Our approach is extremely simple to apply but gains the benefits of sampling the entire distribution over labels at each stage in the pipeline. We apply our method to two tasks \u2013 semantic role labeling and recognizing textual entailment \u2013 and achieve useful performance gains from the superior pipeline architecture.","category_a":"EMNLP","category_b":"ComLing","keywords":["Approximation algorithm","Bayesian network","End-to-end principle","Greedy algorithm","High- and low-level","Natural language processing","Pipeline (computing)","Pipelines","Question answering","Sampling (signal processing)","Semantic role labeling","Textual entailment"],"vec":[-0.2610093527,-0.1112519529],"nodes":["2784228","1812612","1701538"]},"2606e6a5759c030e259ebf3f4261b9c04a36a609":{"id":"2606e6a5759c030e259ebf3f4261b9c04a36a609","date":"2015-01-01T12:12:00Z","text":"Semantically complex queries which include attributes of objects and relations between objects still pose a major challenge to image retrieval systems. Recent work in computer vision has shown that a graph-based semantic representation called a scene graph is an effective representation for very detailed image descriptions and for complex queries for retrieval. In this paper, we show that scene graphs can be effectively created automatically from a natural language scene description. We present a rule-based and a classifierbased scene graph parser whose output can be used for image retrieval. We show that including relations and attributes in the query graph outperforms a model that only considers objects and that using the output of our parsers is almost as effective as using human-constructed scene graphs (Recall@10 of 27.1% vs. 33.4%). Additionally, we demonstrate the general usefulness of parsing to scene graphs by showing that the output can also be used to generate 3D scenes.","category_a":"EMNLP","category_b":"ComLing","keywords":["Computer vision","Image retrieval","Logic programming","Natural language","Parsing","Scene graph","Utility"],"vec":[-0.263266295,-0.5790260693],"nodes":["31916999","2580593","3317599","3216322","1812612"]},"0d060f1edd7e79865f1a48a9c874439c4b10caea":{"id":"0d060f1edd7e79865f1a48a9c874439c4b10caea","date":"2015-01-01T12:12:00Z","text":"We describe our first-place solution to the ECML\/PKDD discovery challenge on taxi destination prediction. The task consisted in predicting the destination of a taxi based on the beginning of its trajectory, represented as a variable-length sequence of GPS points, and diverse associated meta-information, such as the departure time, the driver id and client information. Contrary to most published competitor approaches, we used an almost fully automated approach based on neural networks and we ranked first out of 381 teams. The architectures we tried use multi-layer perceptrons, bidirectional recurrent neural networks and models inspired from recently introduced memory networks. Our approach could easily be adapted to other applications in which the goal is to predict a fixed-length output from a variable-length sequence. Random order, this does not reflect the weights of contributions. ar X iv :1 50 8. 00 02 1v 1 [ cs .L G ] 3 1 Ju l 2 01 5 2 Artificial Neural Networks Applied to Taxi Destination Prediction","category_a":"KDD","category_b":"AI","keywords":["Artificial neural network","Bidirectional recurrent neural networks","ECML PKDD","GPS navigation device","Layer (electronics)","Multilayer perceptron","Neural Networks","Perceptron","Recurrent neural network"],"vec":[-0.3537043501,0.3075958789],"nodes":["2346028","39442397","3082019","1724875","1751762"]},"29cb27e32d56f39b9fa5c5bf62225580530a014c":{"id":"29cb27e32d56f39b9fa5c5bf62225580530a014c","date":"2017-01-01T12:12:00Z","text":"We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-tophoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-tospeech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations. \u2020Authors are listed alphabetically by last name. Submitted February 24, 2017 for review for the International Conference on Machine Learning (ICML) 2017. Copyright 2017 by the author(s).","category_a":"ICML","category_b":"ML","keywords":["Artificial neural network","Connectionism","Deep learning","End-to-end principle","Feature engineering","Graphics processing unit","International Conference on Machine Learning","Machine learning","Speech synthesis"],"vec":[-0.4880650269,-0.1809217753],"nodes":["2676352","35977287","5574038","2040049","9544702","1729563","1692084","28752902","1701538","34042420","2264597","1911755"]},"7400fe80e9e5670fd3e9de000a4afb2e6cc034fc":{"id":"7400fe80e9e5670fd3e9de000a4afb2e6cc034fc","date":"2000-01-01T12:12:00Z","text":"Now there is tremendous interest in data warehousing and OLAP applications. OLAP applications typically view data as having multiple logical dimensions (e.g., product, location) with natural hierarchies de ned on each dimension, and analyze the behavior of various measure attributes (e.g., sales, volume) in terms of the dimensions. OLAP queries typically involve hierarchical selections on some of the dimensions (e.g., product is classi ed under the jeans product category, or location is in the north-east region), often aggregating measure attributes (see, e.g., [6]). Cost-based query optimization of such OLAP queries needs good estimates of the selectivity of hierarchical selections. Histograms capture attribute value distribution statistics in a space-e cient fashion. They have been designed to work well for numeric attribute value domains, and have long been used to support cost-based query optimization in databases [11, 9, 2, 4, 10, 5]. Histograms can be used to estimate the selectivity of OLAP queries by modeling the (hierarchical) conditions on a given dimension as a set of hierarchical ranges (i.e., two ranges are either disjoint or one is contained in the other), and using standard range selectivity estimation techniques (see, e.g., [10]). The quality of selectivity estimates obtained using a histogram depends on computing a good solution to the histogram construction problem, and there has been considerable recent e ort in this area (see, e.g., [10, 5]). However, while OLAP queries make extensive use of hierarchical selection conditions, previous works on computing good histograms, for the most part, consider only equality queries when computing the error incurred by a particular choice of histogram bucket boundaries. This mismatch between the nature of OLAP queries, and the class of queries considered when constructing histograms can result in poor selectivity estimates for OLAP queries. In this paper, we address this problem and focus on e ciently computing optimal histograms for the case of hierarchical range queries. We make the following contributions:","category_a":"","category_b":"Others","keywords":["Attribute\u2013value pair","Database","Histogram","Online analytical processing","Open road tolling","Program optimization","Published Database","Query optimization","Range query (data structures)","Scientific Queries Study Contact","Selectivity (electronic)"],"vec":[1.0602729964,0.1822061756],"nodes":["1721062","1711192","1704011"]},"209ba9f612f34583a23b75c0e8dadc410c400bb2":{"id":"209ba9f612f34583a23b75c0e8dadc410c400bb2","date":"2013-01-01T12:12:00Z","text":"Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.","category_a":"ACL","category_b":"ComLing","keywords":["Artificial neural network","F1 score","Natural language","Neural coding","Parsing","Recursive neural network","Stochastic context-free grammar"],"vec":[-0.5123401467,-0.4809193276],"nodes":["2166511","34178598","1812612","1701538"]},"06d51deada6e771a2571807a47dd991120c8dd1a":{"id":"06d51deada6e771a2571807a47dd991120c8dd1a","date":"2014-01-01T12:12:00Z","text":"Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder\u2013Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.","category_a":"EMNLP","category_b":"ComLing","keywords":["Artificial neural network","Binary decoder","Convolutional neural network","Encoder","Machine translation","Neural machine translation","Recursion","Statistical machine translation","The Sentence"],"vec":[-1.0303140708,0.1002994034],"nodes":["1979489","3158246","3335364","1751762"]},"1e1e274d9dde08a5e6a02daf86405d1cee5ec5cd":{"id":"1e1e274d9dde08a5e6a02daf86405d1cee5ec5cd","date":"2013-01-01T12:12:00Z","text":"Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models \u201cforget\u201d how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm\u2013 the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.","category_a":"ArXiv","category_b":"Journal","keywords":["Activation function","Algorithm","Artificial neural network","Catastrophic interference","Dropout (neural networks)","Gradient","Machine learning","Neural Networks"],"vec":[-0.3911468968,0.3304333278],"nodes":["34740554","1778734","3324256","1760871","1751762"]},"541c68e2c65f6dce6179801c9f92dc7803dc71b5":{"id":"541c68e2c65f6dce6179801c9f92dc7803dc71b5","date":"2013-01-01T12:12:00Z","text":"Classifying scenes (e.g. into \u201cstreet\u201d, \u201chome\u201d or \u201cleisure\u201d) is an important but complicated task nowadays, because images come with variability, ambiguity, and a wide range of illumination or scale conditions. Standard approaches build an intermediate representation of the global image and learn classifiers on it. Recently, it has been proposed to depict an image as an aggregation of its contained objects:the representation on which classifiers are trained is composed of many heterogeneous feature vectors derived from various object detectors. In this paper, we propose to study different approaches to efficiently combine the data extracted by these detectors. We use the features provided by Object-Bank (Li-Jia Li and Fei-Fei, 2010a) (177 different object detectors producing 252 attributes each), and show on several benchmarks for scene categorization that careful combinations, taking into account the structure of the data, allows to greatly improve over original results (from +5% to +11%) while drastically reducing the dimensionality of the representation by 97% (from","category_a":"ICPRAM","category_b":"Others","keywords":["Benchmark (computing)","Categorization","Feature vector","Intermediate representation","Sensor","Spatial variability","Unsupervised learning"],"vec":[-0.698797409,0.1569273902],"nodes":["1935910","2425018","1713934","3119801","1751762","1724875"]},"18cc17c06e34baaa3e196db07e20facdbb17026d":{"id":"18cc17c06e34baaa3e196db07e20facdbb17026d","date":"2015-01-01T12:12:00Z","text":"Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description model. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.","category_a":"ICCV","category_b":"Others","keywords":["Artificial neural network","Audio description","BLEU","Convolutional neural network","Kinesiology","Meteor","Natural language","Pattern recognition","Recurrent neural network"],"vec":[-0.3688597883,-0.0909437236],"nodes":["1685369","1730844","1979489","2482072","1972076","1777528","1760871"]},"2dde68fc4b1af7f21e18d7b9da5f964539f9b515":{"id":"2dde68fc4b1af7f21e18d7b9da5f964539f9b515","date":"2005-01-01T12:12:00Z","text":"Part-of-speech tagging, like any supervised statistical NLP task, is more difficult when test sets are very different from training sets, for example when tagging across genres or language varieties. We examined the problem of POS tagging of different varieties of Mandarin Chinese (PRC-Mainland, PRCHong Kong, and Taiwan). An analytic study first showed that unknown words were a major source of difficulty in cross-variety tagging. Unknown words in English tend to be proper nouns. By contrast, we found that Mandarin unknown words were mostly common nouns and verbs. We showed these results are caused by the high frequency of morphological compounding in Mandarin; in this sense Mandarin is more like German than English. Based on this analysis, we propose a variety of new morphological unknown-word features for POS tagging, extending earlier work by others on unknown-word tagging in English and German. Our features were implemented in a maximum entropy Markov model. Our system achieves state-of-the-art performance in Mandarin tagging, including improving unknown-word tagging performance on unseen varieties in Chinese Treebank 5.0 from 61% to 80% correct.","category_a":"CNLP","category_b":"Others","keywords":["Markov model","Maximum-entropy Markov model","Natural language processing","Part-of-speech tagging","Point of sale","Super Robot Monkey Team Hyperforce Go!","Tag (metadata)","Treebank"],"vec":[-0.671216533,-0.5042574447],"nodes":["39034442","1746807","1812612"]},"5dc1fd136278c61771fbc43473c9e21638f0f46f":{"id":"5dc1fd136278c61771fbc43473c9e21638f0f46f","date":"2015-01-01T12:12:00Z","text":"The task of the Emotion Recognition in the Wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based \u201cbag-of-mouths\u201d model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67\u00a0% on the 2014 dataset.","category_a":"Multimodal User Interfaces","category_b":"Others","keywords":["Artificial neural network","Autoencoder","Convolutional neural network","Deep belief network","Deep learning","Emotion recognition","Hollywood","Illumination (image)","K-means clustering","Modality (human\u2013computer interaction)","Multimodal interaction","Restricted Boltzmann machine","Statistical classification","Streaming media","Test set","Video clip"],"vec":[-0.3907308471,0.3519153702],"nodes":["3127597","2900675","3087941","1854385","1748421","35254095","34564969","2558801","2921469","2488222","39769659","1778734","1923596","1760871","1724875","1710604","1972076","1751762"]},"5aee809e16ec6d065e828fa04968b2d189c29e71":{"id":"5aee809e16ec6d065e828fa04968b2d189c29e71","date":"2011-01-01T12:12:00Z","text":"We introduce a new nonparametric clustering model which combines the recently proposed distance-dependent Chinese restaurant process (dd-CRP) and non-linear, spectral methods for dimensionality reduction. Our model retains the ability of nonparametric methods to learn the number of clusters from data. At the same time it addresses two key limitations of nonparametric Bayesian methods: modeling data that are not exchangeable and have many correlated features. Spectral methods use the similarity between documents to map them into a low-dimensional spectral space where we then compare several clustering methods. Our experiments on handwritten digits and text documents show that nonparametric methods such as the CRP or dd-CRP can perform as well as or better than k-means and also recover the true number of clusters. We improve the performance of the dd-CRP in spectral space by incorporating the original similarity matrix in its prior. This simple modification results in better performance than all other methods we compared to. We offer a new formulation and first experimental evaluation of a general Gibbs sampler for mixture modeling with distance-dependent CRPs.","category_a":"AISTATS","category_b":"ML","keywords":["Bayesian network","Cluster analysis","Dimensionality reduction","Experiment","Gibbs sampling","K-means clustering","Mixture model","Nonlinear system","Sampling (signal processing)","Similarity measure","Spectral method"],"vec":[0.1286580482,0.4042727022],"nodes":["2166511","34961461","1812612"]},"013613f6f3da2094a2fb590d8608ad0b44798baf":{"id":"013613f6f3da2094a2fb590d8608ad0b44798baf","date":"2008-01-01T12:12:00Z","text":"In case of engine failure, skilled pilots can save a helicopter from crashing by executing an emergency procedure known as autorotation. In autorotation, rather than relying on the engine to drive the main rotor, the pilot has to control the helicopter such that potential energy from altitude is transferred to rotor speed. In fact, maintaining a sufficiently high rotor speed is critical to retain sufficient control of the helicopter to land safely. In this paper, we present the first autonomous controller to successfully pilot a remotely controlled (RC) helicopter during an autorotation descent and landing.","category_a":"ISER","category_b":"Others","keywords":["Autonomous car","Remote control"],"vec":[-0.0439656048,-1.1227677037],"nodes":["1689992","5574038","38590642","1701538"]},"0b544dfe355a5070b60986319a3f51fb45d1348e":{"id":"0b544dfe355a5070b60986319a3f51fb45d1348e","date":"2014-01-01T12:12:00Z","text":"In this paper, we propose a novel neural network model called RNN Encoder\u2013 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2013Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.","category_a":"EMNLP","category_b":"ComLing","keywords":["Artificial neural network","Encoder","Linear model","Log-linear model","Machine translation","Network model","Recurrent neural network","Statistical machine translation","String (computer science)"],"vec":[-1.0557692532,0.1646986475],"nodes":["1979489","3158246","1854385","3335364","2076086","1680886","1751762"]},"1979271767a78ec766638b83efcc1fc8e96cc4a2":{"id":"1979271767a78ec766638b83efcc1fc8e96cc4a2","date":"2012-01-01T12:12:00Z","text":"Large-Scale Spike-and-Slab Sparse Coding for Unsupervised Feature Discovery Ian J. Goodfellow, Aaron Courvile, Yoshua Bengio. Dept. IRO, U. Montreal We consider the problem of using a factor model we call spike-and-slab sparse coding (S3C) to learn features for a classification task. The S3C model resembles both the spike-and-slab RBM and sparse coding. Since exact inference in this model is intractable, we derive a structured variational inference procedure and employ a variational EM training algorithm. Prior work on approximate inference for this model has not prioritized the ability to exploit parallel architectures and scale to enormous problem sizes. We present an inference procedure appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors.","category_a":"","category_b":"Others","keywords":["Algorithm","Approximation algorithm","Architecture as Topic","Calculus of variations","Electron Microscopy","Euphorbia aaron-rossii","Factor analysis","Graphics processing unit","Inference","Latent variable","Machine learning","Neural coding","Restricted Boltzmann machine","Slab allocation","Sparse matrix","The Spike (1997)","algorithm"],"vec":[-0.4742638443,0.5457982519],"nodes":["34740554",0,"1751762"]},"013cd20c0eaffb9cab80875a43086e0c3224fe20":{"id":"013cd20c0eaffb9cab80875a43086e0c3224fe20","date":"2013-01-01T12:12:00Z","text":"The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.","category_a":"Pattern Analysis and Machine Intelligence","category_b":"Others","keywords":["Algorithm","Data (computing)","Deep learning","Feature learning","Inference","Machine learning","Models, Statistical","Nonlinear dimensionality reduction","Unsupervised learning","algorithm","explanation","manifold"],"vec":[-0.4380426817,0.3597283271],"nodes":["1751762","1760871","1724875"]},"60744af2f89291897839852d66582b1b2a0be0bc":{"id":"60744af2f89291897839852d66582b1b2a0be0bc","date":"2017-01-01T12:12:00Z","text":"We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. In contrast to the standard loglikelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. We apply the proposed model to the task of dialogue response generation in two challenging domains: the Ubuntu technical support domain, and Twitter conversations. On Ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. On Twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. Finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure. * This work was carried out while the first author was at IBM Research. \u25e6 Email: {iulian.vlad.serban,yoshua.bengio,aaron.courville}@umontreal.ca Email: {tklinger,gtesauro,krtalamad,zhou}@us.ibm.com \u2020 CIFAR Senior Fellow ar X iv :1 60 6. 00 77 6v 1 [ cs .C L ] 2 J un 2 01 6","category_a":"AAAI","category_b":"AI","keywords":["Artificial neural network","Email","Evaluation of machine translation","Experiment","High- and low-level","IBM Research","Natural language","Natural language generation","Off topic","Perplexity","Recurrent neural network","Sparse matrix","Stochastic process","Technical support","Ubuntu"],"vec":[-0.3268544702,0.083572427],"nodes":["35224828","2652016","1699108","2940762","36631371","1751762","1760871"]},"29360559cbb9d8d4012c94262f3a79ea259abe2f":{"id":"29360559cbb9d8d4012c94262f3a79ea259abe2f","date":"2005-01-01T12:12:00Z","text":"We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with metadata features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.","category_a":"VLDB","category_b":"DB","keywords":["Database","MIX","XML","XPath"],"vec":[1.1247506118,0.0593068544],"nodes":["2690683","1721062","1704011"]},"43b39679a5a37585e31260fc426120f4fe00538d":{"id":"43b39679a5a37585e31260fc426120f4fe00538d","date":"2007-01-01T12:12:00Z","text":"Logical constraints, (e.g., `phone numbers in Toronto can have prefixes 416, 647, 905 only'), are ubiquitous in relational databases. Traditional integrity constraints, such as functional dependencies, are examples of such logical constraints as well. However, under frequent database updates, schema evolution and transformations, they can be easily violated. As a result, tables become inconsistent and data quality is degraded. In this paper we study the problem of validating collections of user defined constraints on a number of relational tables. Our primary goal is to quickly identify which tables violate such constraints. Logical constraints are potentially complex logical formuli, and we demonstrate that they cannot be efficiently evaluated by SQL queries. In order to enable fast identification of constraint violations, we propose to build and maintain specialized logical indices on the relational tables. We choose Boolean Decision Diagrams (BDD) as the index structure to aid in this task. We first propose efficient algorithms to construct and maintain such indices in a space efficient manner. We then describe a set of query re-write rules that aid in the efficient utilization of logical indices during constraint validation. We have implemented our approach on top of a relational database and tested our techniques using large collections of real and synthetic data sets. Our results indicate that utilizing our techniques in conjunction with logical indices during constraint validation offers very significant performance advantages.","category_a":"Data Engineering","category_b":"DB","keywords":["Algorithm","Data integrity","Data quality","Database","Functional dependency","Relational database","SQL","Schema evolution","Software verification and validation","Synthetic data","Telephone number"],"vec":[1.1043794612,0.0791924463],"nodes":["36206298","1721062","34777495","1704011"]},"2ae139b247057c02cda352f6661f46f7feb38e45":{"id":"2ae139b247057c02cda352f6661f46f7feb38e45","date":"2013-01-01T12:12:00Z","text":"In this paper we present the techniques used for the University of Montr&#233;al's team submissions to the 2013 Emotion Recognition in the Wild Challenge. The challenge is to classify the emotions expressed by the primary human subject in short video clips extracted from feature length movies. This involves the analysis of video clips of acted scenes lasting approximately one-two seconds, including the audio track which may contain human voices as well as background music. Our approach combines multiple deep neural networks for different data modalities, including: (1) a deep convolutional neural network for the analysis of facial expressions within video frames; (2) a deep belief net to capture audio information; (3) a deep autoencoder to model the spatio-temporal information produced by the human actions depicted within the entire scene; and (4) a shallow network architecture focused on extracted features of the mouth of the primary human subject in the scene. We discuss each of these techniques, their performance characteristics and different strategies to aggregate their predictions. Our best single model was a convolutional neural network trained to predict emotions from static frames using two large data sets, the Toronto Face Database and our own set of faces images harvested from Google image search, followed by a per frame aggregation strategy that used the challenge training data. This yielded a test set accuracy of 35.58%. Using our best strategy for aggregating our top performing models into a single predictor we were able to produce an accuracy of 41.03% on the challenge test set. These compare favorably to the challenge baseline test set accuracy of 27.56%.","category_a":"ICMI","category_b":"Others","keywords":["Artificial neural network","Autoencoder","Baseline (configuration management)","Branch predictor","Convolutional neural network","Deep belief network","Deep learning","Emotion recognition","Image retrieval","Modality (human\u2013computer interaction)","Network architecture","Restricted Boltzmann machine","Test set","Video clip"],"vec":[-0.2710279391,-0.0716177367],"nodes":["3127597","1972076","2900675","2558801","1854385","1710604","1724875","1760871","1751762","39769659","1778734","34564969","8883973","2921469","2488222","1911213","35088087","3087941","40329273","2755582","1996134","1923596","1730844","34751361","2416433","35254095","2839319"]},"524b24a3523123785bedccfa0ef6c4857bf21b5f":{"id":"524b24a3523123785bedccfa0ef6c4857bf21b5f","date":"2011-01-01T12:12:00Z","text":"The spike and slab Restricted Boltzmann Machine (RBM) is defined by having both a real valued \u201cslab\u201d variable and a binary \u201cspike\u201d variable associated with each unit in the hidden layer. In this paper we generalize and extend the spike and slab RBM to include non-zero means of the conditional distribution over the observed variables given the binary spike variables. We also introduce a term, quadratic in the observed data that we exploit to guarantee that all conditionals associated with the model are well defined \u2013 a guarantee that was absent in the original spike and slab RBM. The inclusion of these generalizations improves the performance of the spike and slab RBM as a feature learner and achieves competitive performance on the CIFAR-10 image classification task. The spike and slab model, when trained in a convolutional configuration, can generate sensible samples that demonstrate that the model has captured the broad statistical structure of natural images.","category_a":"ICML","category_b":"ML","keywords":["Action potential","Boltzmann machine","Computer vision","Restricted Boltzmann machine","Slab allocation","The Spike (1997)"],"vec":[-0.4062475741,0.4293763806],"nodes":["1760871","32837403","1751762"]},"4cc6eb1716cabda5a55a8ed103f8e503717ac434":{"id":"4cc6eb1716cabda5a55a8ed103f8e503717ac434","date":"2007-01-01T12:12:00Z","text":"SPIDER, developed at AT&amp;T Labs-Research, is a system that efficiently supports flexible string matching against attribute values in large databases, and is extensively used in AT&amp;T. The scoring methodology is based on tf.idf weighting and cosine similarity, and SPIDER maintains indexes containing string tokens and their weights, for fast matching at query time. Given the \"global\" nature of the weights maintained in the indexes, even a few updates to the underlying database tables would necessitate a (near-complete recomputation of the indexes, which can be prohibitively expensive. In this paper, we explore novel techniques to considerably reduce the cost of propagating updates in SPIDER, without a significant degradation of answer accuracy or query performance. We present experimental evidence using real data sets to demonstrate the practical benefits of our techniques.","category_a":"Data Engineering","category_b":"DB","keywords":["Cosine similarity","Database","String searching algorithm","Table (database)","Tf\u2013idf"],"vec":[1.0094680141,0.1014954949],"nodes":["1721062","2440762","1704011"]},"0851f0dad6de06dab86c82feb34dfa5b807129dc":{"id":"0851f0dad6de06dab86c82feb34dfa5b807129dc","date":"2015-01-01T12:12:00Z","text":"In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GFRNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.","category_a":"ICML","category_b":"ML","keywords":["Artificial neural network","Focus stacking","Grammatical Framework","Interaction","Language model","Long short-term memory","Neural Networks","Python","Recurrent neural network","Top-down and bottom-up design"],"vec":[-0.5340938676,0.5263935261],"nodes":["8270717","1854385","1979489","1751762"]},"78d0ed0106446bfe78851e0ab3e7dd50e586c951":{"id":"78d0ed0106446bfe78851e0ab3e7dd50e586c951","date":"2018-01-01T12:12:00Z","text":"Neural machine translation (NMT) has been a new paradigm in machine translation, and the attention mechanism has become the dominant approach with the state-of-the-art records in many language pairs. While there are variants of the attention mechanism, all of them use only temporal attention where one scalar value is assigned to one context vector corresponding to a source word. In this paper, we propose a fine-grained (or 2D) attention mechanism where each dimension of a context vector will receive a separate attention score. In experiments with the task of En-De and En-Fi translation, the fine-grained attention method improves the translation quality in terms of BLEU score. In addition, our alignment analysis reveals how the fine-grained attention mechanism exploits the internal structure of context vectors.","category_a":"Neurocomputing","category_b":"Others","keywords":["BLEU","Experiment","Machine translation","Neural machine translation","Programming paradigm"],"vec":[-0.6891000578,-0.2459602654],"nodes":["40663606","1979489","1751762"]},"6a0e1ce1ea01eb5eccf7c852a26f0e3db85e856e":{"id":"6a0e1ce1ea01eb5eccf7c852a26f0e3db85e856e","date":"2012-01-01T12:12:00Z","text":"Abstract We consider the problem of using a factor model we call spike-and-slab sparse coding (S3C) to learn features for a classification task. The S3C model resembles both the spike-and-slab RBM and sparse coding. Since exact inference in this model is intractable, we derive a structured variational inference procedure and employ a variational EM training algorithm. Prior work on approximate inference for this model has not prioritized the ability to exploit parallel architectures and scale to enormous problem sizes. We present an inference procedure appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the ssRBM on the CIFAR-10 dataset. We evaluate our approach\u2019s potential for semi-supervised learning on subsets of CIFAR-10. We use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models\u2019 Transfer Learning Challenge.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Approximation algorithm","Calculus of variations","Graphics processing unit","Latent variable","Machine learning","NIPS","Neural coding","Restricted Boltzmann machine","Semi-supervised learning","Slab allocation","Sparse matrix","Supervised learning","The Spike (1997)"],"vec":[-0.4941619759,0.5695096532],"nodes":["34740554","1760871","1751762"]},"053912e76e50c9f923a1fc1c173f1365776060cc":{"id":"053912e76e50c9f923a1fc1c173f1365776060cc","date":"2011-01-01T12:12:00Z","text":"The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between LBFGS\/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-theart result on MNIST among algorithms that do not use distortions or pretraining.","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Artificial neural network","Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm","Computer cluster","Conjugate gradient method","Convolutional neural network","Debugging","Deep learning","Distortion","Experiment","Gradient","Gradient descent","Graphics processing unit","Limited-memory BFGS","Line search","MNIST database","Machine learning","Mathematical optimization","Matrix regularization","Network model","Program optimization","Sparse matrix","Stochastic gradient descent"],"vec":[-0.3923395893,0.1445094174],"nodes":["2827616","2020608","5574038","2957684","2538820","1701538"]},"3bff03b7b0b0c4e8f6384dbb2a95e4338d156524":{"id":"3bff03b7b0b0c4e8f6384dbb2a95e4338d156524","date":"2011-01-01T12:12:00Z","text":"We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model\u2019s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.","category_a":"EMNLP","category_b":"ComLing","keywords":["Algorithm","Autoencoder","Baseline (configuration management)","Lexicon","Machine learning","Multinomial logistic regression","Recursion","Sentiment analysis","User story"],"vec":[-0.2721950523,-0.5154803151],"nodes":["2166511","40113280","39929781","1701538","1812612"]},"0bb754879154b0a10ee9966636348d831081e151":{"id":"0bb754879154b0a10ee9966636348d831081e151","date":"2011-01-01T12:12:00Z","text":"We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pre-training.","category_a":"ArXiv","category_b":"Journal","keywords":["Encoder","Jacobian matrix and determinant","Loss function","Memory-level parallelism","Noise reduction","Nonlinear system"],"vec":[-0.5654938789,0.7349838034],"nodes":["2425018","3012669","3119801","1935910","1751762","1724875"]},"ccf415df5a83b343dae261286d29a40e8b80e6c6":{"id":"ccf415df5a83b343dae261286d29a40e8b80e6c6","date":"2009-01-01T12:12:00Z","text":"Whereas theoretical work suggests that deep architectures might be more efficient at representing highly-varying functions, training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pretraining. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments confirm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization, the positive effect of pre-training in terms of optimization and its role as a regularizer. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples.","category_a":"AISTATS","category_b":"ML","keywords":["Algorithm","Experiment","Initialization (programming)","Program optimization","Simulation","Unsupervised learning"],"vec":[-0.3869040964,0.5308742619],"nodes":["1761978","1798462","1751762","1751569","1724875"]},"3bf1d8b8ef045df1787c703fb6874f01709597da":{"id":"3bf1d8b8ef045df1787c703fb6874f01709597da","date":"2015-01-01T12:12:00Z","text":"Our last model that uses unsupervised learning is again a general learning machine invented by Geoffrey Hinton and Terrance Sejnowski in the mid 1980 called Boltzmann machine. This machine is a general form of a recurrent neural network with visible nodes that receive input or provide output, and hidden notes that are not connected to the outside world directly. Such a stochastic dynamic network, a recurrent system with hidden nodes, together with the adjustable connections, provide the system with enough degrees of freedom to approximate any dynamical system. While this has been recognized for a long time, finding practical training rules for such systems have been a major challenge for which there was only recently major progress. These machines use unsupervised learning to learn hierarchical representations based on the statistics of the world. Such representations are key to more advanced applications of machine learning and to human abilities. The basic building block is a one-layer network with one visible layer and one hidden layer. An example of such a network is shown in Fig. 1.1. The nodes represent random variable similar to the Bayesian networks discussed before. We will specifically consider binary nodes that mimic neuronal states which are either firing or not. The connections between the have weights wij which specify how much they influence the on-state of connected nodes. Such systems can be described by an energy function. The energy between two nodes that are symmetrically connected with strength wij is","category_a":"","category_b":"Others","keywords":["Ability","Academic degree","Anatomic Node","Approximation algorithm","Artificial neural network","Bayesian network","Biological Neural Networks","Boltzmann machine","Dynamical system","Feature learning","Machine learning","Mathematical optimization","Multilayer perceptron","Note (document)","Recurrent neural network","Rule (guideline)","Unsupervised learning"],"vec":[-0.3115987038,0.3343342273],"nodes":["1724875","1777528","1751762","1798462"]},"3549ff6774d1dde5e6c80b2e97bc5b6ce578f677":{"id":"3549ff6774d1dde5e6c80b2e97bc5b6ce578f677","date":"2002-01-01T12:12:00Z","text":"XML is widely recognized as the data interchange standard for tomorrow, because of its ability to represent data from a wide variety sources. Hence, XML is likely to be the format through which data from multiple sources is integrated.In this paper we study the problem of integrating XML data sources through correlations realized as join operations. A challenging aspect of this operation is the XML document structure. Two documents might convey approximately or exactly the same information but may be quite different in structure. Consequently approximate match in structure, in addition to, content has to be folded in the join operation. We quantify approximate match in structure and content using well defined notions of distance. For structure, we propose computationally inexpensive lower and upper bounds for the tree edit distance metric between two trees. We then show how the tree edit distance, and other metrics that quantify distance between trees, can be incorporated in a join framework. We introduce the notion of reference sets to facilitate this operation. Intuitively, a reference set consists of data elements used to project the data space. We characterize what constitutes a good choice of a reference set and we propose sampling based algorithms to identify them. This gives rise to a variety of algorithmic approaches for the problem, which we formulate and analyze. We demonstrate the practical utility of our solutions using large collections of real and synthetic XML data sets.","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","Approximation algorithm","Dataspaces","Document","Edit distance","Graph edit distance","Interchange circuit","Sampling (signal processing)","Software framework","Synthetic data","XML","XML database"],"vec":[1.0168448486,0.0625650581],"nodes":["1679363","1735239","1721062","1704011","1689202"]},"0bfdae2b923805e2453b2affcb1ba05994889d77":{"id":"0bfdae2b923805e2453b2affcb1ba05994889d77","date":"2007-01-01T12:12:00Z","text":"The VLDB Endowment now has a board of 21 elected trustees, who are the legal guardians of the Endowment's charter and activities. Trustees are elected for a six-year period; the election procedure is documented in the code of regulations available on the Endowment's Web site. The board is continuously renewed, with one third of its members being replaced every two years. The trustees are elected among internationally distinguished researchers and professionals in the field of database and information systems who have contributed to the objectives of the Endowment with dedication and distinction. All trustees provide voluntary service.","category_a":"VLDB","category_b":"DB","keywords":["Database","Information system","VLDB"],"vec":[0.0600278872,-0.2254099158],"nodes":["1699537","1712149","1693070","1687400","1741158","1707465","1691108","1785673","1702212","1695700","1770124","1728318","1736846","1694144","39742554","1728620","1770962","1733290","1704011","1743429","1720368",0,"40158786","1688692","40163754","1743774","1862910","6943123","2458285","5963711","3221648","1705257","1759150","39025377","1765659","1719471","1695250","1701215","3423464","1732484","1684197","1735239","1679709","1709145","1704729","1751591","31419949","2286768","8826771","1716799","1746043","1800215","1689081","1681326","1750566","1687984","1720826","1765813","1705151","2760169","1734183","3058662","1734200","1776561","35556052","1747737","9132078","40636799","2433757","1735915","2468810","40637725","1737896","40514015","1709353","1751739","34957142"]},"087f06a00bc3cbaa2aa16baf8a8a98eb50358d16":{"id":"087f06a00bc3cbaa2aa16baf8a8a98eb50358d16","date":"2014-01-01T12:12:00Z","text":"We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context created with a subset of input symbols selected by the attention mechanism. We report initial results demonstrating that this new approach achieves phoneme error rates that are comparable to the state-of-the-art HMM-based decoders, on the TIMIT dataset.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Encoder","Hidden Markov model","Input\/output","Markov model","Recurrent neural network","Speech recognition","TIMIT"],"vec":[-0.3311839033,0.0755917042],"nodes":["2292403","3335364","1979489","1751762"]},"4f5d7dc3d41236ed6a42cf4105cc79fa2fb0828d":{"id":"4f5d7dc3d41236ed6a42cf4105cc79fa2fb0828d","date":"2016-01-01T12:12:00Z","text":"Humans learn a predictive model of the world and use this model to reason about future events and the consequences of actions. In contrast to most machine predictors, we exhibit an impressive ability to generalize to unseen scenarios and reason intelligently in these settings. One important aspect of this ability is physical intuition (Lake et al., 2016). In this work, we explore the potential of unsupervised learning to find features that promote better generalization to settings outside the supervised training distribution. Our task is predicting the stability of towers of square blocks. We demonstrate that an unsupervised model, trained to predict future frames of a video sequence of stable and unstable block configurations, can yield features that support extrapolating stability prediction to blocks configurations outside the training set distribution.","category_a":"ArXiv","category_b":"Journal","keywords":["Extrapolation","Instability","Unsupervised learning"],"vec":[-0.3445596634,0.3288730242],"nodes":["1778734","1760871","1751762"]},"03b18dcde7ba5bb0e87b2bdb68ab7af951daf162":{"id":"03b18dcde7ba5bb0e87b2bdb68ab7af951daf162","date":"2015-01-01T12:12:00Z","text":"Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English\u2192German and English\u2192French translation tasks of WMT\u201914.","category_a":"ACL","category_b":"ComLing","keywords":["Artificial neural network","BLEU","Baseline (configuration management)","Importance sampling","Machine translation","Neural machine translation","Sampling (signal processing)","Statistical machine translation","Vocabulary"],"vec":[-1.0222207785,0.1724838402],"nodes":["34564969","1979489","1710604","1751762"]},"1298f6dd80c11766a3ff8ae1a4c21e12eadb7c17":{"id":"1298f6dd80c11766a3ff8ae1a4c21e12eadb7c17","date":"2014-01-01T12:12:00Z","text":"Privacy-preserving data publishing is an important problem that has been the focus of extensive study. The state-of-the-art solution for this problem is differential privacy, which offers a strong degree of privacy protection without making restrictive assumptions about the adversary. Existing techniques using differential privacy, however, cannot effectively handle the publication of high-dimensional data. In particular, when the input dataset contains a large number of attributes, existing methods require injecting a prohibitive amount of noise compared to the signal in the data, which renders the published data next to useless.\n To address the deficiency of the existing methods, this paper presents P<scp>riv<\/scp>B<scp>ayes<\/scp>, a differentially private method for releasing high-dimensional data. Given a dataset <i>D<\/i>, P<scp>riv<\/scp>B<scp>ayes<\/scp> first constructs a Bayesian network <i>N<\/i>, which (i) provides a succinct model of the correlations among the attributes in <i>D<\/i> and (ii) allows us to approximate the distribution of data in <i>D<\/i> using a set <i>P<\/i> of low-dimensional marginals of <i>D<\/i>. After that, P<scp>riv<\/scp>B<scp>ayes<\/scp> injects noise into each marginal in <i>P<\/i> to ensure differential privacy and then uses the noisy marginals and the Bayesian network to construct an approximation of the data distribution in <i>D<\/i>. Finally, P<scp>riv<\/scp>B<scp>ayes<\/scp> samples tuples from the approximate distribution to construct a synthetic dataset, and then releases the synthetic data. Intuitively, P<scp>riv<\/scp>B<scp>ayes<\/scp> circumvents the curse of dimensionality, as it injects noise into the low-dimensional marginals in <i>P<\/i> instead of the high-dimensional dataset <i>D<\/i>. Private construction of Bayesian networks turns out to be significantly challenging, and we introduce a novel approach that uses a surrogate function for mutual information to build the model more accurately. We experimentally evaluate P<scp>riv<\/scp>B<scp>ayes<\/scp> on real data and demonstrate that it significantly outperforms existing solutions in terms of accuracy.","category_a":"SIGMOD","category_b":"DB","keywords":["Adversary (cryptography)","Angular defect","Approximation algorithm","Bayesian network","Curse of dimensionality","Differential privacy","Information privacy","Mutual information","Service control point","Surrogate key","Synthetic data"],"vec":[0.84909813,0.674862339],"nodes":["1689591","1709589","1738196","1704011","33285410"]},"e93a59b37f3d6282ebd4e5bad61924f6d75385d9":{"id":"e93a59b37f3d6282ebd4e5bad61924f6d75385d9","date":"2014-01-01T12:12:00Z","text":"Neural Autoregressive Distribution Estimators (NADEs) have recently been shown as successful alternatives for modeling high dimensional multimodal distributions. One issue associated with NADEs is that they rely on a particular order of factorization for P (x). This issue has been recently addressed by a variant of NADE called Orderless NADEs and its deeper version, Deep Orderless NADE. Orderless NADEs are trained based on a criterion that stochastically maximizes P (x) with all possible orders of factorizations. Unfortunately, ancestral sampling from deep NADE is very expensive, corresponding to running through a neural net separately predicting each of the visible variables given some others. This work makes a connection between this criterion and the training criterion for Generative Stochastic Networks (GSNs). It shows that training NADEs in this way also trains a GSN, which defines a Markov chain associated with the NADE model. Based on this connection, we show an alternative way to sample from a trained Orderless NADE that allows to trade-off computing time and quality of the samples: a 3 to 10-fold speedup (taking into account the waste due to correlations between consecutive samples of the chain) can be obtained without noticeably reducing the quality of the samples. This is achieved using a novel sampling procedure for GSNs called annealed GSN sampling, similar to tempering methods that combines fast mixing (obtained thanks to steps at high noise levels) with accurate samples (obtained thanks to steps at low noise levels).","category_a":"KDD","category_b":"AI","keywords":["Artificial neural network","Autoregressive model","Eisenstein's criterion","HIPPI","Markov chain","Multimodal interaction","Sampling (signal processing)","Speedup"],"vec":[-0.2758636521,0.6265917652],"nodes":["1685369","1955694","1979489","1751762"]},"3df5b6b3b2b648f3d8224322e6a0f127850df017":{"id":"3df5b6b3b2b648f3d8224322e6a0f127850df017","date":"2015-01-01T12:12:00Z","text":"Stream processing addresses the needs of real-time applications. Transaction processing addresses the coordination and safety of short atomic computations. Heretofore, these two modes of operation existed in separate, stove-piped systems. In this work, we attempt to fuse the two computational paradigms in a single system called S-Store. In this way, S-Store can simultaneously accommodate OLTP and streaming applications. We present a simple transaction model for streams that integrates seamlessly with a traditional OLTP system, and provides both ACID and streamoriented guarantees. We chose to build S-Store as an extension of H-Store an open-source, in-memory, distributed OLTP database system. By implementing S-Store in this way, we can make use of the transaction processing facilities that H-Store already provides, and we can concentrate on the additional features that are needed to support streaming. Similar implementations could be done using other main-memory OLTP platforms. We show that we can actually achieve higher throughput for streaming workloads in S-Store than an equivalent deployment in H-Store alone. We also show how this can be achieved within H-Store with the addition of a modest amount of new functionality. Furthermore, we compare S-Store to two state-of-the-art streaming systems, Esper and Apache Storm, and show how S-Store can sometimes exceed their performance while at the same time providing stronger correctness guarantees.","category_a":"VLDB","category_b":"DB","keywords":["ACID","Apache Storm","App Store","Block cipher mode of operation","Computation","Computer data storage","Correctness (computer science)","Esper_(software)","H-Store","In-memory database","Online transaction processing","Open-source software","Stream processing","Throughput","Transaction processing","Vertical bar"],"vec":[0.6175995147,-0.4811379974],"nodes":["2960286","1773620","2031287","3137330","2109957","40444545","1746961","2033016","1740962","1774210","1695715","1793745","39049654"]},"cc4296eecd15b83a3c9b9d3a84e91327bd14c7bd":{"id":"cc4296eecd15b83a3c9b9d3a84e91327bd14c7bd","date":"2016-01-01T12:12:00Z","text":"Over the last decade, the largest data warehouses have increased from 5 to 100 terabytes, according to Winter Corp., and by 2010, most of today's data warehouses will be 10 times larger, according to The Data Warehouse Institute (TDWI). As data warehouses grow in size to accommodate regulatory requirements and competitive pressures, ensuring adequate database performance will be a big challenge in order to answer more ad hoc queries from more people. This article examines the various ways databases are architected to handle the rapidly increasing scalability requirements, and the best combination of approaches for achieving high performance at low cost. Obviously, there are limits to the performance of any individual processor (CPU) or individual disk. Hence, all high-performance computers include multiple CPUs and multiple disks. Similarly, a high-performance DBMS must take advantage of multiple disks and multiple CPUs. However, there are significant differences between various databases concerning how they are able to take advantage of resources. In the first section of this article (Better Performance through Parallelism: Three Common Approaches), we discuss hardware architectures and indicate which ones are more scalable. As well, we discuss the commercial DBMSs that run on each architecture. In the second section (Going Even Faster: Hardware Acceleration), we turn to proprietary hardware as an approach to providing additional scalability, and indicate why this has not worked well in the past \u2013 and why it is unlikely to do any better in the future. Lastly, the third section (Achieving Scalability through Software) discusses software techniques that can be used to enhance data warehouse performance. There are three widely used approaches for parallelizing work over additional hardware: \u2022 shared memory \u2022 shared disk \u2022 shared nothing Shared memory: In a shared-memory approach, as implemented on many symmetric multi-processor machines, all of the CPUs share a single memory and a single collection of disks. This approach is relatively easy to program: complex distributed locking and commit protocols are not needed, since the lock manager and buffer pool are both stored in the memory system where they can be easily accessed by all the processors. Unfortunately, shared-memory systems have fundamental scalability limitations, as all I\/O and memory requests have to be transferred over the same bus that all of the processors share,","category_a":"","category_b":"Others","keywords":["Architecture as Topic","Automatic parallelization","Buffers","CPU (central processing unit of computer system)","Central processing unit","Computer","Computers","Database","Hardware acceleration","Hoc (programming language)","Lock (computer science)","Multiprocessing","Oracle Database","Parallel computing","Proprietary hardware","Published Database","Requirement","Scalability","Shared memory","Shared nothing architecture","Supercomputer","Symmetric multiprocessing","Terabyte","corporation","database management software"],"vec":[0.7549252748,-0.6732806546],"nodes":["1765659","2033016","1695715"]},"1d6156b17ef02f510394f78c3c57b9a1d93af2b4":{"id":"1d6156b17ef02f510394f78c3c57b9a1d93af2b4","date":"2006-01-01T12:12:00Z","text":"XML schema design has two opposing goals: elimination of update anomalies requires that the schema be as normalized as possible; yet higher query performance and simpler query expression are often obtained through the use of schemas that permit redundancy. In this paper, we show that the recently proposed MCT data model, which extends XML by adding colors, can be used to address this dichotomy effectively. Specifically, we formalize the intuition of anomaly avoidance in MCT using notions of node normal and edge normal forms, and the goal of efficient query processing using notions of association recoverability and direct recoverability. We develop algorithms for transforming design specifications given as ER diagrams into MCT schemas that are in a node or edge normal form and satisfy association or direct recoverability. Experimental results using a wide variety of ER diagrams validate the benefits of our design methodology.","category_a":"ICDE","category_b":"DB","keywords":["Algorithm","Color","Data model","Database","Diagram","Entity\u2013relationship model","Erd\u0151s\u2013R\u00e9nyi model","Mobile data terminal","Normal form (abstract rewriting)","Serializability","Software bug","XML","XML schema"],"vec":[0.8759377697,0.0292797132],"nodes":["3301260","1735239","1708593","1704011"]},"6f6b397267946e3faaac4e5fa587e1876fe69da2":{"id":"6f6b397267946e3faaac4e5fa587e1876fe69da2","date":"2016-01-01T12:12:00Z","text":"Natural language correction has the potential to help language learners improve their writing skills. While approaches with separate classifiers for different error types have high precision, they do not flexibly handle errors such as redundancy or non-idiomatic phrasing. On the other hand, word and phrase-based machine translation methods are not designed to cope with orthographic errors, and have recently been outpaced by neural models. Motivated by these issues, we present a neural network-based approach to language correction. The core component of our method is an encoder-decoder recurrent neural network with an attention mechanism. By operating at the character level, the network avoids the problem of out-of-vocabulary words. We illustrate the flexibility of our approach on dataset of noisy, user-generated text collected from an English learner forum. When combined with a language model, our method achieves a state-of-the-art F0.5-score on the CoNLL 2014 Shared Task. We further illustrate that training the network on additional data with synthesized errors can improve performance.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Bit error rate","Encoder","Errors-in-variables models","Language model","Machine translation","Natural language","Orthographic projection","Recurrent neural network","Redundancy (engineering)","User-generated content","Vocabulary"],"vec":[-0.6485209601,-0.2170055486],"nodes":["40548699","3362484","3365231","1746807","1701538"]},"6e3f2fe073ba009336c0f13636bb5d137ac276f6":{"id":"6e3f2fe073ba009336c0f13636bb5d137ac276f6","date":"2011-01-01T12:12:00Z","text":"Many naturally occurring phenomena such as music, speech, or human motion are inherently sequential. Complex sequences are often non-local (longterm temporal dependencies) and high-dimensional (multi-modal conditional distribution). For the example of polyphonic music, these properties represent the basic components of Western music, namely rhythm and harmony. Here we wish to exploit the recurrent neural network (RNN) internal memory that can in principle represent long-term dependencies, and energy-based models, such as the Restricted Boltzmann Machine (RBM), that allow us to express complex distributions by the means of an energy function. This combination was first put forward with the so-called Temporal RBM (TRBM) [3], the first such probabilistic model which however uses a heuristic training procedure. The Recurrent TRBM (RTRBM) [4] is a slight modification of the TRBM that allows for exact inference and efficient training by contrastive divergence (CD). The RTRBM can be understood as a sequence of RBMs whose parameters","category_a":"","category_b":"Others","keywords":["Artificial neural network","Biological Neural Networks","Boltzmann machine","Computer data storage","Fibrosis of Extraocular Muscles, Congenital, with Synergistic Divergence","Heuristic","Heuristics","Inference","Kinesiology","Man-Machine Systems","Mathematical optimization","Memory Disorders","Modal logic","Recurrent neural network","Restricted Boltzmann machine","Statistical model"],"vec":[-0.3546940342,0.3083728138],"nodes":["2488222","1724875","1751762"]},"bd212586b54cb959c5d5058a4e16ee6b2cdae1cc":{"id":"bd212586b54cb959c5d5058a4e16ee6b2cdae1cc","date":"2011-01-01T12:12:00Z","text":"Microblogs such as Twitter provide a valuable stream of diverse user-generated data. While the data extracted from Twitter is generally timely and accurate, the process by which developers extract structured data from the tweet stream is ad-hoc and requires reimplementation of common data manipulation primitives. In this paper, we present two systems for querying and extracting structure from Twitter-embedded data. The first, TweeQL, provides a streaming SQL-like interface to the Twitter API, making common tweet processing tasks simpler. The second, TwitInfo, shows how end-users can interact with and understand aggregated data from the tweet stream, in addition to showcasing the power of the TweeQL language. Together these systems show the richness of content that can be extracted from Twitter.","category_a":"SIGMOD","category_b":"DB","keywords":["Application programming interface","Data model","Hoc (programming language)","SQL","User-generated content"],"vec":[0.4302633181,-0.6224450305],"nodes":["40030541","35609041","2259673","1743286","2033016","34205614"]},"20dd934a841f775ed18e0d2f8e887f363cbb9099":{"id":"20dd934a841f775ed18e0d2f8e887f363cbb9099","date":"1999-01-01T12:12:00Z","text":"Heirarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal profiles, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way for superior to what conventional relational or object-oriented databases offer. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated &#8220;queries&#8221; involve navigational access.\nIn this paper, we develop the core of a formal data model for network directories, and propose a sequence of efficiently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the efficiency of each algorithm in terms of its I\/O complexity. Our data model and query languages share the flexibility and utility of the recent proposals for semi-structured data models, while at the same time effectively addressing the specific needs of network directory applications, which we demonstrate by means of a representative real-life example.","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","Book","Computer data storage","Data model","Database","Hoc (programming language)","Out-of-core algorithm","Query language","Real life","Semi-structured data"],"vec":[0.8289546824,-0.1469539146],"nodes":["1735239","1708593","1702212","1704011","1778524"]},"764ecb30e4ab5b1ad0cebec96019d9753c0c7f62":{"id":"764ecb30e4ab5b1ad0cebec96019d9753c0c7f62","date":"2005-01-01T12:12:00Z","text":"We present a machine learning approach to robust textual inference, in which parses of the text and the hypothesis sentences are used to measure their asymmetric \u201csimilarity\u201d, and thereby to decide if the hypothesis can be inferred. This idea is realized in two different ways. In the first, each sentence is represented as a graph (extracted from a dependency parser) in which the nodes are words\/phrases, and the links represent dependencies. A learned, asymmetric, graph-matching cost is then computed to measure the similarity between the text and the hypothesis. In the second approach, the text and the hypothesis are parsed into the logical formula-like representation used by (Harabagiu et al., 2000). An abductive theorem prover (using learned costs for making different types of assumptions in the proof) is then applied to try to infer the hypothesis from the text, and the total \u201ccost\u201d of proving the hypothesis is used to decide if the hypothesis is entailed.","category_a":"","category_b":"Others","keywords":["Abductive reasoning","Anatomic Node","Automated theorem proving","Graph (discrete mathematics)","Graph - visual representation","Inference","Machine learning","Matching (graph theory)","Parser","Parsing","Phrases","Well-formed formula","emotional dependency","sentence"],"vec":[-0.4292591478,-0.3539954889],"nodes":["2979876","1761880","13642348","39153843","2987096","3259253","3257930","2241127","1812612","1701538"]},"f8c8619ea7d68e604e40b814b40c72888a755e95":{"id":"f8c8619ea7d68e604e40b814b40c72888a755e95","date":"2012-01-01T12:12:00Z","text":"The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although domain knowledge can be used to help design representations, learning can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, manifold learning, and deep learning. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Data (computing)","Deep learning","Feature learning","Machine learning","Nonlinear dimensionality reduction","Unsupervised learning"],"vec":[-0.4431498988,0.3622749474],"nodes":["1751762","1760871","1724875"]},"4cf66c66ca404ea3b5474e78e6cf94f6bfd05b0d":{"id":"4cf66c66ca404ea3b5474e78e6cf94f6bfd05b0d","date":"2011-01-01T12:12:00Z","text":"We consider the problem of automatically collecting semantic labels during robotic mapping by extending the mapping system to include text detection and recognition modules. In particular, we describe a system by which a SLAM-generated map of an office environment can be annotated with text labels such as room numbers and the names of office occupants. These labels are acquired automatically from signs posted on walls throughout a building. Deploying such a system using current text recognition systems, however, is difficult since even state-of-the-art systems have difficulty reading text from non-document images. Despite these difficulties we present a series of additions to the typical mapping pipeline that nevertheless allow us to create highly usable results. In fact, we show how our text detection and recognition system, combined with several other ingredients, allows us to generate an annotated map that enables our robot to recognize named locations specified by a user in 84% of cases.","category_a":"Robotics and Automation","category_b":"AI","keywords":["Optical character recognition","Robot","Robotic mapping","Semantic mapping (statistics)","Simultaneous localization and mapping"],"vec":[-0.5233207742,-0.4445628919],"nodes":["1714272","39086009","5574038","1701538"]},"afa5f554eba815dc34be4409a595a53f514cb0bc":{"id":"afa5f554eba815dc34be4409a595a53f514cb0bc","date":"2016-01-01T12:12:00Z","text":"The current best-performing methods in broadcoverage corpus-based syntax (tree) learning are based on linear distributional methods (Clark, 2001; Klein and Manning, 2001). In particular, they have so far greatly outperformed methods which learn (P)CFG grammars, despite the linguistic appeal of such local, recursive models. Broadly, this seems to be because decisions made by distributionalmethods are directly backed by superficially observable data, while (P)CFG learning requires careful construction of intermediate syntactic structures whose benefit to a model\u2019s quality is only indirectly observable, if at all. In this abstract, we first describe how linear distributional methods can be applied to the induction of tree structures, discuss a nested but non-recursive model of tree structure, and finally outline two extensions of this basic model, which are currently under investigation.","category_a":"","category_b":"Others","keywords":["Body of uterus","Decision","Linguistics","Observable","Recursion","Tree structure"],"vec":[-0.4047674394,-0.4993072179],"nodes":["38666915","1812612"]},"0ad8d00860f3b65527a40377dd96b55564515167":{"id":"0ad8d00860f3b65527a40377dd96b55564515167","date":"2016-01-01T12:12:00Z","text":"Offline handwritten character recognition system recognizes characters from images. Since the past few decades, many researchers have been developing various handwritten recognition systems for various languages. This paper demonstrates the role and significance of an offline handwritten character recognition system in various applications. References 1. Optical_character_recognition, https:\/\/en.wikipedia.org\/wiki\/Optical_character_recognition 2. Alfons Juan, Veronica Romero, Joan Andreu Sanchez, Nicolas Serrano, Alejandro H. Toselli and Enrique Vidal. 2010. Handwritten Text Recognition for Ancient Documents. Workshop on Applications of Pattern Analysis. JMLR: Workshop and Conference Proceedings","category_a":"","category_b":"Others","keywords":["Handwriting recognition","Journal of Machine Learning Research","Languages","Optical character recognition"],"vec":[-0.1040538322,-0.2654403516],"nodes":["1871451","9369338","2422013","1706755","29563340","1824984","1719087","36010601","7179232","38272296","1698982","36425660",0,"2946873","1975517","5574038","37742741","1714272","2638806","39086009","39082513","25629078","1701538"]},"50d53cc562225549457cbc782546bfbe1ac6f0cf":{"id":"50d53cc562225549457cbc782546bfbe1ac6f0cf","date":"2013-01-01T12:12:00Z","text":"Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the \u201cSumatran tiger\u201d and \u201cBengal tiger.\u201d Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.","category_a":"NIPS","category_b":"ML","keywords":["Entity","Freebase","Knowledge base","Question answering","Text corpus","Tipu's Tiger","Unsupervised learning","Word embedding","WordNet"],"vec":[-0.5298541444,-0.3306328068],"nodes":["2166511","2318091","1812612","1701538"]},"b90196005b0f1eb0953758c16869633d7ca3399d":{"id":"b90196005b0f1eb0953758c16869633d7ca3399d","date":"2010-01-01T12:12:00Z","text":"Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier\u2019s entity cluster output. Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora. This suggests that sievebased approaches could be applied to other NLP tasks.","category_a":"EMNLP","category_b":"ComLing","keywords":["General number field sieve","Multitier architecture","Natural language processing","Text corpus"],"vec":[-0.1773035208,-0.0414929024],"nodes":["2217626","1991257","39935833","35170013","1760868","1746807","1812612"]},"988cbe6332ce9b202ed3d8c6d03c678e9f7c3e2f":{"id":"988cbe6332ce9b202ed3d8c6d03c678e9f7c3e2f","date":"2017-01-01T12:12:00Z","text":"State of the art sensors within a single autonomous vehicle (AV) can produce video and LIDAR data at rates greater than 30 GB\/hour. Unsurprisingly, even small AV research teams can accumulate tens of terabytes of sensor data from multiple trips and multiple vehicles. AV practitioners would like to extract information about specific locations or specific situations for further study, but are often unable to. Queries over AV sensor data are different from generic analytics or spatial queries because they demand reasoning about fields of view as well as heavy computation to extract features from scenes. In this article and demo we present Vroom, a system for ad-hoc queries over AV sensor databases. Vroom combines domain specific properties of AV datasets with selective indexing and multi-query optimization to address challenges posed by AV sensor data.","category_a":"VLDB","category_b":"DB","keywords":["Autonomous car","Computation","Database","Hoc (programming language)","Program optimization","Query optimization","Sensor","Terabyte"],"vec":[0.5207927292,-0.0798811259],"nodes":["8207144","32300803","1769274","2033016","1695715","1867298"]},"961f6d5bb9b43833d7387e377b25d7a68118b23b":{"id":"961f6d5bb9b43833d7387e377b25d7a68118b23b","date":"2013-01-01T12:12:00Z","text":"This supplementary material contains four sections. The first section details the design and methods we used to create the Sentiment Treebank. The second provides an analysis of how much this treebank helps performance on recursive models. The third section gives several examples of the data we performed the negation analysis on and the fourth shows the most negative\/positive n-grams in the dev set selected by the RNTN.","category_a":"","category_b":"Others","keywords":["Electronic Supplementary Materials","Grams","N-gram","Quantity","Recursion","Sentiment analysis","Treebank","gram"],"vec":[-0.225377047,-0.1579309398],"nodes":["2166511","24590005","31971690","1964541","1812612","1701538","2205340"]},"58cfb95d06b6f4ccbf79ed1436c50187d1657252":{"id":"58cfb95d06b6f4ccbf79ed1436c50187d1657252","date":"2004-01-01T12:12:00Z","text":"In file-sharing P2P networks, a fundamental problem is that of identifying databases that are relevant to user queries. This problem is referred to as the location problem in P2P literature. We propose a scalable solution to the location problem in a data-sharing P2P network, consisting of a network of XML database nodes and XML router nodes, and make the following contributions. We develop the internal organization and routing protocols for the XML router nodes, to enable scalable XPath query and update processing, under the open and the agreement cooperation models between nodes. Since router nodes tend to be memory constrained, we facilitate a space\/performance tradeoff by permitting aggregated routing states, and developing algorithms for generating and using such aggregated information. We experimentally demonstrate the scalability of our approach, and the performance of our query and update protocols, using a detailed simulation model, varying key design parameters.","category_a":"Data Engineering","category_b":"DB","keywords":["Algorithm","Database","File sharing","Network theory","Peer-to-peer","Peer-to-peer file sharing","Router (computing)","Routing","Scalability","Simulation","XML","XML database","XPath"],"vec":[0.8671085617,0.0489882445],"nodes":["1721062","1695914","1704011","1689202"]},"3d913755fcda4fde4e0a1365299429038e256c74":{"id":"3d913755fcda4fde4e0a1365299429038e256c74","date":"2015-01-01T12:12:00Z","text":"This paper presents a methodology for tackling the authorship verification problem. The approach is based on comparing the similarity between a given unknown document against the known documents using a graph representation that captures the syntactic sequence of texts and a graph similarity measure. An unknown document can be classified as having been written by the same author if the majority of the comparisons surpass a predefined threshold. The best results were obtained on the Clef PAN 2014 dataset: 79% for the Spanish and 68% for English, showing that the proposed methodology could be a way for determining a document authorship.","category_a":"","category_b":"Others","keywords":["Classification","Graph (abstract data type)","Graph - visual representation","Pan\u2013tilt\u2013zoom camera","Puromycin Aminonucleoside","Similarity measure"],"vec":[-0.4268440929,-0.355269325],"nodes":["2595181","1715480",0,"3465041","2300910","2012715","1738516","9215251","38044531","32406649","1761880","1701538",0,"1718319",0,"32116926","1812612","1717510"]},"17307b4925959a15b2af55bdbb861df41ab879d2":{"id":"17307b4925959a15b2af55bdbb861df41ab879d2","date":"2001-01-01T12:12:00Z","text":"Guided by an initial idea of building a complex (non linear) decision surface with maximal local margin in input space, we give a possible geometrical intuition as to why K-Nearest Neighbor (KNN) algorithms often perform more poorly than SVMs on classification tasks. We then propose modified K-Nearest Neighbor algorithms to overcome the perceived problem. The approach is similar in spirit to Tangent Distance, but with invariances inferred from the local neighborhood rather than prior knowledge. Experimental results on real world classification tasks suggest that the modified KNN algorithms often give a dramatic improvement over standard KNN and perform as well or better than SVMs. 1 Motivation The notion of margin for classification tasks has been largely popularized by the success of the Support Vector Machine (SVM) [1, 10] approach. The margin of SVMs has a nice geometric interpretation1: it can be defined informally as (twice) the smallest Euclidean distance between the decision surface and the closest training point. The decision surface produced by the original SVM algorithm is the hyperplane that maximizes this distance while still correctly separating the two classes. While the notion of keeping the largest possible safety margin between the decision surface and the data points seems very reasonable and intuitively appealing, questions arise when extending the approach to building more complex, non-linear decision surfaces. . . Non-linear SVMs usually use the \u201ckernel trick\u201d to achieve their non-linearity. This conceptually corresponds to first mapping the input into a higher-dimensional feature space with some non-linear transformation and building a maximum-margin hyperplane (a linear decision surface) there. The \u201ctrick\u201d is that this mapping is never computed directly, but implicitly induced by a kernel. In this setting, the margin being maximized is still the smallest Euclidean distance between the decision surface and the training points, but this time measured in some strange, sometimes infinite dimensional, kernel-induced feature space rather than the original input space. It is less clear whether maximizing the margin in this new space, is meaningful in general. Indeed [11] shows cases where for any separating decision surface in input space there is a feature space in which the corresponding decision surface is a maximum margin hyperplane. for the purpose of this discussion, we consider the original hard-margin SVM algorithm for two linearly separable classes. A different approach is to try and build a non-linear decision surface with maximal distance to the closest data point as measured directly in input space. We could for instance restrict ourselves to a certain class of decision functions and try to find the function with maximal margin among this class. But let us take this even further. Extending the idea of building a correctly separating non-linear decision surface as far away as possible from the data points, we define the notion of local margin as the Euclidean distance, in input space, between a given point on the decision surface and the closest training point. Now would it be possible to find an algorithm that could produce a decision surface which correctly separates the classes and such that the local margin is everywhere maximal along its surface? Surprisingly, the plain old Nearest Neighbor algorithm (1NN) [4] does precisely this2! So why does 1NN in practice often perform worse than SVMs? One typical explanation, is that it has too much capacity, compared to SVM, that the class of function it can produce is too rich. But, considering it has infinite capacity, 1NN is still performing quite well. This study is an attempt to better understand what is happening, based on geometrical intuition, and to derive an improved Nearest Neighbor algorithm from this understanding. 2 Fixing a broken Nearest Neighbor algorithm 2.1 Setting and definitions The setting is that of a classical classification problem in IR (the input space). We are given a training set S of l points {x1, . . . , xl}, xi \u2208 IR n and their corresponding class label {y1 = y(x1), . . . , yl = y(xl)}, yi \u2208 C, C = {1, . . . , Nc} where Nc is the number of different classes. The (x, y) pairs are assumed to be samples drawn from an unknown distribution P (X,Y ). Barring duplicate inputs, the class labels associated to each x \u2208 S define a partition of S: let Sc = {x \u2208 S | y(x) = c}. The problem is to find a decision function f\u0303 : IR \u2192 C that will generalize well on new points drawn from P (X,Y ). f\u0303 should ideally minimize the expected classification error, i.e. minimize EP [If\u0303(X)6=Y ] where EP denotes the expectation with respect to P (X,Y ) and If\u0303(x)6=y denotes the indicator function, whose value is 1 if f\u0303(x) 6= y and 0 otherwise. In the previous and following discussion, we often refer to the concept of decision surface, also known as decision boundary. The function f\u0303 corresponding to a given algorithm defines for any class c \u2208 C two regions of the input space: the region Rc = {x \u2208 IR n | f\u0303(x) = c} and its complement IR \u2212 Rc. The decision surface for class c is the interface of these two regions, and can be seen as a n\u2212 1 dimensional manifold (a \u201csurface\u201d in IR) possibly made of several disconnected components. For simplicity, when we mention the decision surface in our discussion we consider only the case of two class discrimination, in which there is a single decision surface. When we mention a test point, we mean a point x \u2208 IR that does not belong to the training set S and for which the algorithm is to decide on a class f\u0303(x). By distance, we mean the usual Euclidean distance in input-space IR. The distance between two points a and b will be written d(a, b) or alternatively \u2016a\u2212 b\u2016. The distance between a single point x and a set of points S is the distance to the closest point of the set: d(x, S) = minp\u2208S d(x, p). The K-neighborhoodV(x) of a test point x is the set of the K points of S whose distance to x is smallest. The K-c-neighborhoodV c (x) of a test point x is the set of K points of Sc whose distance to x is smallest. A formal proof of this is beyond the scope of this article. By Nearest Neighbor algorithm (1NN) we mean the following algorithm: the class of a test point x is decided to be the same as the class of its closest neighbor in S. By K-Nearest Neighbor algorithm (KNN) we mean the following algorithm: the class of a test point x is decided to be the same as the class appearing most frequently among the K-neighborhood of x.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Binary prefix","Characteristic function (convex analysis)","Data point","Decision boundary","Euclidean distance","Expectation propagation","Feature vector","Formal proof","K-nearest neighbors algorithm","Kernel method","Linear separability","Maxima and minima","Nearest neighbor search","Nearest neighbour algorithm","Nonlinear system","Support vector machine","Test point"],"vec":[-0.4491704632,0.5134107229],"nodes":["1724875","1751762"]},"60abc9657a01a128c5503673609bfe8e7637c00b":{"id":"60abc9657a01a128c5503673609bfe8e7637c00b","date":"2007-01-01T12:12:00Z","text":"Many tasks in robotics can be described as a trajectory that the robot should follow. Unfortunately, specifying the desired trajectory is often a non-trivial task. For example, when asked to describe the trajectory that a helicopter should follow to perform an aerobatic flip, one would have to not only (a) specify a complete trajectory in state space that intuitively corresponds to the aerobatic flip task, but also (b) ensure that the state space trajectory is consistent with the helicopter\u2019s dynamics. This is a non-trivial task for systems with complicated dynamics.","category_a":"","category_b":"Others","keywords":["Reinforcement learning","Robotics","Robotics","State space"],"vec":[-0.0605365589,-1.0768207222],"nodes":["5574038","1689992","1701538"]},"6e1fd106a17d67078a1d0ab8d5ef4f64d0feec1e":{"id":"6e1fd106a17d67078a1d0ab8d5ef4f64d0feec1e","date":"2001-01-01T12:12:00Z","text":"Unsupervised grammar induction systems commonly judge potential constituents on the basis of their effects on the likelihood of the data. Linguistic justifications of constituency, on the other hand, rely on notions such as substitutability and varying external contexts. We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features. The advantages and disadvantages of these systems are examined, including precision\/recall trade-offs, error analysis, and extensibility.","category_a":"CoNLL","category_b":"Others","keywords":["Error analysis (mathematics)","Extensibility","Grammar induction","Part-of-speech tagging","Phrase structure rules"],"vec":[-0.3959587584,-0.4875465265],"nodes":["38666915","1812612"]},"098a170095b1e84b3a8eb673adfe6408fc0dcff5":{"id":"098a170095b1e84b3a8eb673adfe6408fc0dcff5","date":"2005-01-01T12:12:00Z","text":"This thesis focuses on building effective statistical models for disambiguation of sophisticated syntactic and semantic natural language (NL) structures. We advance the state of the art in several domains by (i) choosing representations that encode domain knowledge more effectively and (ii) developing machine learning algorithms that deal with the specific properties of NL disambiguation tasks \u2013 sparsity of training data and large, structured spaces of hidden labels. For the task of syntactic disambiguation, we propose a novel representation of parse trees that connects the words of the sentence with the hidden syntactic structure in a direct way. Experimental evaluation on parse selection for a Head Driven Phrase Structure Grammar shows the new representation achieves superior performance compared to previous models. For the task of disambiguating the semantic role structure of verbs, we build a more accurate model, which captures the knowledge that the semantic frame of a verb is a joint structure with strong dependencies between arguments. We achieve this using a Conditional Random Field without Markov independence assumptions on the sequence of semantic role labels. To address the sparsity problem in machine learning for NL, we develop a method for incorporating many additional sources of information, using Markov chains in the space of words. The Markov chain framework makes it possible to combine multiple knowledge sources, to learn how much to trust each of them, and to chain inferences together. It achieves large gains in the task of disambiguating prepositional phrase attachments.","category_a":"","category_b":"Others","keywords":["Algorithm","Conditional random field","Frame language","Immunoglobulin kappa-Chains","Labels (device)","Language Disorders","Machine learning","Markov chain","Markov property","Models, Statistical","NL (complexity)","Natural language","Parsing","Phrase structure grammar","Sparse matrix","Statistical model","Temporomandibular Joint Disorders","Test set","The Sentence","Trees (plant)","Word-sense disambiguation","algorithm"],"vec":[-0.5126820008,-0.5626044653],"nodes":[0,"1812612","1701538"]},"e50c2e4b8f7df21763f47c8941aa560c75d133b1":{"id":"e50c2e4b8f7df21763f47c8941aa560c75d133b1","date":"2017-01-01T12:12:00Z","text":"","category_a":"Computer Speech & Language","category_b":"Others","keywords":["Language model","Machine translation","Neural machine translation"],"vec":[-0.1899234326,0.000680411],"nodes":["1854385","2345617","36303818","1979489","1751762"]},"5796ba3a657261a49ca9333865b8606980111371":{"id":"5796ba3a657261a49ca9333865b8606980111371","date":"2017-01-01T12:12:00Z","text":"We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.","category_a":"ICML","category_b":"ML","keywords":["Artificial neural network","Deep learning","Dropout (neural networks)","Experiment","Gradient","Hall effect","Matrix regularization","Program optimization","Test set"],"vec":[-0.3981478607,0.4917151116],"nodes":["2309967","38060141","2482072","38018150","2416433","19308176","3422058","35988982","1760871","1751762","1685481"]},"011f7f9ba9e6f9bc7f05994271725bc0fc9c3b94":{"id":"011f7f9ba9e6f9bc7f05994271725bc0fc9c3b94","date":"2017-01-01T12:12:00Z","text":"In many organizations, it is often challenging for users to find relevant data for specific tasks, since the data is usually scattered across the enterprise and often inconsistent. In fact, data scientists routinely report that the majority of their effort is spent finding, cleaning, integrating, and accessing data of interest to a task at hand. In order to decrease the \u201cgrunt work\u201d needed to facilitate the analysis of data \u201cin the wild\u201d, we present DATA CIVILIZER, an end-to-end big data management system. DATA CIVILIZER has a linkage graph computation module to build a linkage graph for the data and a data discovery module which utilizes the linkage graph to help identify data that is relevant to user tasks. It also uses the linkage graph to discover possible join paths that can then be used in a query. For the actual query execution, we use a polystore DBMS, which federates query processing across disparate systems. In addition, DATA CIVILIZER integrates data cleaning operations into query processing. Because different users need to invoke the above tasks in different orders, DATA CIVILIZER embeds a workflow engine which enables the arbitrary composition of different modules, as well as the handling of data updates. We have deployed our preliminary DATA CIVILIZER system in two institutions, MIT and Merck and describe initial positive experiences that show the system shortens the time and effort required to find, prepare, and analyze data.","category_a":"CIDR","category_b":"Others","keywords":["Big data","Computation","Data science","Database","End-to-end principle","Experience","Federation (information technology)","Grunt","Linkage (software)","Merck Index","Sputter cleaning","Workflow engine"],"vec":[0.6626547259,-0.5079858452],"nodes":["38737063","34568734","2034349","39996718","1695715","1740095","1743316","2033016","2168047","8669763"]},"700e1d1b7c54711e96645691438d7fd9fda229ef":{"id":"700e1d1b7c54711e96645691438d7fd9fda229ef","date":"2014-01-01T12:12:00Z","text":"This paper introduces a new benchmark designed to test database management system (DBMS) performance on a mix of data management tasks (joins, filters, etc.) and complex analytics (regression, singular value decomposition, etc.) Such mixed workloads are prevalent in a number of application areas including most science workloads and web analytics. As a specific use case, we have chosen genomics data for our benchmark and have constructed a collection of typical tasks in this domain. In addition to being representative of a mixed data management and analytics workload, this benchmark is also meant to scale to large dataset sizes and multiple nodes across a cluster. Besides presenting this benchmark, we have run it on a variety of storage systems including traditional row stores, newer column stores, Hadoop, and an array DBMS. We present performance numbers on all systems on single and multiple nodes, and show that performance differs by orders of magnitude between the various solutions. In addition, we demonstrate that most platforms have scalability issues. We also test offloading the analytics onto a coprocessor. The intent of this benchmark is to focus research interest in this area; to this end, all of our data, data generators, and scripts are available on our web site.","category_a":"SIGMOD","category_b":"DB","keywords":["Apache Hadoop","Array DBMS","Benchmark (computing)","Coprocessor","Database","Management system","Scalability","Singular value decomposition","Web analytics"],"vec":[0.6673311626,-0.4889275904],"nodes":["37773490","3039341","1759942","1789372","2033016","1695715"]},"46018a894d533813d67322827ca51f78aed6d59e":{"id":"46018a894d533813d67322827ca51f78aed6d59e","date":"2017-01-01T12:12:00Z","text":"In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.","category_a":"Medical Image Analysis","category_b":"Others","keywords":["Brain Neoplasms","Cascade Device Component","Computer vision","Convolutional neural network","Glioblastoma","Information source","Machine learning","Neoplasms","Neural Networks","Neural Tube Defects","Scientific Publication","Test data","childhood brain stem glioma"],"vec":[-0.4316543375,0.0360980445],"nodes":["2203743","37634267","1923596","3058152","1760871","1751762","1972076","1687510","1777528"]},"389bfe18dc161fda4980ec426ffaccec76a918bb":{"id":"389bfe18dc161fda4980ec426ffaccec76a918bb","date":"2013-01-01T12:12:00Z","text":"We apply the spike-and-slab Restricted Boltzmann Machine (ssRBM) to texture modeling. The ssRBM with tiled-convolution weight sharing (TssRBM) achieves or surpasses the state-of-the-art on texture synthesis and inpainting by parametric models. We also develop a novel RBM model with a spikeand-slab visible layer and binary variables in the hidden layer. This model is designed to be stacked on top of the ssRBM. We show the resulting deep belief network (DBN) is a powerful generative model that improves on single-layer models and is capable of modeling not only single high-resolution and challenging textures but also multiple textures with fixed-size filters in the bottom layer.","category_a":"AISTATS","category_b":"ML","keywords":["Bayesian network","Boltzmann machine","Convolution","Deep belief network","Generative model","Image resolution","Inpainting","Restricted Boltzmann machine","Slab allocation","Texture synthesis","The Spike (1997)"],"vec":[-0.3697464786,0.3720409755],"nodes":["37232249","8883973","1760871","1751762"]},"4700e447d1a0cddda978cd87177301790bb58efd":{"id":"4700e447d1a0cddda978cd87177301790bb58efd","date":"2009-01-01T12:12:00Z","text":"We investigate a simple yet effective method to introduce inhibitory and excitatory interactions between units in the layers of a deep neural network classifier. The method is based on the greedy layer-wise procedure of deep learning algorithms and extends the denoising autoencoder (Vincent et al., 2008) by adding asymmetric lateral connections between its hidden coding units, in a manner that is much simpler and computationally more efficient than previously proposed approaches. We present experiments on two character recognition problems which show for the first time that lateral connections can significantly improve the classification performance of deep networks.","category_a":"AISTATS","category_b":"ML","keywords":["Algorithm","Artificial neural network","Autoencoder","Deep learning","Effective method","Experiment","Greedy algorithm","Interaction","Interdependence","Machine learning","Noise reduction","Optical character recognition"],"vec":[-0.3350491288,0.3502026242],"nodes":["1777528","1761978","1724875"]},"2064c2a33eb0b4c8acd23fd60b98c12d6c1ad61e":{"id":"2064c2a33eb0b4c8acd23fd60b98c12d6c1ad61e","date":"2014-01-01T12:12:00Z","text":"Restricted Boltzmann machines (RBMs) are powerful machine learning models, but learning and some kinds of inference in the model require sampling-based approximations, which, in classical digital computers, are implemented using expensive MCMC. Physical computation offers the opportunity to reduce the cost of sampling by building physical systems whose natural dynamics correspond to drawing samples from the desired RBM distribution. Such a system avoids the burn-in and mixing cost of a Markov chain. However, hardware implementations of this variety usually entail limitations such as low-precision and limited range of the parameters and restrictions on the size and topology of the RBM. We conduct software simulations to determine how harmful each of these restrictions is. Our simulations are based on the D-Wave Two computer, but the issues we investigate arise in most forms of physical computation. Our findings suggest that designers of new physical computing hardware and algorithms for physical computers should focus their efforts on overcoming the limitations imposed by the topology restrictions of currently existing physical computers.","category_a":"AAAI","category_b":"AI","keywords":["Algorithm","Approximation","Burn-in","Computation","Computational physics","Computer","Computer hardware","D-Wave Two","Machine learning","Markov chain","Markov chain Monte Carlo","Physical computing","Restricted Boltzmann machine","Sampling (signal processing)","Simulation"],"vec":[0.1541100935,-0.2180749527],"nodes":["3074927","34740554","1760871","1751762"]},"314547de686f67e48c075ef8bbe6fe3a1d22dc92":{"id":"314547de686f67e48c075ef8bbe6fe3a1d22dc92","date":"2014-01-01T12:12:00Z","text":"Database researchers paint big data as a defining challenge. To make the most of the enormous opportunities at hand will require focusing on five research areas.","category_a":"Commun. ACM","category_b":"Journal","keywords":["Big data","Mathematical optimization"],"vec":[0.2177188276,-0.2354087591],"nodes":["2254232","1680081","1728318","1718134","1737944","1703347","1728620","1796515","3030274","1712149","1710965","2046257","1770962","1695576","1684197","1735239","1691108","2033016","1686199","1702212","5151034","1709145","1733290","2072569","1693070","1803218","9246931","1695715","1681578","1737896"]},"6c96ca73b381c4d8191ad73ea2fd9272ff0799c4":{"id":"6c96ca73b381c4d8191ad73ea2fd9272ff0799c4","date":"2009-01-01T12:12:00Z","text":"Autonomous helicopter flight is widely regarded to be a highly challenging control problem. As helicopters are highly unstable and exhibit complicated dynamical behavior, it is particularly difficult to design controllers that achieve high performance over a broad flight regime.\n While these aircraft are notoriously difficult to control, there are expert human pilots who are nonetheless capable of demonstrating a wide variety of maneuvers, including aerobatic maneuvers at the edge of the helicopter's performance envelope. In this paper, we present algorithms for modeling and control that leverage these demonstrations to build high-performance control systems for autonomous helicopters. More specifically, we detail our experiences with the Stanford Autonomous Helicopter, which is now capable of extreme aerobatic flight meeting or exceeding the performance of our own expert pilot.","category_a":"Commun. ACM","category_b":"Journal","keywords":["Algorithm","Apprenticeship learning","Autonomous car","Control system","Experience","Extreme programming","Instability"],"vec":[-0.0335486033,-1.2779606838],"nodes":["5574038","1689992","1701538"]},"65c978a97f54cf255f01c6846d6c51b37c61f836":{"id":"65c978a97f54cf255f01c6846d6c51b37c61f836","date":"2016-01-01T12:12:00Z","text":"Microtask crowdsourcing is increasingly critical to the creation of extremely large datasets. As a result, crowd workers spend weeks or months repeating the exact same tasks \u2014 making it necessary to understand their behavior over these long periods of time. We utilize three large, longitudinal datasets of nine million annotations collected from Amazon Mechanical Turk to examine claims that workers fatigue or satisfice over these long periods, producing lower quality work. We find that, contrary to these claims, workers are extremely stable in their accuracy over the entire period. To understand whether workers set their accuracy based on the task\u2019s requirements for acceptance, we then perform an experiment where we vary the required accuracy for a large crowdsourcing task. Workers did not adjust their accuracy based on the acceptance threshold: workers who were above the threshold continued working at their usual quality level, and workers below the threshold self-selected themselves out of the task. Capitalizing on this consistency, we demonstrate that it is possible to predict workers\u2019 long-term accuracy using just a glimpse of their performance on the first five tasks.","category_a":"ArXiv","category_b":"Journal","keywords":["Amazon Mechanical Turk","Crowdsourcing","Requirement","The Turk"],"vec":[0.3965156473,-0.9261596169],"nodes":["35163655","2580593","3216322","35609041"]},"0bbf348983a12f21c1607b8a963357c33f18f120":{"id":"0bbf348983a12f21c1607b8a963357c33f18f120","date":"2009-01-01T12:12:00Z","text":"Temporal data analysis in data warehouses and datastreaming systems often uses time decay to reduce the importance of older tuples, without eliminating their influence, on the results of the analysis. While exponential time decay is commonly used in practice, other decay functions (e.g. polynomial decay) are not, even though they have been identified as useful. We argue that this is because the usual definitions of time decay are \"backwards\": the decayed weight of a tuple is based on its age, measured backward from the current time. Since this age is constantly changing, such decay is too complex and unwieldy for scalable implementation. In this paper, we propose a new class of \"forward\" decay functions based on measuring forward from a fixed point in time. We show that this model captures the more practical models already known, such as exponential decay and landmark windows, but also includes a wide class of other types of time decay. We provide efficient algorithms to compute a variety of aggregates and draw samples under forward decay, and show that these are easy to implement scalably. Further, we provide empirical evidence that these can be executed in a production data stream management system with little or no overhead compared to the undecayed computations. Our implementation required no extensions to the query language or the DSMS, demonstrating that forward decay represents a practical model of time decay for systems that deal with time-based data.","category_a":"Data Engineering","category_b":"DB","keywords":["Algorithm","Computation","EXPTIME","Fixed point (mathematics)","Forward declaration","Microsoft Windows","Polynomial","Query language","Scalability","Time complexity"],"vec":[0.7628866902,0.2703437534],"nodes":["1709589","1745049","1704011","39719831"]},"0e16487103cbe1d9bf2ce04ea5718c3fe9f08cf8":{"id":"0e16487103cbe1d9bf2ce04ea5718c3fe9f08cf8","date":"1999-01-01T12:12:00Z","text":"Physical layout of data is a crucial determinant of performance in a data warehouse. The optimal clustering of data on disk, for minimizing expected I\/O, depends on the query workload. In practice, we often have a reasonable sense of the likelihood of different classes of queries, e.g., 40% of the queries concern calls made from <italic>some<\/italic> specific telephone number in <italic>some<\/italic> month. In this paper, we address the problem of finding an optimal clustering of records of a fact table on disk, given an expected workload in the form of a probability distribution over query classes.\nAttributes in a data warehouse fact table typically have hierarchies defined on them (by means of auxiliary dimension tables). The product of the dimensional hierarchy levels forms a lattice and leads to a natural notion of query classes. Optimal clustering in this context is a combinatorially explosive problem with a huge search space (doubly exponential in number of hierarchy levels). We identify an important subclass of clustering strategies called <italic>lattice paths<\/italic>, and present a dynamic programming algorithm for finding the optimal lattice path clustering, in time linear in the lattice size. We additionally propose a technique called <italic>snaking<\/italic>, which when applied to a lattice path, always reduces its cost. For a representative class of star schemas, we show that for every workload, there is a snaked lattice path which is globally optimal. Further, we prove that the clustering obtained by applying snaking to the optimal lattice path is never much worse than the globally optimal snaked lattice path clustering. We complement our analyses and validate the practical utility of our techniques with experiments using TPC-D benchmark data.","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","Benchmark (computing)","Cluster analysis","Dynamic programming","Experiment","IBM Tivoli Storage Productivity Center","Maxima and minima","Memory-mapped I\/O","Star schema","Telephone number","Time complexity"],"vec":[1.1298858406,0.0754538114],"nodes":["1735239","1708593","1704011"]},"90df2b8696ad2ff4db8178fded1d17303f917d92":{"id":"90df2b8696ad2ff4db8178fded1d17303f917d92","date":"2009-01-01T12:12:00Z","text":"This paper describes the joint Stanford-UBC knowledge base population system. We developed several entity linking systems based on frequencies of backlinks, training on contexts of anchors, overlap of context with the text of the entity in Wikipedia, and both heuristic and supervised combinations. Our combined systems performed better than the individual components, which situates our runs better than the median of participants. For slot filling, we implemented a straightforward distant supervision system, trained using snippets of the document collection containing both entity and filler from Wikipedia infoboxes. In this case our results are below the median.","category_a":"TAC","category_b":"Others","keywords":["Archive","Backlink","Crostata","Entity linking","Heuristic","Knowledge base","Star filler","Wikipedia"],"vec":[-0.3845556972,-0.2039806233],"nodes":["1733049","3317599","1746807","1812612","2556884","3518463"]},"1780dab3ed9253b596cb68a10cf3c2d9b8e5015f":{"id":"1780dab3ed9253b596cb68a10cf3c2d9b8e5015f","date":"2003-01-01T12:12:00Z","text":"Keyword proximity search is a user-friendly information discovery technique that has been extensively studied for text documents. In extending this technique to structured databases, recent works [6, 7, 4, 2] provide keyword proximity search on labeled graphs. A keyword proximity search does not require the user to know the structure of the graph, the role of the objects containing the keywords, or the type of the connections between the objects. The user simply submits a list of keywords and the system returns the sub-graph that connect the objects containing the keywords. XML and its labeled graph\/tree abstractions are becoming the data model of choice for representing semistructured, self-describing data, and keyword proximity search is well-suited to XML documents as well. We describe a system that provides keyword proximity search on XML data that are modeled as labeled graphs or trees, the edges correspond to the element-subelement relationship and to ID\/IDREF links (in the case of graphs). Our work differs from prior systems for proximity search on labeled graphs in that it can take advantage of knowledge of the schema, e.g., the XML Schema [12], to which the XML data con-","category_a":"VLDB","category_b":"DB","keywords":["Data model","Database","Google Search","Graph labeling","Information discovery","Proximity search (text)","Self-documenting code","Structured analysis","Usability","Web search engine","XML","XML database","XML schema"],"vec":[0.9271264842,0.0775016671],"nodes":["1790787","1754970","1721062","1786049","1704011","2075313"]},"3f099575d82b0248c6be9b042c286b3d81f01e63":{"id":"3f099575d82b0248c6be9b042c286b3d81f01e63","date":"2002-01-01T12:12:00Z","text":"This paper describes the overall design and architecture of the Timber XML database system currently being implemented at the University of Michigan. The system is based upon a bulk algebra for manipulating trees, and natively stores XML. New access methods have been developed to evaluate queries in the XML context, and new cost estimation and query optimization techniques have also been developed. We present performance numbers to support some of our design decisions. We believe that the key intellectual contribution of this system is a comprehensive set-at-a-time query processing ability in a native XML store, with all the standard components of relational query processing, including algebraic rewriting and a cost-based optimizer.","category_a":"VLDB","category_b":"DB","keywords":["Database","F-algebra","Program optimization","Query optimization","Relational database","Rewriting","XML","XML database"],"vec":[0.8808371584,0.0254938782],"nodes":["1735239","1978097","1801163","1708593","2183125","2822695","2042232","1704011","3301260","1683593","1746900"]},"9d4e4136353d6ed07cfb61565151e0e6e8fd7cf6":{"id":"9d4e4136353d6ed07cfb61565151e0e6e8fd7cf6","date":"2006-01-01T12:12:00Z","text":"We present a novel approach to speaker identification in robot dialogue that allows a robot to recognize people during natural conversation and address them by name. Our STanford AI Robot (STAIR) dialogue system attempts to mirror the human speaker identification process. We model the robot dialogue problem as a Markov Decision Process (MDP) and apply a reinforcement learning algorithm to try to learn the optimal dialogue actions. The MDP model works in conjunction with a traditional statistical cluster based speaker identification subsystem. Our approach also addresses open-set speaker identification, dynamically adding new speaker profiles as well as continuously updating known profiles.","category_a":"INTERSPEECH","category_b":"Others","keywords":["Algorithm","Dialog system","Markov decision process","Reinforcement learning","Robot","Speaker recognition"],"vec":[-0.4543793871,-0.2061702533],"nodes":["3234441","38796176","1746807","1701538"]},"26c8d040bef85ad6dde55a8f71af936fb38356ad":{"id":"26c8d040bef85ad6dde55a8f71af936fb38356ad","date":"2015-01-01T12:12:00Z","text":"Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1, 2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.","category_a":"NIPS","category_b":"ML","keywords":["Location awareness","Machine translation","Speech recognition","Speech synthesis","TIMIT"],"vec":[-0.4298645633,0.0644907772],"nodes":["2292403","3335364","1862138","1979489","1751762"]},"2b6a2adddfbf70ea5cb3d9d749c1ef70db8c230a":{"id":"2b6a2adddfbf70ea5cb3d9d749c1ef70db8c230a","date":"2015-01-01T12:12:00Z","text":"Humans are able to accelerate their learning by selecting training materials that are the most informative and at the appropriate level of difficulty. We propose a framework for distributing deep learning in which one set of workers search for the most informative examples in parallel while a single worker updates the model on examples selected by importance sampling. This leads the model to update using an unbiased estimate of the gradient which also has minimum variance when the sampling proposal is proportional to the L2-norm of the gradient. We show experimentally that this method reduces gradient variance even in a context where the cost of synchronization across machines cannot be ignored, and where the factors for importance sampling are not updated instantly across the training set.","category_a":"ArXiv","category_b":"Journal","keywords":["Deep learning","Gradient","Importance sampling","Information","Sampling (signal processing)","Test set","Variance reduction"],"vec":[-0.2467895813,0.2899831968],"nodes":["1815021","2059369","21669342","1760871","1751762"]},"10eb7bfa7687f498268bdf74b2f60020a151bdc6":{"id":"10eb7bfa7687f498268bdf74b2f60020a151bdc6","date":"2008-01-01T12:12:00Z","text":"We present a new technique called \u201ct-SNE\u201d that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.","category_a":"","category_b":"Others","keywords":["Imagery","Isomap","Leigh Disease","Nonlinear dimensionality reduction","Physical object","Sammon mapping","Stochastic process","Subgroup","T-distributed stochastic neighbor embedding"],"vec":[-0.294524484,0.3105787829],"nodes":["1803520","1695689","1751762"]},"5cb40de2c796b9644346821c4cb600fd05e8e727":{"id":"5cb40de2c796b9644346821c4cb600fd05e8e727","date":"2003-01-01T12:12:00Z","text":"This tutorial provides a comprehensive and cohesive overview of the key research results in the area of data stream query processing, both for SQL-like and XML query languages.","category_a":"ICDE","category_b":"DB","keywords":["Database","Query language","SQL","XML"],"vec":[0.2696060171,-0.1350182304],"nodes":["1721062","1704011"]},"55c3597ebe9ce20c3f2374be4dda465726335403":{"id":"55c3597ebe9ce20c3f2374be4dda465726335403","date":"2007-01-01T12:12:00Z","text":"With increasing amounts of data being exchanged and even generated or stored in XML, a natural question is how to perform OLAP on XML data, which can be structurally heterogeneous (e.g., parse trees) and\/or marked-up text documents. A core operator for OLAP is the data cube. While the relational cube can be extended in a straightforward way to XML, we argue such an extension would not address the specific issues posed by XML. While in a relational warehouse, facts are flat records and dimensions may have hierarchies, in an XML warehouse, both facts and dimensions may be hierarchical. Second, XML is flexible: (a) an element may have missing or repeated subelements; (b) different instances of the same element type may have different structure. We identify the challenges introduced by these features of XML for cube definition and computation. We propose a definition for cube adapted for XML data warehouse, including a suitably generalized specification mechanism. We define a cube lattice over the aggregates so defined. We then identify properties of this cube lattice that can be leveraged to allow optimized computation of the cube. Finally, we present the results of an extensive performance evaluation experiment gauging the behavior of alternative algorithms for cube computation.","category_a":"Data Engineering","category_b":"DB","keywords":["Algorithm","Computation","Cube","Data cube","Online analytical processing","Parse tree","Parsing","Performance Evaluation","XML"],"vec":[1.0638485944,0.0607979094],"nodes":["3301260","1735239","1708593","1704011"]},"0d3233d858660aff451a6c2561a05378ed09725a":{"id":"0d3233d858660aff451a6c2561a05378ed09725a","date":"2013-01-01T12:12:00Z","text":"We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.","category_a":"EMNLP","category_b":"ComLing","keywords":["BLEU","Machine translation","Semantic similarity"],"vec":[-0.6935125752,-0.1978048628],"nodes":["2860351","2166511","3208422","1812612"]},"8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092":{"id":"8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092","date":"2013-01-01T12:12:00Z","text":"Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 75.8%.","category_a":"ArXiv","category_b":"Journal","keywords":["Entity","Knowledge base","Text corpus","Unsupervised learning","Word embedding","WordNet"],"vec":[-0.642950677,-0.6220529734],"nodes":["2318091","2166511","1812612","1701538"]},"fff3fa15f1fd07484edf8f355eb53e58d0ed8ccf":{"id":"fff3fa15f1fd07484edf8f355eb53e58d0ed8ccf","date":"2009-01-01T12:12:00Z","text":"The prevalence in Chinese of grammatical structures that translate into English in different word orders is an important cause of translation difficulty. While previous work has used phrase-structure parses to deal with such ordering problems, we introduce a richer set of Chinese grammatical relations that describes more semantically abstract relations between words. Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77). Our Chinese grammatical relations are also likely to be useful for other NLP tasks.","category_a":"HLT-NAACL","category_b":"ComLing","keywords":["BLEU","Log probability","Natural language processing","Type system"],"vec":[-0.9146845701,-0.2337955353],"nodes":["2923276","39034442","1746807","1812612"]},"89677d5cb68a4c020f4604e99b9a72f6c4532581":{"id":"89677d5cb68a4c020f4604e99b9a72f6c4532581","date":"2016-01-01T12:12:00Z","text":"Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words defined by those definitions. We present two applications of these architectures: reverse dictionaries that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant taskspecific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing models that understand phrases and sentences.","category_a":"TACL","category_b":"Others","keywords":["Crossword","Dictionary","Natural language processing","Reverse engineering"],"vec":[-1.1392353058,0.1440930954],"nodes":["38909889","1979489","1941113","1751762"]},"51691435d646c5b9152162f328d29511755328ba":{"id":"51691435d646c5b9152162f328d29511755328ba","date":"2003-01-01T12:12:00Z","text":"In this paper, we show a direct equivalence between spectral clustering and kernel PCA, and how both are special cases of a more general learning problem, that of learning the principal eigenfunctions of a kernel, when the functions are from a function space whose scalar product is defined with respect to a density model. This defines a natural mapping for new data points, for methods that only provided an embedding, such as spectral clustering and Laplacian eigenmaps. The analysis hinges on a notion of generalization for embedding algorithms based on the estimation of underlying eigenfunctions, and suggests ways to improve this generalization by smoothing the data empirical distribution.","category_a":"","category_b":"Others","keywords":["Algorithm","Cluster analysis","Data point","Embedding","Generalization (Psychology)","Kernel","Kernel principal component analysis","Laplacian matrix","Natural mapping (interface design)","Nonlinear dimensionality reduction","Smoothing","Spectral clustering","algorithm","statistical cluster"],"vec":[-0.430784359,0.5185926506],"nodes":["1751762","1724875","39651651"]},"74225938608ccf6a825df576d4dfe2fca9726200":{"id":"74225938608ccf6a825df576d4dfe2fca9726200","date":"2014-01-01T12:12:00Z","text":"Deep Neural Networks (DNNs) are often successful in problems needing to extract information from complexe, high-dimensional inputs, for which useful features are not obvious to design. This paper presents our work on applying DNNs to brain tumor segmentation for the BRATS challenge. We are currently experimenting with several several DNN architectures, leveraging the recent advances in the field such as convolutional layers, max pooling, Maxout units and Dropout regularization. We present preliminary results, for our best performing network on the BRATS2013 training set, leaderboard dataset and challenge dataset. The results are obtained from the evaluation tool available on the Virtual Skeleton database. While we do not beat the best results of BRATS2013 participants with our current architecture, our results are promising.","category_a":"","category_b":"Others","keywords":["Architecture as Topic","Brain Neoplasms","Convolutional neural network","Dropout (neural networks)","Manuscripts","Matrix regularization","Neoplasms","Neural Networks","Neural Tube Defects","Silo (dataset)","Skeletal animation","Unit","anatomical layer","research study"],"vec":[-0.2549437141,0.0700430171],"nodes":["37634267","2203743","1923596","3058152","8888433","1687510","40632605","1777528","35490022","1751762"]},"245cd97fc34fad6d6e7efb373874e5a42576adfe":{"id":"245cd97fc34fad6d6e7efb373874e5a42576adfe","date":"2009-01-01T12:12:00Z","text":"User generated content and social media (in the form of blogs, wikis, online video, microblogs, etc) are proliferating online. Grapevine conducts large scale data analysis on the social media collective, distilling and extracting information in real time. It aims to track entities and stories of interest in millions of blog posts, thousands of tweets, news items, etc., daily. Grapevine facilitates the interactive exploration of content, allowing users to discover interesting or surprising stories, optionally narrowed down on a specific demographic of interest (e.g. \"What are Torontonians talking about on blogs?\", \"What are popular stories across news sources in Canada?\", \"What are financiers in Texas blogging about today?\"). Stories of interest can be explored in a variety of ways, such as modifying their scope, obtaining related content (blog posts, news, etc), and examining their temporal evolution.","category_a":"SIGMOD","category_b":"DB","keywords":["Blog","Information extraction","Social media","User-generated content","Video clip","Wiki"],"vec":[0.4227512882,-0.5029375313],"nodes":["1761975","1721062","1796624","1704011"]},"0c325c32039656541760b2d8f02be4636e026785":{"id":"0c325c32039656541760b2d8f02be4636e026785","date":"2009-01-01T12:12:00Z","text":"In CIDR 2009, we presented a collection of requirements for SciDB, a DBMS that would meet the needs of scientific users. These included a nested-array data model, sciencespecific operations such as regrid, and support for uncertainty, lineage, and named versions. In this paper, we present an overview of SciDB\u2019s key features and outline a demonstration of the first version of SciDB on data and operations from one of our lighthouse users, the Large Synoptic Survey Telescope (LSST).","category_a":"VLDB","category_b":"DB","keywords":["Classless Inter-Domain Routing","Data model","Database","SciDB"],"vec":[0.5104009739,-0.3018875085],"nodes":["1680925","32228637","34531502","38449161","2812902","1807578","2439661","2385473","1718134","1727443","1765659","35025968","1740962","2033016","2042232","1695715","2031287"]},"1c7647fd9ab65e46e821bb0e045535500e2584d4":{"id":"1c7647fd9ab65e46e821bb0e045535500e2584d4","date":"2001-01-01T12:12:00Z","text":"This paper presents a novel approach to the unsupervised learning of syntactic analyses of natural language text. Most previous work has focused on maximizing likelihood according to generative PCFG models. In contrast, we employ a simpler probabilistic model over trees based directly on constituent identity and linear context, and use an EM-like iterative procedure to induce structure. This method produces much higher quality analyses, giving the best published results on the ATIS dataset.","category_a":"NIPS","category_b":"ML","keywords":["Automatic Transmitter Identification System (television)","Grammar induction","Iterative method","Natural language","Statistical model","Stochastic context-free grammar","Unsupervised learning"],"vec":[-0.4323591278,-0.5742843273],"nodes":["38666915","1812612"]},"7e463877264e70d53c844cf4b1bf3b15baec8cfb":{"id":"7e463877264e70d53c844cf4b1bf3b15baec8cfb","date":"2015-01-01T12:12:00Z","text":"In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Benchmark (computing)","Convolutional neural network","Deep learning","MNIST database","Network architecture","Outline of object recognition","Recurrent neural network"],"vec":[-0.2450086985,0.1084972106],"nodes":["2077146","2182706","1979489","1745043","1760871","1751762"]},"6b570069f14c7588e066f7138e1f21af59d62e61":{"id":"6b570069f14c7588e066f7138e1f21af59d62e61","date":"2016-01-01T12:12:00Z","text":"Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.","category_a":"ArXiv","category_b":"Journal","keywords":["Compiler","Computation","Graphics processing unit","Machine learning","Python","TensorFlow","Theano (software)"],"vec":[-0.4432662578,0.333465187],"nodes":["2556436","1815021","2634674","2414348","3335364","2482072","3227028","2636539","31337499","7330729","1751762","40128194","32837403","8089133","32308836","14362225","2488222","2900675","2346028","1967465","8883973","1979489","2292403","29848635","2348758","40638665","39977229","1760871","2921469","2460212","2698759","2755582","2153749","35094433","2812151","3074927","3127597","1761978","40557013","2345617","39844381","3119801","34740554","1865055","1854385","39271617","8498339","18131002","2507883","25056820","1917170","34564969","38907455","3025583","33454496","2059369","3087941","3218143","40201308","3947169","2582494","2387233","1947598","3146592","3245814","40532172","7286257","40640545","1798462","3422889","1914552","1710604","3158246","1748421","1778734","40479190","1972076","1996134","2719055","2402716","2132219","3146111","3995639","37201953","36950882","3373139","1875295","2311943","1874218","40116153","35224828","1862138","3197429","39442397","11115628","2588725","3407592","7783188","3220768","34999784","19555508","1724875","2077146","1952531","1923596","27358391","39561601","36303818","2954189","1685369","35097114","1720117"]},"c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d":{"id":"c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d","date":"2014-01-01T12:12:00Z","text":"Many state-of-the-art results obtained with deep networks are achieved with the largest models that could be trained, and if more computation power was available, we might be able to exploit much larger datasets in order to improve generalization ability. Whereas in learning algorithms such as decision trees the ratio of capacity (e.g., the number of parameters) to computation is very favorable (up to exponentially more parameters than computation), the ratio is essentially 1 for deep neural networks. Conditional computation has been proposed as a way to increase the capacity of a deep neural network without increasing the amount of computation required, by activating some parameters and computation \u201con-demand\u201d, on a per-example basis. In this note, we propose a novel parametrization of weight matrices in neural networks which has the potential to increase up to exponentially the ratio of the number of parameters to computation. The proposed approach is based on turning on some parameters (weight matrices) when specific bit patterns of hidden unit activations are obtained. In order to better control for the overfitting that might result, we propose a parametrization that is tree-structured, where each node of the tree corresponds to a prefix of a sequence of sign bits, or gating units, associated with hidden units. 1 Conditional Computation for Deep Nets Deep learning is about learning hierarchically-organized representations, with higher levels corresponding to more abstract concepts automatically learned from data, either in a supervised, unsupervised, semi-supervised way, or via reinforcement learning (Mnih et al., 2013). See Bengio et al. (2013b) for a recent review. There have been a number of breakthroughs in the application of deep learning, e.g., in speech (Hinton et al., 2012a) and computer vision (Krizhevsky et al., 2012). Most of these involve deep neural networks that have as much capacity (the number of units and parameters) as possible, given the constraints on training and test time that made these experiments reasonably feasible. It has recently been reported that bigger models could yield better generalization on a number of datasets (Coates et al., 2011; Hinton et al., 2012b; Krizhevsky et al., 2012; Goodfellow et al., 2013) provided appropriate regularization such as dropout (Hinton et al., 2012b) is used. These experiments however have generally been limited by training time in which the amount of training data that could be exploited. An important factor in these recent breakthroughs has been the availability of GPUs which have allowed training deep nets at least 10 times faster, often more (Raina et al., 2009). However, whereas the task of recognizing handwritten digits, traffic signs (Ciresan et al., 2012) or faces (?) is solved to the point of achieving roughly human-level performance, this is far from true for other tasks","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Artificial neural network","Computation","Computer vision","Decision tree","Deep learning","Dropout (neural networks)","Experiment","Exploit (computer security)","Graphics processing unit","Machine learning","Matrix regularization","Overfitting","Reinforcement learning","Semi-supervised learning","Supervised learning","Test set"],"vec":[-0.3534945667,0.6077853157],"nodes":["1979489","1751762"]},"12806c298e01083a79db77927530367d85939907":{"id":"12806c298e01083a79db77927530367d85939907","date":"2015-01-01T12:12:00Z","text":"Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. In this paper, we presented a number of empirical evaluations of recent deep learning advances. Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Artificial neural network","Autonomous car","Computer vision","Convolutional neural network","Deep learning","Real-time computing"],"vec":[-0.3778803656,-0.2111507516],"nodes":["2570381","1685072","34947630","2512747","3117966","1896859","1906895","2706258","2178889","2221840","2204860","5574038","1701538"]},"0bb7e5432be8ae6799eddf88d35e525b4b1744f7":{"id":"0bb7e5432be8ae6799eddf88d35e525b4b1744f7","date":"2016-01-01T12:12:00Z","text":"Over the last several years, a great deal of progress has been made in the area of stream-processing engines (SPEs) [9, 11, 17]. Three basic tenets distinguish SPEs from current data processing engines. First, they must support primitives for streaming applications. Unlike Online Transaction Processing (OLTP), which processes messages in isolation, streaming applications entail time series operations on streams of messages. Although a time series \u201cblade\u201d was added to the Illustra Object-Relational DBMS, generally speaking, time series operations are not well supported by current DBMSs. Second, streaming applications entail a real-time component. If one is content to see an answer later, then one can store incoming messages in a data warehouse and run a historical query on the warehouse to find information of interest. This tactic does not work if the answer must be constructed in real time. The need for real-time answers also dictates a fundamentally different storage architecture. DBMSs universally store and index data records before making them available for query activity. Such outbound processing, where data are stored before being processed, cannot deliver real-time latency, as required by SPEs. To meet more stringent latency requirements, SPEs must adopt an alternate model, which we refer to as \u201cinbound processing\u201d, where query processing is performed","category_a":"Data Stream Management","category_b":"Others","keywords":["Aurora","Database","Illustra","Inbound marketing","Online transaction processing","Outbound laptop","Real-time computing","Requirement","Stream processing","Time series","Transaction processing"],"vec":[0.7904547751,-0.6757593122],"nodes":["2109957","2254232","1841612","1712771","1718134","31987589","3164984","2033016","2523666","6850164","2583998","1695715","1773620","36883488","2031287"]},"146f6f6ed688c905fb6e346ad02332efd5464616":{"id":"146f6f6ed688c905fb6e346ad02332efd5464616","date":"2015-01-01T12:12:00Z","text":"Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.","category_a":"ICML","category_b":"ML","keywords":["Backpropagation","Benchmark (computing)","Calculus of variations","Machine translation","Object detection"],"vec":[-0.35919001,-0.1011072162],"nodes":["36303818","2503659","3450996","1979489","1760871","1776908","1804104","1751762"]},"db8c3cfaae04a14c1209d62953029b6fa53e23c7":{"id":"db8c3cfaae04a14c1209d62953029b6fa53e23c7","date":"2013-01-01T12:12:00Z","text":"The ICML 2013 Workshop on Challenges in Representation Learning(1) focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.","category_a":"ICONIP","category_b":"Others","keywords":["Black box","Creation","International Conference on Machine Learning","Machine learning","Multimodal learning"],"vec":[-0.2194593276,0.0725253891],"nodes":["34740554","1761978","8883973","1760871","1778734","3033919","3155742","34312504","1859336","2738638","34872128","1764124","39825530","2462591","1680068","19998730","1792322","2449832","2286095","1817759","1721557","2599036","32837403","2191189","1743912","30139116","3092897","1751762"]},"6de2b1058c5b717878cce4e7e50d3a372cc4aaa6":{"id":"6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","date":"2014-01-01T12:12:00Z","text":"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 12 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.","category_a":"NIPS","category_b":"ML","keywords":["Approximation algorithm","Backpropagation","Discriminative model","Experiment","Generative model","Markov chain","Markov decision process","Minimax","Multilayer perceptron","Perceptron","Test set"],"vec":[-0.2421867491,0.8691528267],"nodes":["34740554","2409581","1778734","30139116","1923596","1955694","1760871","1751762"]},"946232a2fe2549a2d2f52612b09ed8d66a74f34b":{"id":"946232a2fe2549a2d2f52612b09ed8d66a74f34b","date":"2009-01-01T12:12:00Z","text":"In this paper we present an algorithm for finding the k highest-ranked (or Top-k) answers in a distributed network. A Top-K query returns the subset of most relevant answers, in place of all answers, for two reasons: i) to minimize the cost metric that is associated with the retrieval of all answers; and ii) to improve the recall and the precision of the answer-set, such that the user is not overwhelmed with irrelevant results. Our study focuses on multi-hop distributed networks in which the data is accessible by traversing a network of nodes. Such a setting captures very well the computation framework of emerging Sensor Networks, Peer-to-Peer Networks and Vehicular Networks. We present the Threshold Join Algorithm (TJA), an efficient algorithm that utilizes a non-uniform threshold on the queried attribute in order to minimize the transfer of data when a query is executed. Additionally, TJA resolves queries in the network rather than in a centralized fashion which further minimizes the consumption of bandwidth and delay. We performed an extensive experimental evaluation of our algorithm using a real testbed of 75 workstations along with a trace-driven experimental methodology. Our results indicate that TJA requires an order of magnitude less communication than the state-of-the-art, scales well with respect to the parameter k and the network topology.","category_a":"Computer Networks","category_b":"Others","keywords":["Algorithm","Centralisation","Computation","Join (SQL)","Network topology","Relevance","Testbed","Workstation"],"vec":[0.7628018394,0.0141022425],"nodes":["2605417","3104475","1685532","1736832","1761528","1699100","1721062","1704011"]},"157852dd14a9b518bf62fa6511be0f02f5d04a79":{"id":"157852dd14a9b518bf62fa6511be0f02f5d04a79","date":"2006-01-01T12:12:00Z","text":"This tutorial provides a comprehensive and cohesive overview of the key research results in the area of record linkage methodologies and algorithms for identifying approximate duplicate records, and available tools for this purpose. It encompasses techniques introduced in several communities including databases, information retrieval, statistics and machine learning. It aims to identify similarities and differences across the techniques as well as their merits and limitations.","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","Approximation algorithm","Database","Information retrieval","Linkage (software)","Machine learning"],"vec":[0.4142290098,-0.1122634766],"nodes":["1721062","1770124","1704011"]},"2f87364ce9255e345b35d431b00ba7271329cba8":{"id":"2f87364ce9255e345b35d431b00ba7271329cba8","date":"2009-01-01T12:12:00Z","text":"Current evaluation metrics for machine translation have increasing difficulty in distinguishing good from merely fair translations. We believe the main problem to be their inability to properly capture meaning: A good translation candidate means the same thing as the reference translation, regardless of formulation. We propose a metric that assesses the quality of MT output through its semantic equivalence to the reference translation, based on a rich set of match and mismatch features motivated by textual entailment. We first evaluate this metric in an evaluation setting against a combination metric of four state-of-the-art scores. Our metric predicts human judgments better than the combination metric. Combining the entailment and traditional features yields further improvements. Then, we demonstrate that the entailment metric can also be used as learning criterion in minimum error rate training (MERT) to improve parameter estimation in MT system training. A manual evaluation of the resulting translations indicates that the new model obtains a significant improvement in translation quality.","category_a":"Machine Translation","category_b":"Others","keywords":["Eisenstein's criterion","Estimation theory","Machine translation","Multi-Environment Real-Time","Textual entailment"],"vec":[-1.0058134542,-0.235627874],"nodes":["1708581","3208422","1947267","1746807","1812612"]},"83b625ae40c921c47255da5f2e24266e75a48d9b":{"id":"83b625ae40c921c47255da5f2e24266e75a48d9b","date":"2010-01-01T12:12:00Z","text":"Alternating Gibbs sampling is the most common scheme used for sampling from Restricted Boltzmann Machines (RBM), a crucial component in deep architectures such as Deep Belief Networks. However, we find that it often does a very poor job of rendering the diversity of modes captured by the trained model. We suspect that this hinders the advantage that could in principle be brought by training algorithms relying on Gibbs sampling for uncovering spurious modes, such as the Persistent Contrastive Divergence algorithm. To alleviate this problem, we explore the use of tempered Markov Chain Monte-Carlo for sampling in RBMs. We find both through visualization of samples and measures of likelihood that it helps both sampling and learning.","category_a":"AISTATS","category_b":"ML","keywords":["Algorithm","Bayesian network","Deep learning","Gibbs sampling","Kullback\u2013Leibler divergence","Markov chain","Markov chain Monte Carlo","Monte Carlo","Monte Carlo method","Restricted Boltzmann machine","Sampling (signal processing)"],"vec":[-0.2552169336,0.5736903843],"nodes":["2755582","1760871","1751762","1724875","2460212"]},"517c31e5390d1d743aca69d16098be6ca30ebd2d":{"id":"517c31e5390d1d743aca69d16098be6ca30ebd2d","date":"2012-01-01T12:12:00Z","text":"The deep Boltzmann machine (DBM) has been an important development in the quest for powerful \u201cdeep\u201d probabilistic models. To date, simultaneous or joint training of all layers of the DBM has been largely unsuccessful with existing training methods. We introduce a simple regularization scheme that encourages the weight vectors associated with each hidden unit to have similar norms. We demonstrate that this regularization can be easily combined with standard stochastic maximum likelihood to yield an effective training strategy for the simultaneous training of all layers of the deep Boltzmann machine.","category_a":"ArXiv","category_b":"Journal","keywords":["Boltzmann machine","Decibel","Matrix regularization"],"vec":[-0.3455753467,0.3302712538],"nodes":["2755582","1760871","1751762"]},"6f568d757d2c1ab42f2006faa25690b74c3d2d44":{"id":"6f568d757d2c1ab42f2006faa25690b74c3d2d44","date":"2011-01-01T12:12:00Z","text":"While vector quantization (VQ) has been applied widely to generate features for visual recognition problems, much recent work has focused on more powerful methods. In particular, sparse coding has emerged as a strong alternative to traditional VQ approaches and has been shown to achieve consistently higher performance on benchmark datasets. Both approaches can be split into a training phase, where the system learns a dictionary of basis functions, and an encoding phase, where the dictionary is used to extract features from new inputs. In this work, we investigate the reasons for the success of sparse coding over VQ by decoupling these phases, allowing us to separate out the contributions of training and encoding in a controlled way. Through extensive experiments on CIFAR, NORB and Caltech 101 datasets, we compare several training and encoding schemes, including sparse coding and a form of VQ with a soft threshold activation function. Our results show not only that we can use fast VQ algorithms for training, but that we can just as well use randomly chosen exemplars from the training set. Rather than spend resources on training, we find it is more important to choose a good encoder\u2014which can often be a simple feed forward non-linearity. Our results include state-of-the-art performance on both CIFAR and NORB.","category_a":"ICML","category_b":"ML","keywords":["Activation function","Algorithm","Basis function","Benchmark (computing)","Caltech 101","Decoupling (electronics)","Dictionary","Encoder","Experiment","Feed forward (control)","Neural coding","Nonlinear system","Sparse matrix","Test set","Vector quantization"],"vec":[-0.4151652562,0.1827691692],"nodes":["5574038","1701538"]},"22825681ceb3f065098eb71ca9346b458b566252":{"id":"22825681ceb3f065098eb71ca9346b458b566252","date":"2010-01-01T12:12:00Z","text":"Translation systems are generally trained to optimize BLEU, but many alternative metrics are available. We explore how optimizing toward various automatic evaluation metrics (BLEU, METEOR, NIST, TER) affects the resulting model. We train a state-of-the-art MT system using MERT on many parameterizations of each metric and evaluate the resulting models on the other metrics and also using human judges. In accordance with popular wisdom, we find that it\u2019s important to train on the same metric used in testing. However, we also find that training to a newer metric is only useful to the extent that the MT model\u2019s structure and features allow it to take advantage of the metric. Contrasting with TER\u2019s good correlation with human judgments, we show that people tend to prefer BLEU and NIST trained models to those trained on edit distance based metrics like TER or WER. Human preferences for METEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training.","category_a":"HLT-NAACL","category_b":"ComLing","keywords":["BLEU","Edit distance","Evaluation of machine translation","METEOR","Meteor","Multi-Environment Real-Time","Word error rate"],"vec":[-0.9796062295,-0.2167310212],"nodes":["3208422","1812612","1746807"]},"01ad77475dedda66e3961f707a19fe588ef48f39":{"id":"01ad77475dedda66e3961f707a19fe588ef48f39","date":"2016-01-01T12:12:00Z","text":"In this paper, we show a direct equivalence between spectral clustering and kernel PCA, and how both are special cases of a more general learning problem, that of learning the principal eigenfunctions of a kernel, when the functions are from a Hilbert space whose inner product is defined with respect to a density model. This suggests a new approach to unsupervised learning in which abstractions (such as manifolds and clusters) that represent the main features of the data density are extracted. Abstractions discovered at one level can be used to build higher-level abstractions. This paper also discusses how these abstractions can be used to recover a quantitative model of the input density, which is at least useful for evaluative and comparative purposes.","category_a":"","category_b":"Others","keywords":["Cluster analysis","Hilbert space","Kernel","Kernel principal component analysis","Spectral clustering","Unsupervised learning","statistical cluster"],"vec":[-0.4409865085,0.4964215841],"nodes":["1751762","1724875","39651651"]},"8a9a10170ee907acb3e582742bec5fa09116f302":{"id":"8a9a10170ee907acb3e582742bec5fa09116f302","date":"2011-01-01T12:12:00Z","text":"We combine three important ideas present in previous work for building classifiers: the semi-supervised hypothesis (the input distribution contains information about the classifier), the unsupervised manifold hypothesis (data density concentrates near low-dimensional manifolds), and the manifold hypothesis for classification (different classes correspond to disjoint manifolds separated by low density). We exploit a novel algorithm for capturing manifold structure (high-order contractive auto-encoders) and we show how it builds a topological atlas of charts, each chart being characterized by the principal singular vectors of the Jacobian of a representation mapping. This representation learning algorithm can be stacked to yield a deep architecture, and we combine it with a domain knowledge-free version of the TangentProp algorithm to encourage the classifier to be insensitive to local directions changes along the manifold. Record-breaking classification results are obtained.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Encoder","Feature learning","Jacobian matrix and determinant","Machine learning","Nautical chart","Semi-supervised learning","Singular value decomposition"],"vec":[-0.5279578519,0.4537787827],"nodes":["2425018","2921469","1724875","1751762","3012669"]},"a5eb54e8a30406ac723ba258111239e156e25523":{"id":"a5eb54e8a30406ac723ba258111239e156e25523","date":"2010-01-01T12:12:00Z","text":"Alternating Gibbs sampling between visible and latent units is the most common scheme used for sampling from Restricted Boltzmann Machines (RBM), a crucial component in deep architectures such as Deep Belief Networks (DBN). However, we find that it often does a very poor job of rendering the diversity of modes captured by the trained model. We suspect that this property hinders RBM training methods such as the Contrastive Divergence and Persistent Contrastive Divergence algorithm that rely on Gibbs sampling to approximate the likelihood gradient. To alleviate this problem, we explore the use of tempered Markov Chain Monte-Carlo for sampling in RBMs. We find both through visualization of samples and measures of likelihood on a toy dataset that it helps both sampling and learning.","category_a":"","category_b":"Others","keywords":["Algorithm","Approximation algorithm","Architecture as Topic","Bayesian network","Deep learning","Gibbs sampling","Gradient","Gradient","Imagery","Kullback\u2013Leibler divergence","Markov chain","Markov chain Monte Carlo","Monte Carlo method","Parallel tempering","Restricted Boltzmann machine","Sampling (signal processing)","Sampling - Surgical action","Unit","algorithm"],"vec":[-0.2496213176,0.5675883177],"nodes":["2755582","40632605","1751762","1724875","2460212"]},"d8c11242130cc02c9f9b7a9506c303ab05e2daea":{"id":"d8c11242130cc02c9f9b7a9506c303ab05e2daea","date":"2003-01-01T12:12:00Z","text":"Measuring and monitoring complex, dynamic phenomena \u2013 traffic evolution in internet and telephone communication infrastructures, usage of the web, email and newsgroups, movement of financial markets, atmospheric conditions \u2013 produces highly detailed stream data, i.e., data that arrives as a series of \u201cobservations\u201d, often very rapidly. With traditional data feeds, one modifies and augments underlying databases and data warehouses: complex queries over the data are performed in an offline fashion, and real time queries are typically restricted to simple filters. However, the monitoring applications that operate on modern data streams require sophisticated real time queries (often in an exploratory mode) to identify, e.g., unusual\/anomalous activity (such as network intrusion detection or telecom fraud detection), based on intricate relationships between the values of the underlying data streams. Stream data are also generated naturally by (messagebased) web services, in which loosely coupled systems interact by exchanging high volumes of business data (e.g., purchase orders, retail transactions) tagged in XML (the lingua franca of web services), forming continuous XML data streams. A central aspect of web services is the ability to efficiently operate on these XML data streams executing queries (expressed in some XML query language) to continuously match, extract and transform parts of the XML data stream to drive legacy back-end business applications. Manipulating stream data presents many technical challenges which are just beginning to be addressed in the database, systems, algorithms, networking and other computer science communities. This is an active research area in the database community, involving new stream operators, SQL extensions, query optimization methods, operator scheduling techniques, etc., with the goal of developing general-purpose (e.g., NiagaraCQ, Stanford Stream, Telegraph, Aurora) and specialized (e.g., Gigascope) data","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Aurora","Computer science","Data breach","Database","Email","Exploratory testing","Intrusion detection system","Loose coupling","Program optimization","Query language","Query optimization","SQL","Scheduling (computing)","Value (ethics)","Web service","XML"],"vec":[1.0300909017,0.0903797376],"nodes":["1721062","1704011"]},"40e6f827c72158e4b0a7dcc20f18448b4dd7e042":{"id":"40e6f827c72158e4b0a7dcc20f18448b4dd7e042","date":"2008-01-01T12:12:00Z","text":"Monitoring aggregates on network traffic streams is a compelling application of data stream management systems. Often, streaming aggregation queries involve joining multiple inputs (e.g., client requests and server responses) using temporal join conditions (e.g., within 5 seconds), followed by computation of aggregates (e.g., COUNT) over temporal windows (e.g., every 5 minutes). These types of queries help identify malfunctioning servers (missing responses), malicious clients (bursts of requests during a denial-of-service attack), or improperly configured protocols (short timeout intervals causing many retransmissions). However, while such query expression is natural, its evaluation over massive data streams is inefficient.\n In this paper, we develop rewriting techniques for streaming aggregation queries that join multiple inputs. Our techniques identify conditions under which expensive joins can be optimized away, while providing error bounds for the results of the rewritten queries. The basis of the optimization is a powerful but decidable theory in which constraints over data streams can be formulated. We show the efficiency and accuracy of our solutions via experimental evaluation on real-life IP network data using the Gigascope stream processing engine.","category_a":"SSPS","category_b":"Others","keywords":["Computation","Denial-of-service attack","Microsoft Windows","Network traffic control","Program optimization","Real life","Rewriting","Server (computing)","Stream processing","Streaming media"],"vec":[0.9968961379,0.1231189381],"nodes":["1688834","1685257","1721062","1704011","1725663"]},"2185c1d1e465dd443b7ccb47c9b2d520da65a319":{"id":"2185c1d1e465dd443b7ccb47c9b2d520da65a319","date":"2005-01-01T12:12:00Z","text":"The ability to compute top-k matches to XML queries is gaining importance due to the increasing number of large XML repositories. The efficiency of top-k query evaluation relies on using scores to prune irrelevant answers as early as possible in the evaluation process. In this context, evaluating the same query plan for all answers might be too rigid because, at any time in the evaluation, answers have gone through the same number and sequence of operations, which limits the speed at which scores grow. Therefore, adaptive query processing that permits different plans for different partial matches and maximizes the best scores is more appropriate. In this paper, we propose an architecture and adaptive algorithms for efficiently computing top-k matches to XML queries. Our techniques can be used to evaluate both exact and approximate matches where approximation is defined by relaxing XPath axes. In order to compute the scores of query answers, we extend the traditional tf*idf measure to account for document structure. We conduct extensive experiments on a variety of benchmark data and queries, and demonstrate the usefulness of the adaptive approach for computing top-k queries in XML.","category_a":"ICDE","category_b":"DB","keywords":["Algorithm","Approximation algorithm","Benchmark (computing)","Database","Experiment","Query plan","Relevance","Tf\u2013idf","Utility","XML","XPath"],"vec":[0.9855825893,0.1089136772],"nodes":["2394972","1736846","1721062","1704011"]},"31b58ddd4a558f7d9e41e19ef75993668ee69119":{"id":"31b58ddd4a558f7d9e41e19ef75993668ee69119","date":"2012-01-01T12:12:00Z","text":"Differential privacy has recently emerged as the de facto standard for private data release. This makes it possible to provide strong theoretical guarantees on the privacy and utility of released data. While it is well-understood how to release data based on counts and simple functions under this guarantee, it remains to provide general purpose techniques to release data that is useful for a variety of queries. In this paper, we focus on spatial data such as locations and more generally any multi-dimensional data that can be indexed by a tree structure. Directly applying existing differential privacy methods to this type of data simply generates noise. We propose instead the class of \"private spatial decompositions'': these adapt standard spatial indexing methods such as quad trees and kd-trees to provide a private description of the data distribution. Equipping such structures with differential privacy requires several steps to ensure that they provide meaningful privacy guarantees. Various basic steps, such as choosing splitting points and describing the distribution of points within a region, must be done privately, and the guarantees of the different building blocks composed to provide an overall guarantee. Consequently, we expose the design space for private spatial decompositions, and analyze some key examples. A major contribution of our work is to provide new techniques for parameter setting and post-processing the output to improve the accuracy of query answers. Our experimental study demonstrates that it is possible to build such decompositions efficiently, and use them to answer a variety of queries privately with high accuracy.","category_a":"Data Engineering","category_b":"DB","keywords":["Differential privacy","Experiment","Information privacy","Privacy","Private network","Tree structure","Video post-processing","Virtual private network"],"vec":[1.0225987018,0.6212970986],"nodes":["1709589","1738196","1704011","39659954","1689202"]},"3fa58a0dcbf7392e02f744a0f729ba8010b7334c":{"id":"3fa58a0dcbf7392e02f744a0f729ba8010b7334c","date":"2003-01-01T12:12:00Z","text":"We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.","category_a":"HLT-NAACL","category_b":"ComLing","keywords":["Brill tagger","Part-of-speech tagging","Poisson regression","Tag (game)","The Wall Street Journal","Treebank"],"vec":[-0.5177716095,-0.5566404734],"nodes":["3259253","38666915","1812612","1740765"]},"533ee188324b833e059cb59b654e6160776d5812":{"id":"533ee188324b833e059cb59b654e6160776d5812","date":"2013-01-01T12:12:00Z","text":"In this paper, we explore different ways to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-tohidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs benefit from the depth and outperform the conventional, shallow RNNs.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Deep learning","Feedforward neural network","Language model","Neural Networks","Recurrent neural network"],"vec":[-0.4811602314,0.4949834861],"nodes":["1996134","1854385","1979489","1751762"]},"195d0a8233a7a46329c742eaff56c276f847fadc":{"id":"195d0a8233a7a46329c742eaff56c276f847fadc","date":"2011-01-01T12:12:00Z","text":"We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized autoencoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining. Appearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author(s)\/owner(s).","category_a":"ICML","category_b":"ML","keywords":["Encoder","Feature extraction","International Conference on Machine Learning","Jacobian matrix and determinant","Loss function","Machine learning","Memory-level parallelism","Noise reduction","Nonlinear system"],"vec":[-0.5638643216,0.7306158065],"nodes":["2425018","1724875","3012669","3119801","1751762"]},"1e185688f3c5e73641633459dbc17b30d34fd307":{"id":"1e185688f3c5e73641633459dbc17b30d34fd307","date":"2017-01-01T12:12:00Z","text":"Four key subprocesses in data integration are: data preparation (i.e., transforming and cleaning data), schema integration (i.e., lining up like attributes), entity resolution (i.e., finding clusters of records that represent the same entity) and entity consolidation (i.e., merging each cluster into a \u201cgolden record\u201d which contains the canonical values for each attribute). In real scenarios, the output of entity resolution typically contains multiple data formats and different abbreviations for cell values, in addition to the omnipresent problem of missing data. These issues make entity consolidation challenging. In this paper, we study the entity consolidation problem. Truth discovery systems can be used to solve this problem. They usually employ simplistic heuristics such as majority consensus (MC) or source authority to determine the golden record. However, these techniques are not capable of recognizing simple data variation, such as Jeff \u2194 Jeffery, and may give biased results. To address this issue, we propose to first reduce attribute variation by merging duplicate values before applying the truth discovery system to create the golden records. For this purpose, we first align the attribute values within the same cluster to generate candidate matchings (substring pairs could be replaced by each other, e.g., 9th\u2194 9 and Jeff\u2194 Jeffery). Then we aggregate candidate matchings with common characteristics into groups. Finally, we solicit a human to validate these matching groups and apply the approved ones to merge duplicate values. Comparing to the existing data transformation solutions, which typically try to transform an entire column from one format to another, our approach is more robust to data variety as we leverage the hidden matchings within the clusters. We tried our methods on three real world datasets. In the best case, our methods reduced the variation in clusters by 75% with high precision (>98%) by having a human confirm only 100 generated matching groups. When we invoked our algorithm prior to running MC, we were able to improve the precision of golden record creation by 40%. This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Proceedings of the VLDB Endowment, Vol. 10, No. 6 Copyright 2017 VLDB Endowment 2150-8097\/17\/02.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Attribute-value system","Best, worst and average case","Discovery system","Heuristic","Information system","Matching (graph theory)","Missing data","Semiconductor consolidation","Sputter cleaning","Substring","VLDB"],"vec":[0.9339619726,0.0502696395],"nodes":["38737063","2893500","2034349","1740095","1743316","2033016","2168047","1695715","8669763"]},"41f50d08e4c237d0f192f5c09f78d7e1d09d9cef":{"id":"41f50d08e4c237d0f192f5c09f78d7e1d09d9cef","date":"2013-01-01T12:12:00Z","text":"We describe the use of two spike-and-slab models for modeling real-valued data, with an emphasis on their applications to object recognition. The first model, which we call spike-and-slab sparse coding (S3C), is a preexisting model for which we introduce a faster approximate inference algorithm. We introduce a deep variant of S3C, which we call the partially directed deep Boltzmann machine (PD-DBM) and extend our S3C inference algorithm for use on this model. We describe learning procedures for each. We demonstrate that our inference procedure for S3C enables scaling the model to unprecedented large problem sizes, and demonstrate that using S3C as a feature extractor results in very good object recognition performance, particularly when the number of labeled examples is low. We show that the PD-DBM generates better samples than its shallow counterpart, and that unlike DBMs or DBNs, the PD-DBM may be trained successfully without greedy layerwise training.","category_a":"Pattern Analysis and Machine Intelligence","category_b":"Others","keywords":["Algorithm","Approximation algorithm","Boltzmann machine","Decibel","Deep belief network","Extractor (mathematics)","Extractors","Feature learning","Greedy algorithm","Inference","Man-Machine Systems","Mitobronitol","Multidimensional scaling","Neural coding","Outline of object recognition","Parkinson Disease","Slab allocation","Sparse matrix","Tracer","algorithm"],"vec":[-0.4806473617,0.5526939366],"nodes":["34740554","1760871","1751762"]},"d0f04a8e3a21b0142f46067b98530ec833672e93":{"id":"d0f04a8e3a21b0142f46067b98530ec833672e93","date":"2002-01-01T12:12:00Z","text":"","category_a":"VLDB","category_b":"DB","keywords":["Access control","Accessibility","Web accessibility","XML"],"vec":[-0.1919626437,-0.0128845117],"nodes":["1689202","1704011","1708593","1735239"]},"aeb38c8c4b826ad0dc63911dc5198f8c565298f5":{"id":"aeb38c8c4b826ad0dc63911dc5198f8c565298f5","date":"2012-01-01T12:12:00Z","text":"We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classification tasks. 1 Deep Boltzmann machines A deep Boltzmann machine (Salakhutdinov and Hinton, 2009) is a probabilistic model consisting of many layers of random variables, most of which are latent. Typically, a DBM contains a set of D input features v that are called the visible units because they are always observed during both training and evaluation. The DBM is usually applied to classification problems and thus often represents the class label with a one-of-k code in the form of a discrete-valued label unit y. y is observed (on examples for which it is available) during training. The DBM also contains several hidden units, which are usually organized into L layers h of size Ni, i = 1, . . . , L,with each unit in a layer conditionally independent of the other units in the layer given the neighboring layers. These conditional independence properties allow fast Gibbs sampling because an entire layer of units can be sampled at a time. Likewise, mean field inference with fixed point equations is fast because each fixed point equation gives a solution to an entire layer of variational parameters. A DBM defines a probability distribution by exponentiating and normalizing an energy function P (v, h, y) = 1 Z exp (\u2212E(v, h, y)) Preliminary work presented to Bruno Olshausen\u2019s lab and Google Brain, December 2012. where Z = \u2211 v,h,y exp (\u2212E(v\u2032, h\u2032, y\u2032)) . Z, the partition function, is intractable, due to the summation over all possible states. Maximum likelihood learning requires computing the gradient of logZ. Fortunately, the gradient can be estimated using an MCMC procedure (Younes, 1999; Tieleman, 2008). Block Gibbs sampling of the layers makes this procedure efficient. The structure of the interactions in h determines whether further approximations are necessary. In the pathological case where every element of h is conditionally independent of the others given the visible units, the DBM is simply an RBM and logZ is the only intractable term of the log likelihood. In the general case, interactions between different elements of h render the posterior P (h | v, y) intractable. Salakhutdinov and Hinton (2009) overcome this by maximizing the lower bound on the log likelihood given by the mean field approximation to the posterior rather than maximizing the log likelihood itself. Again, block mean field inference over the layers makes this procedure efficient. An interesting property of the DBM is that the training procedure thus involves feedback connections between the layers. Consider the simple DBM consisting of all binary valued units, with the energy function E(v, h) = \u2212vW h \u2212 hW h. Approximate inference in this model involves repeatedly applying two fixed-point update equations to solve for the mean field approximation to the posterior. Essentially it involves running a recurrent net in order to obtain approximate expectations of the latent variables. Beyond their theoretical appeal as a deep model that admits simultaneous training of all components using a generative cost, DBMs have achieved excellent performance in practice. When they were first introduced, DBMs set the state of the art on the permutationinvariant version of the MNIST handwritten digit Manuscript under review by AISTATS 2013 recognition task at 0.95. (By permutation-invariant, we mean that permuting all of the input pixels prior to learning the network should not cause a change in performance, so using synthetic image distortions or convolution to engineer knowledge about the structure of the images into the system is not allowed). Recently, new techniques were used in conjunction with DBM pretraining to set a new state of the art of 0.79 % test error (Hinton et al., 2012). 2 The joint training problem Unfortunately, it is not possible to train a deep Boltzmann machine using only the varational bound and approximate gradient described above. Salakhutdinov and Hinton (2009) found that instead it must be trained one layer at a time, where each layer is trained as an RBM. The RBMs can then be modified slightly, assembled into a DBM, and the DBM may be trained with the learning rule described above. In this paper, we propose a method that enables the deep Boltzmann machine to be jointly trained.","category_a":"ArXiv","category_b":"Journal","keywords":["Approximation","Approximation algorithm","Boltzmann machine","Calculus of variations","Convolution","Dbm","Decibel","Deep learning","Distortion","EXPTIME","Feedback","Fixed point (mathematics)","Gibbs sampling","Google Brain","Gradient","Greedy algorithm","Interaction","Latent variable","Learning rule","MNIST database","Markov chain Monte Carlo","Mathematical optimization","PC Bruno","Partition function (mathematics)","Pixel","Restricted Boltzmann machine","Sampling (signal processing)","Statistical model","Synthetic data","Variational method (quantum mechanics)"],"vec":[-0.4699069683,0.7666871409],"nodes":["34740554","1760871","1751762"]},"441a89d393eba5091afe76fbaa7c35cbac1c31dc":{"id":"441a89d393eba5091afe76fbaa7c35cbac1c31dc","date":"2004-01-01T12:12:00Z","text":"Data Cleaning is an important process that has been at the center of research interest in recent years. Poor data quality is the result of a variety of reasons, including data entry errors and multiple conventions for recording database fields, and has a significant impact on a variety of business issues. Hence, there is a pressing need for technologies that enable flexible (fuzzy) matching of string information in a database. Cosine similarity with tf-idf is a well-established metric for comparing text, and recent proposals have adapted this similarity measure for flexibly matching a query string with values in a single attribute of a relation. In deploying tf-idf based flexible string matching against real AT&T databases, we observed that this technique needed to be enhanced in many ways. First, along the functionality dimension, where there was a need to flexibly match along multiple string-valued attributes, and also take advantage of known semantic equivalences. Second, we identified various performance enhancements to speed up the matching process, potentially trading off a small degree of accuracy for substantial performance gains. In this paper, we report on our techniques and experience in dealing with flexible string matching against real AT&T databases.","category_a":"VLDB","category_b":"DB","keywords":["Cosine similarity","Data quality","Database","Fuzzy logic","Query string","Similarity measure","Sputter cleaning","String searching algorithm","Tf\u2013idf"],"vec":[1.0006006816,0.1048233547],"nodes":["1721062","2440762","1704011"]},"a9d41b228cd14532fcabb87d65739a8213f69299":{"id":"a9d41b228cd14532fcabb87d65739a8213f69299","date":"2010-01-01T12:12:00Z","text":"Data streams characterize the high speed and large volume input of a new class of applications such as network monitoring, web content analysis and sensor networks. Among these applications, network monitoring may be the most compelling one\u2014the backbone of a large internet service provider can generate 1 petabyte of data per day. For many network monitoring tasks such as traffic analysis and statistics collection, aggregation is a primitive operation. Various analytical and statistical needs naturally lead to related aggregate queries. In this article, we address the problem of efficiently computing multiple aggregations over high-speed data streams based on the two-level query processing architecture of GS, a real data stream management system deployed in AT & T. We discern that additionally computing and maintaining fine-granularity aggregations (called phantoms) has the benefit of supporting shared computation. Based on a thorough analysis, we propose algorithms to identify the best set of phantoms to maintain and determine allocation of resources (particularly, space) to compute the aggregations. Experiments show that our algorithm achieves near-optimal computation costs, which outperforms the best adapted algorithm by more than an order of magnitude.","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Computation","Database","Experiment","Internet backbone","Object composition","Petabyte","Traffic analysis","Web content"],"vec":[0.9791037155,0.1048700607],"nodes":["40482866","1721062","1693070","1704011","38089781"]},"0dbac3fa87f3288454bb1c5a24e97c9fa4401cbe":{"id":"0dbac3fa87f3288454bb1c5a24e97c9fa4401cbe","date":"2012-01-01T12:12:00Z","text":"XML is recognized as a standard for data storage and exchange for web applications. This is because it has certain unique features like it is self describing, extensible and it is stored in the form of text document. In spite of all these unique features XML has an inherent limitation of verbosity. Because of the strong presence of XML in database technology and its inherent verbosity there is ever increasing need to design compact storage for XML which can be effectively utilized for efficient indexing and querying of XML. The proposed technique creates a structure index which is a compact summarization of the XML document and data index which groups and stores the contents of all similar paths at one place. Based on this compact storage a novel query algorithm is proposed which can answer xpath queries very efficiently. This approach dramatically reduces the storage requirement for XML coupled with efficient processing of xpath queries. The implementation of this technique and comparison with other techniques confirms our claim.","category_a":"","category_b":"Others","keywords":["Algorithm","Computer data storage","Extensible Markup Language","Question (inquiry)","Web application","XML","XPath","algorithm"],"vec":[1.1131149291,0.048723635],"nodes":["10389704","2643158","3157876",0,"1721062","2042232","35682095","2707676","1971338","1774634","3252167",0,"12645620","4791284","32224308","1710965","2096611","1721062",0,"1704011"]},"704dfc1636911592c5ea02a198077d2586bb8d23":{"id":"704dfc1636911592c5ea02a198077d2586bb8d23","date":"2010-01-01T12:12:00Z","text":"The principle of anonymization for data sharing has become a very popular paradigm for the preservation of privacy of the data subjects. Since the introduction of k-anonymity, dozens of methods and enhanced privacy definitions have been proposed. However, over-eager attempts to minimize the information lost by the anonymization potentially allow private information to be inferred. Proof-ofconcept of this \u201cminimality attack\u201d has been demonstrated for a variety of algorithms and definitions [16]. In this paper, we provide a comprehensive analysis and study of this attack, and demonstrate that with care its effect can be almost entirely countered. The attack allows an adversary to increase his (probabilistic) belief in certain facts about individuals over the data. We show that (a) a large class of algorithms are not affected by this attack, (b) for a class of algorithms that have a \u201csymmetric\u201d property, the attacker\u2019s belief increases by at most a small constant, and (c) even for an algorithm chosen to be highly susceptible to the attack, the attacker\u2019s belief when using the attack increases by at most a small constant factor. We also provide a series of experiments that show in all these cases that the confidence about the sensitive value of any individual remains low in practice, while the published data is still useful for its intended purpose. From this, we conclude that the impact of such method-based attacks can be minimized.","category_a":"VLDB","category_b":"DB","keywords":["Adversary (cryptography)","Algorithm","Data anonymization","Denial-of-service attack","Experiment","Personally identifiable information"],"vec":[0.838676106,0.6302175656],"nodes":["1709589","1740882","2644293","1704011"]},"25f0625a92f6054b11057423111f9285c78376fe":{"id":"25f0625a92f6054b11057423111f9285c78376fe","date":"2014-01-01T12:12:00Z","text":"In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Experiment","Long short-term memory","Neural Networks","Recurrent neural network"],"vec":[-0.5179019601,0.5271809205],"nodes":["8270717","1854385","1979489","1751762"]},"14d904e10cca3f0cb6d9c623db1a50152cec6360":{"id":"14d904e10cca3f0cb6d9c623db1a50152cec6360","date":"2009-01-01T12:12:00Z","text":"The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an infinite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer defines a conditional factorial prior distribution over the binary latent variables of the layer below via a noisy-or mechanism. We explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment.","category_a":"NIPS","category_b":"ML","keywords":["Dynamic Bayesian network","Latent variable"],"vec":[-0.2240598372,0.2689284546],"nodes":["1760871","2396681","1751762"]},"7f917de89d395e7dba3f2c5cb54b4a13c06a8dee":{"id":"7f917de89d395e7dba3f2c5cb54b4a13c06a8dee","date":"2008-01-01T12:12:00Z","text":"We consider the problem of learning to follow a desired trajectory when given a small number of demonstrations from a sub-optimal expert. We present an algorithm that (i) extracts the---initially unknown---desired trajectory from the sub-optimal expert's demonstrations and (ii) learns a local model suitable for control along the learned trajectory. We apply our algorithm to the problem of autonomous helicopter flight. In all cases, the autonomous helicopter's performance exceeds that of our expert helicopter pilot's demonstrations. Even stronger, our results significantly extend the state-of-the-art in autonomous helicopter aerobatics. In particular, our results include the first autonomous tic-tocs, loops and hurricane, vastly superior performance on previously performed aerobatic maneuvers (such as in-place flips and rolls), and a complete airshow, which requires autonomous transitions between these and various other maneuvers.","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Autonomous car","In-place algorithm"],"vec":[-0.0304015354,-1.2621382965],"nodes":["5574038","1689992","1701538"]},"2d5069a99bfa0b47c095bbb5cefd6dba974f72a7":{"id":"2d5069a99bfa0b47c095bbb5cefd6dba974f72a7","date":"2017-01-01T12:12:00Z","text":"Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequencelevel settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in ngram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Language model","Machine translation","N-gram","Smoothing"],"vec":[-0.2552575524,0.5480997069],"nodes":["40548699","8729431","5183779","1771053","9925959","1746807","1701538"]},"422955d1f193e70bebb546b33d53564421c5085a":{"id":"422955d1f193e70bebb546b33d53564421c5085a","date":"2008-01-01T12:12:00Z","text":"Data collections often have inconsistencies that arise due to a variety of reasons, and it is desirable to be able to identify and resolve them efficiently. Set similarity queries are commonly used in data cleaning for matching similar data. In this work we concentrate on set similarity selection queries: Given a query set, retrieve all sets in a collection with similarity greater than some threshold. Various set similarity measures have been proposed in the past for data cleaning purposes. In this work we concentrate on weighted similarity functions like TF\/IDF, and introduce variants that are well suited for set similarity selections in a relational database context. These variants have special semantic properties that can be exploited to design very efficient index structures and algorithms for answering queries efficiently. We present modifications of existing technologies to work for set similarity selection queries. We also introduce three novel algorithms based on the Threshold Algorithm, that exploit the semantic properties of the new similarity measures to achieve the best performance in theory and practice.","category_a":"Data Engineering","category_b":"DB","keywords":["Algorithm","Relational database","Sputter cleaning","Tf\u2013idf"],"vec":[1.0773417596,0.0898123243],"nodes":["1836901","36206298","1721062","1704011"]},"373cf414cc038516a2cff11d7caafa3ff1031c6d":{"id":"373cf414cc038516a2cff11d7caafa3ff1031c6d","date":"2013-01-01T12:12:00Z","text":"Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying datagenerating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).","category_a":"NIPS","category_b":"ML","keywords":["Encoder","Generalized linear model","Markov chain Monte Carlo","Metropolis","Metropolis\u2013Hastings algorithm","Noise reduction","Perlin noise","Sampling (signal processing)"],"vec":[-0.2707586222,0.6696651843],"nodes":["1751762","1685369","1815021","1724875"]},"190b27eb221eec46b97373d80c5b96d95c4153c3":{"id":"190b27eb221eec46b97373d80c5b96d95c4153c3","date":"2012-01-01T12:12:00Z","text":"As data analytics becomes mainstream, and the complexity of the underlying data and computation grows, it will be increasingly important to provide tools that help analysts understand the underlying reasons when they encounter errors in the result. While data provenance has been a large step in providing tools to help debug complex workflows, its current form has limited utility when debugging aggregation operators that compute a single output from a large collection of inputs. Traditional provenance will return the entire input collection, which has very low precision. In contrast, users are seeking precise descriptions of the inputs that caused the errors. We propose a Ranked Provenance System, which identifies subsets of inputs that influenced the output error, describes each subset with human readable predicates and orders them by contribution to the error. In this demonstration, we will present DBWipes, a novel data cleaning system that allows users to execute aggregate queries, and interactively detect, understand, and clean errors in the query results. Conference attendees will explore anomalies in campaign donations from the current US presidential election and in readings from a 54-node sensor deployment.","category_a":"","category_b":"Others","keywords":["Computation","Debugging","Human-readable medium","Interactivity","Question (inquiry)","Reading (activity)","Sputter cleaning"],"vec":[0.6384257009,-0.4727709155],"nodes":["40366108","2033016","1695715"]},"221cf7b15aa771f9f9f8c0dc21899e22cd736fb8":{"id":"221cf7b15aa771f9f9f8c0dc21899e22cd736fb8","date":"2016-01-01T12:12:00Z","text":"We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in characterand word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization (Cooijmans et al., 2016) yields state-of-the-art results on permuted sequential MNIST.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Dropout (neural networks)","Feedforward neural network","Gradient","Gradient descent","Language model","MNIST database","Noise (electronics)","Treebank"],"vec":[-0.222503285,-0.0967923743],"nodes":["38018150","3422058","2448674","2719055","2482072","3195305","1996705","1751762","1777528","1760871","1972076"]},"124f6f240ba622cd74a9a0ea554ec2a5011eaadf":{"id":"124f6f240ba622cd74a9a0ea554ec2a5011eaadf","date":"2001-01-01T12:12:00Z","text":"String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data especially for more complex queries involving joins. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string joins directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string join capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on matching short substrings of length , called -grams, and taking into account both positions of individual matches and the total number of such matches. Our approach applies to both approximate full string matching and approximate substring matching, with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers. We demonstrate experimentally the benefits of our technique over the direct use of UDFs, using commercial database systems and real data. To study the I\/O and CPU behavior of approximate string join algorithms with variations in edit distance and -gram length, we also describe detailed experiments based on a prototype implementation. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and\/or special permission from the Endowment. Proceedings of the 27th VLDB Conference, Roma, Italy, 2001","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Approximation algorithm","Central processing unit","Database","Edit distance","Experiment","Join (SQL)","Relational operator","Representation oligonucleotide microarray analysis","String searching algorithm","Substring","User-defined function","VLDB"],"vec":[0.9887794871,0.077192438],"nodes":["1684012","2942126","1735239","1721062","1711192","1704011"]},"167abf2c9eda9ce21907fcc188d2e41da37d9f0b":{"id":"167abf2c9eda9ce21907fcc188d2e41da37d9f0b","date":"2011-01-01T12:12:00Z","text":"Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the wordand phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.","category_a":"NIPS","category_b":"ML","keywords":["Autoencoder","Net (polyhedron)","Recursion","Semantic analysis (compilers)","Statistical classification","Unsupervised learning"],"vec":[-0.5584610697,-0.451580044],"nodes":["2166511","39929781","40113280","1701538","1812612"]},"60f22ad725f041fff81d6371242485bbe5c3ebb6":{"id":"60f22ad725f041fff81d6371242485bbe5c3ebb6","date":"2013-01-01T12:12:00Z","text":"Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloudlike computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Artificial neural network","Benchmark (computing)","Commodity computing","Deep learning","Graphics processing unit","High- and low-level","InfiniBand","Machine learning"],"vec":[-0.3849229964,-0.1872057601],"nodes":["5574038","2570381","1685072","25629078","2301680","1701538"]},"3845947052d99e45f0f3e7789b66c6582d098e48":{"id":"3845947052d99e45f0f3e7789b66c6582d098e48","date":"2017-01-01T12:12:00Z","text":"A polystore system is a database management system composed of integrated heterogeneous database engines and multiple programming languages. By matching data to the storage engine best suited to its needs, complex analytics run faster and flexible storage choices helps improve data organization. BigDAWG (Big Data Working Group) is our prototype implementation of a polystore system. In this paper, we describe the current BigDAWG software release which supports PostgreSQL, Accumulo and SciDB. We describe the overall architecture, API and initial results of applying BigDAWG to the MIMIC II medical dataset.","category_a":"HPEC","category_b":"Others","keywords":["Apache Accumulo","Application programming interface","Big data","Database","Database engine","Heterogeneous database system","MIMIC Simulator","PostgreSQL","Programming language","SciDB","Software release life cycle"],"vec":[0.6189934414,-0.4009785409],"nodes":["1867298","39425362","7485473","1787375","3257323","2033016","2682150","38449161","7946702","1695715"]},"8e370946562e09f8895f8454a7719848cb176d79":{"id":"8e370946562e09f8895f8454a7719848cb176d79","date":"2009-01-01T12:12:00Z","text":"Before sharing to support ad hoc aggregate analyses, microdata often need to be anonymized to protect the privacy of individuals. A variety of privacy models have been proposed for microdata anonymization. Many of these models (e.g., -closeness) essentially require that, after anonymization, groups of sensitive attribute values follow specified distributions. To support such models, in this paper we study the problem of transforming a group of sensitive attribute values to follow a certain target distribution with minimal data distortion. Specifically, we develop and evaluate a novel methodology that combines the use of sensitive attribute permutation and generalization with the addition of fake sensitive attribute values to achieve this transformation. We identify metrics related to accuracy of aggregate query answers over the transformed data, and develop efficient anonymization algorithms to optimize these accuracy metrics. Using a variety of data sets, we experimentally demonstrate the effectiveness of our techniques.","category_a":"VLDB","category_b":"DB","keywords":["Aggregate data","Aggregate function","Algorithm","Data anonymization","Distortion","Hoc (programming language)","Microdata Corporation","Value (ethics)"],"vec":[1.1598195411,0.5809667749],"nodes":["1721062","1704011","1689202","1681771"]},"e39d1345a5aef8a5ee32c0a774de877b903de50c":{"id":"e39d1345a5aef8a5ee32c0a774de877b903de50c","date":"2013-01-01T12:12:00Z","text":"Classifying scenes (e.g. into \u201cstreet\u201d, \u201chome\u201d or \u201cleisure\u201d) is an important but complicated task nowadays, because images come with variability, ambiguity, and a wide range of illumination or scale conditions. Standard approaches build an intermediate representation of the global image and learn classifiers on it. Recently, it has been proposed to depict an image as an aggregation of its contained objects: the representation on which classifiers are trained is composed of many heterogeneous feature vectors derived from various object detectors. In this paper, we propose to study different approaches to efficiently learn contextual semantics out of these object detections. We use the features provided by Object-Bank [24] (177 different object detectors producing 252 attributes each), and show on several benchmarks for scene categorization that careful combinations, taking into account the structure of the data, allows to greatly improve over original results (from +5 to +11%) while drastically reducing the dimensionality of the representation by 97% (from 44,604 to 1,000). We also show that the uncertainty relative to object detectors hampers the use of external semantic knowledge to improve detectors combination, unlike our unsupervised learning approach.","category_a":"ICPRAM","category_b":"Others","keywords":["Categorization","Feature vector","Intermediate representation","Sensor","Spatial variability","Unsupervised learning"],"vec":[-0.7144009049,0.1536268069],"nodes":["1935910","2425018","1713934","3119801","1751762","1724875"]},"1b4f5bb49dc95340a66c75e1c4c719f0f96439c8":{"id":"1b4f5bb49dc95340a66c75e1c4c719f0f96439c8","date":"2017-01-01T12:12:00Z","text":"Determining if two sets are related \u2013 that is, if they have similar values or if one set contains the other \u2013 is an important problem with many applications in data cleaning, data integration, and information retrieval. For example, set relatedness can be a useful tool to discover whether columns from two different databases are joinable; if enough of the values in the columns match, it may make sense to join them. A common metric is to measure the relatedness of two sets by treating the elements as vertices of a bipartite graph and calculating the score of the maximum matching pairing between elements. Compared to other metrics which require exact matchings between elements, this metric uses a similarity function to compare elements between the two sets, making it robust to small dissimilarities in elements and more useful for real-world, dirty data. Unfortunately, the metric suffers from expensive computational cost, taking O(n) time, where n is the number of elements in the sets, for each set-to-set comparison. Thus for applications that try to search for all pairings of related sets in a brute-force manner, the runtime becomes unacceptably large. To address this challenge, we developed SILKMOTH, a system capable of rapidly discovering related set pairs in collections of sets. Internally, SILKMOTH creates a signature for each set, with the property that any other set which is related must match the signature. SILKMOTH then uses these signatures to prune the search space, so only sets that match the signatures are left as candidates. Finally, SILKMOTH applies the maximum matching metric on remaining candidates to verify which of these candidates are truly related sets. An important property of SILKMOTH is that it is guaranteed to output exactly the same related set pairings as the bruteforce method, unlike approximate techniques. Thus, a contribution of this paper is the characterization of the space of signatures which enable this property. We show that selecting the optimal signature in this space is NP-complete, and based on insights from the characterization of the space, we propose two novel filters which help to prune the candidates further before verification. In addition, we introduce a simple optimization to the calculation of the maximum matching metric itself based on the triangle inequality. Compared to related approaches, SILKMOTH is much more general, handling \u2217Both authors contributed equally to this work. This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Proceedings of the VLDB Endowment, Vol. 10, No. 10 Copyright 2017 VLDB Endowment 2150-8097\/17\/06. Table 1: Two related datasets.","category_a":"VLDB","category_b":"DB","keywords":["Algorithmic efficiency","Approximation algorithm","Brute-force attack","Column (database)","Database","Dirty data","Information retrieval","Matching (graph theory)","NP (complexity)","Program optimization","Similarity measure","Social inequality","Sputter cleaning","Type signature","VLDB","Value (ethics)","Vertex (graph theory)"],"vec":[0.9689281689,0.0934038392],"nodes":["38737063","2671663","2033016","1695715"]},"6e3fbbeec1f3ad2a8efe9a7207ba92bee322b78e":{"id":"6e3fbbeec1f3ad2a8efe9a7207ba92bee322b78e","date":"2012-01-01T12:12:00Z","text":"Differential privacy is fast becoming the method of choice for releasing data under strong privacy guarantees. A standard mechanism is to add noise to the counts in contingency tables derived from the dataset. However, when the dataset is sparse in its underlying domain, this vastly increases the size of the published data, to the point of making the mechanism infeasible.\n We propose a general framework to overcome this problem. Our approach releases a compact summary of the noisy data with the same privacy guarantee and with similar utility. Our main result is an efficient method for computing the summary directly from the input data, without materializing the vast noisy data. We instantiate this general framework for several summarization methods. Our experiments show that this is a highly practical solution: The summaries are up to 1000 times smaller, and can be computed in less than 1% of the time compared to standard methods. Finally, our framework works with various data transformations, such as wavelets or sketches.","category_a":"ICDT","category_b":"Others","keywords":["Contingency table","Differential privacy","Experiment","Privacy","Sparse matrix","Wavelet"],"vec":[0.7834709264,0.6401561334],"nodes":["1709589","1738196","1704011","36323928"]},"5640ce300f3bea8347824e887f4b688b0f364873":{"id":"5640ce300f3bea8347824e887f4b688b0f364873","date":"2008-01-01T12:12:00Z","text":"Many factors are thought to increase the chances of misrecognizing a word in ASR, including low frequency, nearby disfluencies, short duration, and being at the start of a turn. However, few of these factors have been formally examined. This paper analyzes a variety of lexical, prosodic, and disfluency factors to determine which are likely to increase ASR error rates. Findings include the following. (1) For disfluencies, effects depend on the type of disfluency: errors increase by up to 15% (absolute) for words near fragments, but decrease by up to 7.2% (absolute) for words near repetitions. This decrease seems to be due to longer word duration. (2) For prosodic features, there are more errors for words with extreme values than words with typical values. (3) Although our results are based on output from a system with speaker adaptation, speaker differences are a major factor influencing error rates, and the effects of features such as frequency, pitch, and intensity may vary between speakers.","category_a":"ACL","category_b":"ComLing","keywords":["Constant-voltage speaker system","Pitch (music)","Speech recognition","Value (ethics)"],"vec":[-0.6944371439,-0.5101724324],"nodes":["1991315","1746807","1812612"]},"124acb2ceccd1d1cd9b4715e00d24eab31099aa8":{"id":"124acb2ceccd1d1cd9b4715e00d24eab31099aa8","date":"2017-01-01T12:12:00Z","text":"Understanding architectural choices for deep neural networks (DNNs) is crucial to improving state-of-the-art speech recognition systems. We investigate which aspects of DNN acoustic model design are most important for speech recognition system performance, focusing on feed-forward networks. We study the effects of parameters like model size (number of layers, total parameters), architecture (convolutional networks), and training details (loss function, regularization methods) on DNN classifier performance and speech recognizer word error rates. On the Switchboard benchmark corpus we compare standard DNNs to convolutional networks, and present the first experiments using locally-connected, untied neural networks for acoustic modeling. Using a much larger 2100-hour training corpus (combining Switchboard and Fisher) we examine the performance of very large DNN models \u2013 with up to ten times more parameters than those typically used in speech recognition systems. The results suggest that a relatively simple DNN architecture and optimization technique give strong performance, and we offer intuitions about architectural choices like network depth over breadth. Our findings extend previous works to help establish a set of best practices for building DNN hybrid speech recognition systems and constitute an important first step toward analyzing more complex recurrent, sequence-discriminative, and HMM-free architectures. \u00a9 2016 Elsevier Ltd. All rights reserved.","category_a":"Computer Speech & Language","category_b":"Others","keywords":["Acoustic model","Artificial neural network","Benchmark (computing)","Best practice","Deep learning","Experiment","Finite-state machine","Hidden Markov model","Loss function","Matrix regularization","Program optimization","Speech recognition","Telephone exchange","Vocabulary"],"vec":[-0.4651638374,-0.1904999755],"nodes":["34961461","40537533","40548699","2893056","2403705","1746807","1701538"]},"8225f757f6bb34624aa8e9658e9af295432c29b5":{"id":"8225f757f6bb34624aa8e9658e9af295432c29b5","date":"2004-01-01T12:12:00Z","text":"In this letter, we show a direct relation between spectral embedding methods and kernel principal components analysis and how both are special cases of a more general learning problem: learning the principal eigenfunctions of an operator defined from a kernel and the unknown data-generating density. Whereas spectral embedding methods provided only coordinates for the training points, the analysis justifies a simple extension to out-of-sample examples (the Nystr\u00f6m formula) for multidimensional scaling (MDS), spectral clustering, Laplacian eigenmaps, locally linear embedding (LLE), and Isomap. The analysis provides, for all such spectral embedding methods, the definition of a loss function, whose empirical average is minimized by the traditional algorithms. The asymptotic expected value of that loss defines a generalization performance and clarifies what these algorithms are trying to learn. Experiments with LLE, Isomap, spectral clustering, and MDS show that this out-of-sample embedding formula generalizes well, with a level of error comparable to the effect of small perturbations of the training set on the embedding.","category_a":"Neural Computation","category_b":"ML","keywords":["Algorithm","Arc diagram","Cluster analysis","Embedding","Generalization (Psychology)","Hearing Loss, High-Frequency","Isomap","Kernel","Kernel (operating system)","Kernel principal component analysis","Linear IgA Bullous Dermatosis","Loss function","Multidimensional scaling","Nonlinear dimensionality reduction","Perturbation theory","Principal Component Analysis","Principal component analysis","Spectral clustering","algorithm","multidimensional scaling","statistical cluster"],"vec":[-0.3949667032,0.5399829252],"nodes":["1751762","2460212","7245737","39651651","1724875","2120888"]},"136eefe33796c388a15d25ca03cb8d5077d14f37":{"id":"136eefe33796c388a15d25ca03cb8d5077d14f37","date":"2007-01-01T12:12:00Z","text":"In previous papers [SC05, SBC+07], some of us predicted the end of \u201cone size fits all\u201d as a commercial relational DBMS paradigm. These papers presented reasons and experimental evidence that showed that the major RDBMS vendors can be outperformed by 1-2 orders of magnitude by specialized engines in the data warehouse, stream processing, text, and scientific database markets. Assuming that specialized engines dominate these markets over time, the current relational DBMS code lines will be left with the business data processing (OLTP) market and hybrid markets where more than one kind of capability is required. In this paper we show that current RDBMSs can be beaten by nearly two orders of magnitude in the OLTP market as well. The experimental evidence comes from comparing a new OLTP prototype, H-Store, which we have built at M.I.T., to a popular RDBMS on the standard transactional benchmark, TPC-C. We conclude that the current RDBMS code lines, while attempting to be a \u201cone size fits all\u201d solution, in fact, excel at nothing. Hence, they are 25 year old legacy code lines that should be retired in favor of a collection of \u201cfrom scratch\u201d specialized engines. The DBMS vendors (and the research community) should start with a clean sheet of paper and design systems for tomorrow\u2019s requirements, not continue to push code lines and architectures designed for yesterday\u2019s needs.","category_a":"VLDB","category_b":"DB","keywords":["Benchmark (computing)","Database","FITS","H-Store","IBM Tivoli Storage Productivity Center","Legacy code","Online transaction processing","Relational database management system","Requirement","Rewrite (programming)","Stream processing"],"vec":[0.7753998064,-0.6825116211],"nodes":["1695715","2033016","2254232","2367532","1975015","2549968"]},"01a903739564f575b81c87f7a9e2cb7b609f7ada":{"id":"01a903739564f575b81c87f7a9e2cb7b609f7ada","date":"2015-01-01T12:12:00Z","text":"This paper develops a novel framework for semantic image retrieval based on the notion of a scene graph. Our scene graphs represent objects (&#x201C;man&#x201D;, &#x201C;boat&#x201D;), attributes of objects (&#x201C;boat is white&#x201D;) and relationships between objects (&#x201C;man standing on boat&#x201D;). We use these scene graphs as queries to retrieve semantically related images. To this end, we design a conditional random field model that reasons about possible groundings of scene graphs to test images. The likelihoods of these groundings are used as ranking scores for retrieval. We introduce a novel dataset of 5,000 human-generated scene graphs grounded to images and use this dataset to evaluate our method for image retrieval. In particular, we evaluate retrieval using full scene graphs and small scene subgraphs, and show that our method outperforms retrieval methods that use only objects or low-level image features. In addition, we show that our full model can be used to improve object localization compared to baseline methods.","category_a":"CVPR","category_b":"ComVis","keywords":["Baseline (configuration management)","Conditional random field","High- and low-level","Image retrieval","Scene graph"],"vec":[-0.2888356983,-0.55813545],"nodes":["2143297","2580593","37718254","33642044","1760364","35609041","3216322"]},"33e46d74b066402f0ae912627dff5b700006f9b2":{"id":"33e46d74b066402f0ae912627dff5b700006f9b2","date":"2008-01-01T12:12:00Z","text":"Private data often come in the form of associations between entities, such as customers and products bought from a pharmacy, which are naturally represented in the form of a large, sparse bipartite graph. As with tabular data, it is desirable to be able to publish anonymized versions of such data, to allow others to perform ad hoc analysis of aggregate graph properties. However, existing tabular anonymization techniques do not give useful or meaningful results when applied to graphs: small changes or masking of the edge structure can radically change aggregate graph properties. We introduce a new family of anonymizations for bipartite graph data, called (k, \u2113)-groupings. These groupings preserve the underlying graph structure perfectly, and instead anonymize the mapping from entities to nodes of the graph. We identify a class of \u201csafe\u201d (k, \u2113)-groupings that have provable guarantees to resist a variety of attacks, and show how to find such safe groupings. We perform experiments on real bipartite graph data to study the utility of the anonymized version, and the impact of publishing alternate groupings of the same graph data. Our experiments demonstrate that (k, \u2113)-groupings offer strong tradeoffs between privacy and utility.","category_a":"VLDB","category_b":"DB","keywords":["Data anonymization","Entity","Experiment","Graph embedding","Graph property","Hoc (programming language)","Information privacy","Provable security","Sparse matrix","Table (information)"],"vec":[1.1482155739,0.6287855753],"nodes":["1709589","1704011","1689202","1681771"]},"18c021c9cce95ed5615a060f590b8388b604e7c5":{"id":"18c021c9cce95ed5615a060f590b8388b604e7c5","date":"2015-01-01T12:12:00Z","text":"This paper presents a new view of federated databases to address the growing need for managing information that spans multiple data models. This trend is fueled by the proliferation of storage engines and query languages based on the observation that &#226;  no one size fits all&#226;  . To address this shift, we propose a polystore architecture; it is designed to unify querying over multiple data models. We consider the challenges and opportunities associated with polystores. Open questions in this space revolve around query optimization and the assignment of objects to storage engines. We introduce our approach to these topics and discuss our prototype in the context of the Intel Science and Technology Center for Big Data","category_a":"SIGMOD","category_b":"DB","keywords":["Big data","Data model","Database","Database engine","FITS","Program optimization","Query language","Query optimization"],"vec":[0.5888085622,-0.2784828247],"nodes":["38449161","1787375","1695715","1718134","1686294","3257323","2033016","1740962","2682150","2031287"]},"99126d196c0539e3014b61d91538a29ee2de8c54":{"id":"99126d196c0539e3014b61d91538a29ee2de8c54","date":"2012-01-01T12:12:00Z","text":"Uncertainties in data can arise for a number of reasons: when data is incomplete, contains conflicting information or has been deliberately perturbed or coarsened to remove sensitive details. An important case which arises in many real applications is when the data describes a set of possibilities, but with cardinality constraints. These constraints represent correlations between tuples encoding, e.g. that at most two possible records are correct, or that there is an (unknown) one-to-one mapping between a set of tuples and attribute values. Although there has been much effort to handle uncertain data, current systems are not equipped to handle such correlations, beyond simple mutual exclusion and co-existence constraints. Vitally, they have little support for efficiently handling aggregate queries on such data. In this paper, we aim to address some of these deficiencies, by introducing LICM (Linear Integer Constraint Model), which can succinctly represent many types of tuple correlations, particularly a class of cardinality constraints. We motivate and explain the model with examples from data cleaning and masking sensitive data, to show that it enables modeling and querying such data, which was not previously possible. We develop an efficient strategy to answer conjunctive and aggregate queries on possibilistic data by describing how to implement relational operators over data in the model. LICM compactly integrates the encoding of correlations, query answering and lineage recording. In combination with off-the-shelf linear integer programming solvers, our approach provides exact bounds for aggregate queries. Our prototype implementation demonstrates that query answering with LICM can be effective and scalable.","category_a":"Data Engineering","category_b":"DB","keywords":["Aggregate data","Aggregate function","Integer programming","Loop-invariant code motion","Mutual exclusion","Relational operator","Scalability","Sputter cleaning","Uncertain data"],"vec":[1.0930380966,0.0872313756],"nodes":["1709589","1704011","39659954","1689202"]},"78b729049a0135dc75a021ce5bbc127902253fde":{"id":"78b729049a0135dc75a021ce5bbc127902253fde","date":"2003-01-01T12:12:00Z","text":"The integration of data produced and collected across autonomous, heterogeneous web services is an increasingly important and challenging problem. Due to the lack of global identifiers, the same entity (e.g., a product) might have different textual representations across databases. Textual data is also often noisy because of transcription errors, incomplete information, and lack of standard formats. A fundamental task during data integration is matching of strings that refer to the same entity. In this paper, we adopt the widely used and established cosine similarity metric from the information retrieval field in order to identify potential string matches across web sources. We then use this similarity metric to characterize this key aspect of data integration as a join between relations on textual attributes, where the similarity of matches exceeds a specified threshold. Computing an exact answer to the text join can be expensive. For query processing efficiency, we propose a sampling-based join approximation strategy for execution in a standard, unmodified relational database management system (RDBMS), since more and more web sites are powered by RDBMSs with a web-based front end. We implement the join inside an RDBMS, using SQL queries, for scalability and robustness reasons. Finally, we present a detailed performance evaluation of an implementation of our algorithm within a commercial RDBMS, using real-life data sets. Our experimental results demonstrate the efficiency and accuracy of our techniques.","category_a":"WWW","category_b":"Others","keywords":["Algorithm","Autonomous car","Cosine similarity","Database","Identifier","Information retrieval","Join (SQL)","Microsoft SQL Server","Performance Evaluation","Real life","Relational database","Relational database management system","SQL","Scalability","String (computer science)","Transcription (software)","Web application","Web service"],"vec":[0.9993412329,0.1053204463],"nodes":["1684012","2942126","1721062","1704011"]},"4893c08f1c76fe4eb7cad2bb496da526b0fa5dd3":{"id":"4893c08f1c76fe4eb7cad2bb496da526b0fa5dd3","date":"2013-01-01T12:12:00Z","text":"Procedures to anonymize data sets are vital for companies, government agencies and other bodies to meet their obligations to share data without compromising the privacy of the individuals contributing to it. Despite much work on this topic, the area has not yet reached stability. Early models (k-anonymity and &#x2113;-diversity) are now thought to offer insufficient privacy. Noise-based methods like differential privacy are seen as providing stronger privacy, but less utility. However, across all methods sensitive information of some individuals can often be inferred with relatively high accuracy. In this paper, we reverse the idea of a `privacy attack,' by incorporating it into a measure of privacy. Hence, we advocate the notion of empirical privacy, based on the posterior beliefs of an adversary, and their ability to draw inferences about sensitive values in the data. This is not a new model, but rather a unifying view: it allows us to study several well-known privacy models which are not directly comparable otherwise. We also consider an empirical approach to measuring utility, based on a workload of queries. Consequently, we are able to place different privacy models including differential privacy and early syntactic models on the same scale, and compare their privacy\/utility tradeoff. We learn that, in practice, the difference between differential privacy and various syntactic models is less dramatic than previously thought, but there are still clear domination relations between them.","category_a":"ICDEW","category_b":"Others","keywords":["Adversary (cryptography)","Denial-of-service attack","Differential privacy","Dominating set","Information sensitivity","Mathematical model","Privacy","Value (ethics)"],"vec":[0.8566563586,0.6203428604],"nodes":["1709589","1738196","39659954","1704011","1689202"]},"50654221b831a14e3ddad4579accc9c92685f29a":{"id":"50654221b831a14e3ddad4579accc9c92685f29a","date":"2002-01-01T12:12:00Z","text":"We present two results which arise from a model-based approach to hierarchical agglomerative clustering. First, we show formally that the common heuristic agglomerative clustering algorithms \u2013 Ward\u2019s method, single-link, complete-link, and a variant of group-average \u2013 are each equivalent to a hierarchical model-based method. This interpretation gives a theoretical explanation of the empirical behavior of these algorithms, as well as a principled approach to resolving practical issues, such as number of clusters or the choice of method. Second, we show how a model-based viewpoint can suggest variations on these basic agglomerative algorithms. We introduce adjusted complete-link, Mahalanobis-link, and line-link as variants, and demonstrate their utility.","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Cluster analysis","Heuristic","Hierarchical clustering","Hierarchical database model","Ward's method"],"vec":[0.3324683658,0.3502486875],"nodes":["2833700","38666915","1812612"]},"6bf6a3fd2c4c17c4326b81424ce19aba0a4b9c42":{"id":"6bf6a3fd2c4c17c4326b81424ce19aba0a4b9c42","date":"2015-01-01T12:12:00Z","text":"Recent work has shown that collaborative filter-based recommender systems can be improved by incorporating side information, such as natural language reviews, as a way of regularizing the derived product representations. Motivated by the success of this approach, we introduce two different models of reviews and study their effect on collaborative filtering performance. While the previous state-of-the-art approach is based on a latent Dirichlet allocation (LDA) model of reviews, the models we explore are neural network based: a bag-of-words product-of-experts model and a recurrent neural network. We demonstrate that the increased flexibility offered by the product-of-experts model allowed it to achieve state-of-the-art performance on the Amazon review dataset, outperforming the LDA-based approach. However, interestingly, the greater modeling power offered by the recurrent neural network appears to undermine the model's ability to act as a regularizer of the product representations.","category_a":"RecSys","category_b":"Others","keywords":["Artificial neural network","Bag-of-words model","Collaborative filtering","Latent Dirichlet allocation","Natural language","Product of experts","Recommender system","Recurrent neural network"],"vec":[-0.222430912,-0.1312290589],"nodes":["2634674","2182706","1979489","1760871"]},"0bddc08769e9ff5d42ed36336f69bb3b1f42e716":{"id":"0bddc08769e9ff5d42ed36336f69bb3b1f42e716","date":"2004-01-01T12:12:00Z","text":"Helicopters have highly stochastic, nonlinear, dynamics, and autonomous helicopter flight is widely regarded to be a challenging control problem. As helicopters are highly unstable at low speeds, it is particularly difficult to design controllers for low speed aerobatic maneuvers. In this paper, we describe a successful application of reinforcement learning to designing a controller for sustained inverted flight on an autonomous helicopter. Using data collected from the helicopter in flight, we began by learning a stochastic, nonlinear model of the helicopter\u2019s dynamics. Then, a reinforcement learning algorithm was applied to automatically learn a controller for autonomous inverted hovering. Finally, the resulting controller was successfully tested on our autonomous helicopter platform.","category_a":"ISER","category_b":"Others","keywords":["Algorithm","Autonomous car","Instability","Nonlinear system","Reinforcement learning"],"vec":[-0.0383939461,-1.2590823636],"nodes":["1701538","5574038","1935095","1981419","37051432","2712072","38850919","40274002"]},"1aeee8b379e6fcf10d33c15df78e135b16b24d5e":{"id":"1aeee8b379e6fcf10d33c15df78e135b16b24d5e","date":"2005-01-01T12:12:00Z","text":"The applicability of many current information extraction techniques is severely limited by the need for supervised training data. We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.","category_a":"ACL","category_b":"ComLing","keywords":["Generative model","Hidden Markov model","Information extraction","Semi-supervised learning","Structured text","Supervised learning","Test set","Unsupervised learning"],"vec":[-0.7867651889,-0.6225792941],"nodes":["3050250","38666915","1812612"]},"a3bd824b2d0077e1e2f97bb73942151d711cdf93":{"id":"a3bd824b2d0077e1e2f97bb73942151d711cdf93","date":"2017-01-01T12:12:00Z","text":"","category_a":"Computer Speech & Language","category_b":"Others","keywords":["Machine translation","Neural machine translation"],"vec":[-0.1899576234,0.0006806639],"nodes":["2345617","1979489","38665506","1787034","1751762"]},"4a350352b2fb426530693d42f45effd049537cab":{"id":"4a350352b2fb426530693d42f45effd049537cab","date":"2015-01-01T12:12:00Z","text":"We propose a structured prediction architecture for images centered around deep recurrent neural networks. The proposed network, called ReSeg, is based on the recently introduced ReNet model for object classification. We modify and extend it to perform object segmentation, noting that the avoidance of pooling can greatly simplify pixel-wise tasks for images. The ReSeg layer is composed of four recurrent neural networks that sweep the image horizontally and vertically in both directions, along with a final layer that expands the prediction back to the original image size. ReSeg combines multiple ReSeg layers with several possible input layers as well as a final layer which expands the prediction back to the original image size, making it suitable for a variety of structured prediction tasks. We evaluate ReSeg on the specific task of object segmentation with three widely-used image segmentation datasets, namely Weizmann Horse, Fashionista and Oxford Flower. The results suggest that ReSeg can challenge the state of the art in object segmentation, and may have further applications in structured prediction at large.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Image resolution","Image segmentation","Pixel","Recurrent neural network","Structured prediction"],"vec":[-0.5525403526,0.3331627956],"nodes":["2077146","2182706","1760871","1751762","1745043","1979489"]},"5656fa5aa6e1beeb98703fc53ec112ad227c49ca":{"id":"5656fa5aa6e1beeb98703fc53ec112ad227c49ca","date":"2013-01-01T12:12:00Z","text":"We introduce the multi-prediction deep Boltzmann machine (MP-DBM). The MPDBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classification tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM does not require greedy layerwise pretraining, and outperforms the standard DBM at classification, classification with missing inputs, and mean field prediction tasks.1","category_a":"NIPS","category_b":"ML","keywords":["Boltzmann machine","Calculus of variations","Decibel","Greedy algorithm","Statistical model"],"vec":[-0.3835605235,0.4018270768],"nodes":["34740554","1778734","1760871","1751762"]},"70fa7c2a8a12509da107fe82b04d5e422120984f":{"id":"70fa7c2a8a12509da107fe82b04d5e422120984f","date":"2002-01-01T12:12:00Z","text":"We recently conducted a research project for a large North American automobile insurer. This study was the most exhaustive ever undertaken by this particular insurer and lasted over an entire year. We analyzed the discriminating power of each variable used for ratemaking. We analyzed the performance of several models within five broad categories: linear regressions, generalized linear models, decision trees, neural networks and support vector machines. In this paper, we present the main results of this study. We qualitatively compare models and show how neural networks can represent high-order nonlinear dependencies with a small number of parameters, each of which is estimated on a large proportion of the data, thus yielding low variance. We thoroughly explain the purpose of the nonlinear sigmoidal transforms which are at the very heart of neural networks\u2019 performances. The main numerical result is a statistically significant reduction in the out-of-sample meansquared error using the neural network model and our ability to substantially reduce the median premium by charging more to the highest risks. This in turn can translate into substantial savings and financial benefits for an insurer. We hope this paper goes a long way in convincing actuaries to include neural networks within their set of modeling tools for ratemaking.","category_a":"","category_b":"Others","keywords":["Artificial neural network","Categories","Decision tree","Financial savings","Generalized linear model","Insurance Carriers","Linear Models","Linear model","Machine learning","Medical algorithm","Network model","Neural Network Simulation","Nonlinear system","Numerical analysis","Performance","Regression - mental defense mechanism","Sigmoid function","Support Vector Machine","Support vector machine","Trees (plant)","algorithm","benefit"],"vec":[0.029978057,-0.1415188299],"nodes":["2701388","1751762","2748188","1724875"]},"412a9e54bbb31e12d008a9579994e009c5b40b46":{"id":"412a9e54bbb31e12d008a9579994e009c5b40b46","date":"2008-01-01T12:12:00Z","text":"Our previous work has shown that architectural and application shifts have resulted in modern OLTP databases increasingly falling short of optimal performance [10]. In particular, the availability of multiple-cores, the abundance of main memory, the lack of user stalls, and the dominant use of stored procedures are factors that portend a clean-slate redesign of RDBMSs. This previous work showed that such a redesign has the potential to outperform legacy OLTP databases by a significant factor. These results, however, were obtained using a bare-bones prototype that was developed just to demonstrate the potential of such a system. We have since set out to design a more complete execution platform, and to implement some of the ideas presented in the original paper. Our demonstration presented here provides insight on the development of a distributed main memory OLTP database and allows for the further study of the challenges inherent in this operating environment.","category_a":"VLDB","category_b":"DB","keywords":["Computer data storage","Database","H-Store","Online transaction processing","Operating environment","Relational database management system","Software transactional memory","Stored procedure","Transaction processing","Transaction processing system"],"vec":[0.376991107,-0.3344419082],"nodes":["32376727","32228637","32038228","1774210","6850164","2031287","40272511","2033016","1695715","1698138","34665293","2254232"]},"78507d14c925e16d628bc643c75c449267fef64c":{"id":"78507d14c925e16d628bc643c75c449267fef64c","date":"2016-01-01T12:12:00Z","text":"In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Multilayer perceptron","Perceptron","Recurrent neural network","State (computer science)"],"vec":[-0.3332265023,0.3066695089],"nodes":["34719201","18897617","2708454","39458024","39935246","40040968","1760871","1751762"]},"64b05d23afb0bd5754fcb70d87233c9b4326eef2":{"id":"64b05d23afb0bd5754fcb70d87233c9b4326eef2","date":"2000-01-01T12:12:00Z","text":"With the increasing importance of XML, LDAP directories, and text-based information sources on the Internet, there is an ever-greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes\/dimensions, with correlations between the multiple dimensions. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. For the 1-D problem, we present a novel technique called MO (Maximal Overlap). We then develop and analyze two 1-D estimation algorithms, MOC and MOLC, based on MO and a constraint-based characterization of all possible completions of a given PST. For the k-D problem, we first generalize PSTs to multiple dimensions and develop a space- and time-efficient probabilistic algorithm to construct k-D PSTs directly. We then show how to extend MO to multiple dimensions. Finally, we demonstrate, both analytically and experimentally, that MO is both practical and substantially superior to competing algorithms.","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Data structure","Experiment","Internet","Lightweight Directory Access Protocol","Overlap\u2013add method","Planar separator theorem","Program optimization","Query optimization","Randomized algorithm","Selectivity (electronic)","Substring","Text-based (computing)","XML"],"vec":[0.9776555459,0.1608560657],"nodes":["1735239","1821764","1736021","1704011"]},"160cb0d0c0930f2782d8caa212d892845fc02aaf":{"id":"160cb0d0c0930f2782d8caa212d892845fc02aaf","date":"2009-01-01T12:12:00Z","text":"Approximate string matching is a problem that has received a lot of attention recently. Existing work on information retrieval has concentrated on a variety of similarity measures <b>TF\/IDF, BM25, HMM<\/b>, etc.) specifically tailored for document retrieval purposes. As new applications that depend on retrieving short strings are becoming popular(e.g., local search engines like YellowPages.com, Yahoo!Local, and Google Maps) new indexing methods are needed, tailored for short strings. For that purpose, a number of indexing techniques and related algorithms have been proposed based on length normalized similarity measures. A common denominator of indexes for length normalized measures is that maintaining the underlying structures in the presence of incremental updates is inefficient, mainly due to data dependent, precomputed weights associated with each distinct token and string. Incorporating updates usually is accomplished by rebuilding the indexes at regular time intervals. In this paper we present a framework that advocates lazy update propagation with the following key feature: Efficient, incremental updates that immediately reflect the new data in the indexes in a way that gives strict guarantees on the quality of subsequent query answers. More specifically, our techniques guarantee against false negatives and limit the number of false positives produced. We implement a fully working prototype and illustrate that the proposed ideas work really well in practice for real datasets.","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","Approximate string matching","Approximation algorithm","Bing Maps","Cosine similarity","Document retrieval","Information retrieval","Lazy evaluation","Local search (optimization)","String (computer science)","String searching algorithm","Tf\u2013idf","Web search engine"],"vec":[1.0877690159,0.0669404864],"nodes":["1836901","1721062","1704011"]},"4856e7719e566f2466369ae2031afb07c934d4d3":{"id":"4856e7719e566f2466369ae2031afb07c934d4d3","date":"2014-01-01T12:12:00Z","text":"A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Artificial neural network","Convex optimization","Gradient","Gradient descent","Information theory","Maxima and minima","Network theory","Newton","Newton's method","Numerical analysis","Program optimization","Quasi-Newton method","Recurrent neural network"],"vec":[-0.3200670672,0.5583865396],"nodes":["2921469","1996134","1854385","1979489","25769960","1751762"]},"5c0fe8ba39bda83d6ca3b9705a780809d52a67b4":{"id":"5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","date":"2012-01-01T12:12:00Z","text":"Learning good representations from a large set of unlabeled data is a particularly challenging task. Recent work (see Bengio (2009) for a review) shows that training deep architectures is a good way to extract such representations, by extracting and disentangling gradually higher-level factors of variation characterizing the input distribution. In this paper, we describe different kinds of layers we trained for learning representations in the setting of the Unsupervised and Transfer Learning Challenge. The strategy of our team won the final phase of the challenge. It combined and stacked different one-layer unsupervised learning algorithms, adapted to each of the five datasets of the competition. This paper describes that strategy and the particular one-layer learning algorithms feeding a simple linear classifier with a tiny number of labeled training samples (1 to 64 per class).","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Deep learning","Linear classifier","Machine learning","Unsupervised learning"],"vec":[-0.5072907324,0.3513558977],"nodes":["1935910","2921469","3119801","2425018","1751762","34740554","1948303","3012669","2755582","1923596","1724875","1760871","32837403"]},"4d82a5741b03a0a134380400d42552c53c89ac83":{"id":"4d82a5741b03a0a134380400d42552c53c89ac83","date":"2006-01-01T12:12:00Z","text":"Join techniques deploying approximate match predicates are fundamental data cleaning operations. A variety of predicates have been utilized to quantify approximate match in such operations and some have been embedded in a declarative data cleaning framework. These techniques return pairs of tuples from both relations, tagged with a score, signifying the degree of similarity between the tuples in the pair according to the specific approximate match predicate. In this paper, we consider the problem of estimating various parameters on the output of declarative approximate join algorithms for planning purposes. Such algorithms are highly time consuming, so precise knowledge of the result size as well as its score distribution is a pressing concern. This knowledge aids decisions as to which operations are more promising for identifying highly similar tuples, which is a key operation for data cleaning. We propose solution strategies that fully comply with a declarative framework and analytically reason about the quality of the estimates we obtain as well as the performance of our strategies. We present the results of a detailed performance evaluation of all strategies proposed. Our experimental results validate our analytical expectations and shed additional light on the quality and performance of our estimation framework. Our study offers a set of simple, fully declarative techniques for this problem, which can be readily deployed in data cleaning systems.","category_a":"ICDE","category_b":"DB","keywords":["Algorithm","Approximation algorithm","Declarative programming","Embedded system","Join (SQL)","Performance Evaluation","Sputter cleaning"],"vec":[0.9965718107,0.1062121624],"nodes":["1679363","1721062","1704011","1805402"]},"2a92f8fb49dcae62af89d0c1cb9a3001b1be7f5a":{"id":"2a92f8fb49dcae62af89d0c1cb9a3001b1be7f5a","date":"1999-01-01T12:12:00Z","text":"With the explosion of the Internet, LDAP directories and XML, there is an ever greater need to evaluate queries involving (sub)string matching. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees as the basic framework for substring selectivity estimation. We present a novel technique to obtain a good estimate for a given substring matching query, called MO (for Maximal Overlap), that estimates the selectivity of a query based on all maximal substrings of the query in the pruned count-suffix tree. We show that MO is provably better than the (independence-based) substring selectivity estimation technique proposed by Krishnan et al. [6], called KVI, under the natural assumption that strings exhibit the so-called \u201cshort memory\u201d property. We complement our analysis with an experiment, using a real AT&T data set, that demonstrates that MO is substantially superior to KVI in the quality of the estimate. Finally, we develop and analyze two selectivity estimation algorithms, MOC and MOLC, based on MO and a constraint-based characterization of all possible completions of a given pruned count-suffix tree. We show that KVI, MO, MOC and MOLC illustrate an interesting tradeoff between estimation accuracy and computational efficiency. *This work was done when the author was at AT&T Labs-Research, Florham Park, NJ 07932, USA. +This work was done when the author was on sabbatical at AT&T Labs-Research, Florham Park, NJ 07932, USA. Permission to make digital or hard copies ol\u2019all or part ol\u2018this work for personal or classroom use is granted without fee provided that topics are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the lirst page. To copy otherwise, to republish, lo post on servers or to redistribute to lists. requires prior specific permission andior a fee. PODS \u2018W Philadelphia PA Copyright ACM 1999 1-58 113-062-7\/99\/05...$5.00","category_a":"PODS","category_b":"Others","keywords":["Algorithm","Lightweight Directory Access Protocol","Maxima and minima","Overlap\u2013add method","Program optimization","Query optimization","Selectivity (electronic)","Substring","Suffix tree","XML"],"vec":[1.0392515567,0.0818352358],"nodes":["1735239","1736021","1704011"]},"1513c758ce244032348b751c40a100846e9aa321":{"id":"1513c758ce244032348b751c40a100846e9aa321","date":"2002-01-01T12:12:00Z","text":"XML queries typically specify patterns of selection predicates on multiple elements that have some specified tree structured relationships. The primitive tree structured relationships are parent-child and ancestor-descendant, and finding all occurrences of these relationships in an XML database is a core operation for XML query processing. In this paper, we develop two families of structural join algorithms for this task: tree-merge and stack-tree. The tree-merge algorithms are a natural extension of traditional merge joins and the recently proposed multi-predicate merge joins, while the stack-tree algorithms have no counterpart in traditional relational join processing. We present experimental results on a range of data and queries using the TIMBER native XML query engine built on top of SHORE. We show that while, in some cases, tree-merge algorithms can have performance comparable to stack-tree algorithms, in many cases they are considerably worse. This behavior is explained by analytical results that demonstrate that, on sorted inputs, the stack-tree algorithms have worst-case I\/O and CPU complexities linear in the sum of the sizes of inputs and output, while the tree-merge algorithms do not have the same guarantee.","category_a":"ICDE","category_b":"DB","keywords":["Algorithm","Best, worst and average case","Binary tree","Central processing unit","Database","Efficient XML Interchange","Join (SQL)","Memory-mapped I\/O","Pattern matching","Relational algebra","Tree (data structure)","XML","XML database"],"vec":[1.0146286842,0.0318708499],"nodes":["1978097","1735239","2042232","1683593","1721062","1704011"]},"0a6193e3693356ae19c3ec3ed7c1375260a9e4a1":{"id":"0a6193e3693356ae19c3ec3ed7c1375260a9e4a1","date":"2015-01-01T12:12:00Z","text":"Recently there has been growing interest in building \u201cactive\u201d visual object recognizers, as opposed to \u201cpassive\u201d recognizers which classifies a given static image into a predefined set of object categories. In this paper we propose to generalize recent end-to-end active visual recognizers into a controller-recognizer framework. In this framework, the interfaces with an external manipulator, while the recognizer classifies the visual input adjusted by the manipulator. We describe two recently proposed controllerrecognizer models\u2013 the recurrent attention model (Mnih et al., 2014) and spatial transformer network (Jaderberg et al., 2015)\u2013 as representative examples of controller-recognizer models. Based on this description we observe that most existing end-to-end controller-recognizers tightly couple the controller and recognizer. We consider whether this tight coupling is necessary, and try to answer this empirically by investigating a decoupled controller and recognizer. Our experiments revealed that it is not always necessary to tightly couple them, and that by decoupling the controller and recognizer, there is a possibility to build a generic controller that is pretrained and works together with any subsequent recognizer.","category_a":"ArXiv","category_b":"Journal","keywords":["Computer cluster","Decoupling (electronics)","End-to-end principle","Experiment","Finite-state machine","Transformer"],"vec":[-0.3067858101,-0.0990233596],"nodes":["3009779","36303818","1760871","1979489"]},"12dd078034f72e4ebd9dfd9f80010d2ae7aaa337":{"id":"12dd078034f72e4ebd9dfd9f80010d2ae7aaa337","date":"2016-01-01T12:12:00Z","text":"The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network\u2019s own one-stepahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Domain adaptation","Glossary of computer graphics","Language model","MNIST database","Recurrent neural network","Sampling (signal processing)","Scheduling (computing)","Treebank"],"vec":[-0.3126590556,0.5196271534],"nodes":["1996705","2059369","1720117","35097114","1760871","1751762"]},"1e9e3694954822d720570c5c53136c8814b2c4be":{"id":"1e9e3694954822d720570c5c53136c8814b2c4be","date":"2016-01-01T12:12:00Z","text":"Digital documents are easy to handle, share and store than hard copy of documents. These made people to prefer digital document over hard copy of documents. Digital documents are nothing but scanned images of a document or natural images of notice boards, traffic signs. Text detection is an important process required to extract text from images. Text from images can be extracted using Optical Character Recognition (OCR). OCR works in three phases as pre-processing, segmentation, character recognition. Pre-processing is the first phase which uses different techniques for making text easy to extract from images. In segmentation phase, each character is isolated. Then this will be given as input to OCR recognition phase which will compare it with training data-set and will recognize character. In this survey paper, different techniques for OCR are discussed.","category_a":"","category_b":"Others","keywords":["Document","Document retrieval","Image processing","Image scanner","Optical character recognition","Preprocessor","Scanning","Test set"],"vec":[-0.5153820896,-0.4781951979],"nodes":["9096174","32873684",0,0,0,"14092493","1728108","2896700","1778174","1701294","12600623","2026212","1739786","1881146","3090801","1740803","2887018","2522004","1738703","1685922","1948591",0,"2596938","31359933","2199009","38082850","40634862",0,0,"30421557","5574038","37742741","1714272","2638806","39086009","39624042","25629078","1701538","36195767"]},"1fd8bf7f21e2f117ce00ea9cf3005d4aa37f155a":{"id":"1fd8bf7f21e2f117ce00ea9cf3005d4aa37f155a","date":"2017-01-01T12:12:00Z","text":"","category_a":"ArXiv","category_b":"Journal","keywords":[],"vec":[-0.1922954973,-0.0052714067],"nodes":["1867298","39425362","7485473","1787375","3257323","2033016","2682150","38449161","7946702","1695715"]},"0658b1922d6a30a0191bb5fe13b0a9e43e49c999":{"id":"0658b1922d6a30a0191bb5fe13b0a9e43e49c999","date":"2010-01-01T12:12:00Z","text":"MapReduce complements DBMSs since databases are not designed for extract-transform-load tasks, a MapReduce specialty.","category_a":"Commun. ACM","category_b":"Journal","keywords":["Database","MapReduce"],"vec":[-0.0030620799,-0.1055169186],"nodes":["1695715","2254232","1765659","2033016","12067489","1774210","6850164"]},"08e5cce87b6af2ae0ee09b58b86d81aa229b46d0":{"id":"08e5cce87b6af2ae0ee09b58b86d81aa229b46d0","date":"2008-01-01T12:12:00Z","text":"Much real data consists of more than one dimension, such as financial transactions (eg, price \u00d7 volume) and IP network flows (eg, duration \u00d7 numBytes), and capture relationships between the variables. For a single dimension, quantiles are intuitive and robust descriptors. Processing and analyzing such data, particularly in data warehouse or data streaming settings, requires similarly robust and informative statistical descriptors that go beyond one-dimension. Applying quantile methods to summarize a multidimensional distribution along only singleton attributes ignores the rich dependence amongst the variables. In this paper, we present new skyline-based statistical descriptors for capturing the distributions over pairs of dimensions. They generalize the notion of quantiles in the individual dimensions, and also incorporate properties of the joint distribution. We introduce \u03c6-quantours and \u03b1-radials, which are skyline points over subsets of the data, and propose (\u03c6, \u03b1)-quantiles, found from the union of these skylines, as statistical descriptors of two-dimensional distributions. We present efficient online algorithms for tracking (\u03c6, \u03b1)-quantiles on two-dimensional streams using guaranteed small space. We identify the principal properties of the proposed descriptors and perform extensive experiments with synthetic and real IP traffic data to study the efficiency of our proposed algorithms.","category_a":"SSDBM","category_b":"Others","keywords":["Algorithm","Experiment","Information","Online algorithm","Stock and flow","Synthetic data"],"vec":[1.0396141415,0.3768615629],"nodes":["1709589","2096611","1711192","1704011"]},"3598b9eb49e226c1f49f123a058c1896459af339":{"id":"3598b9eb49e226c1f49f123a058c1896459af339","date":"2005-01-01T12:12:00Z","text":"Various index structures have been proposed to speed up the evaluation of XML path expressions. However, existing XML path indices suffer from at least one of three limitations: they focus only on indexing the structure (relying on a separate index for node content), they are useful only for simple path expressions such as root-to-leaf paths, or they cannot be tightly integrated with a relational query processor. Moreover, there is no unified framework to compare these index structures. In this paper, we present a framework defining a family of index structures that includes most existing XMLpath indices.We also propose two novel index structures in this family, with different space-time tradeoffs, that are effective for the evaluation of XML branching path expressions (i.e., twigs) with value conditions. We also show how this family of index structures can be implemented using the access methods of the underlying database system. Finally, we present an experimental evaluation that shows the performance tradeoff between index space and matching time. The experimental results show that our novel indices achieve orders ofmagnitude improvement in performance for evaluating twig queries, albeit at a higher space cost, over the use of previously proposed XML path indices that can be tightly integrated with a relational query processor.","category_a":"ICDEW","category_b":"Others","keywords":["Path expression","Relational database","Twig (database)","Unified Framework","XML","XPath"],"vec":[1.1158730017,0.0841913321],"nodes":["39507820","1710965","2096611","1721062","1769868","1704011"]},"3c7da5d87776db54d04978700f01f3c41156d3e2":{"id":"3c7da5d87776db54d04978700f01f3c41156d3e2","date":"1999-01-01T12:12:00Z","text":"Data in a warehouse typically has multiple dimensions of interest, such as location, time, and product. It is well-recognized that these dimensions have hierarchies deened on them, such as \\store-city-state-region\" for location. The standard way to model such data is with a star\/snowwake schema. However, current approaches do not give a rst-class status to dimensions. Consequently, a substantial class of interesting queries involving dimension hierarchies and their interaction with the fact tables are quite verbose to write, hard to read, and diicult to optimize. We propose the SQL(H) model and a natural extension to the SQL query language, that gives a rst-class status to dimensions, and we pin down its semantics. Our model permits structural and schematic heterogeneity in dimension hierarchies, situations often arising in practice that cannot be modeled satisfactorily using the star\/snowwake approach. We show using examples that sophisticated queries involving dimension hierarchies and their interplay with aggregation can be expressed concisely in SQL(H). By comparison, expressing such queries in SQL would involve a union of numerous complex sequences of joins. Finally, we develop an eecient implementation strategy for computing SQL queries, based on an algorithm for hierarchical joins, and the use of dimension indexes. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and\/or special permission from the Endowment.","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Query language","SQL","Schematic","Star schema","VLDB"],"vec":[1.0645713552,0.061238096],"nodes":["1735239","1708593","1704011"]},"323011da81c396825df6ec185ac17a21f2272ea0":{"id":"323011da81c396825df6ec185ac17a21f2272ea0","date":"2007-01-01T12:12:00Z","text":"Privacy is a serious concern when microdata need to be released for ad hoc analyses. The privacy goals of existing privacy protection approaches (e.g., k-anonymity and l-diversity) are suitable only for categorical sensitive attributes. Since applying them directly to numerical sensitive attributes (e.g., salary) may result in undesirable information leakage, we propose privacy goals to better capture the need of privacy protection for numerical sensitive attributes. Complementing the desire for privacy is the need to support ad hoc aggregate analyses over microdata. Existing generalization-based anonymization approaches cannot answer aggregate queries with reasonable accuracy. We present a general framework of permutation-based anonymization to support accurate answering of aggregate queries and show that, for the same grouping, permutation-based techniques can always answer aggregate queries more accurately than generalization-based approaches. We further propose several criteria to optimize permutations for accurate answering of aggregate queries, and develop efficient algorithms for each criterion.","category_a":"Data Engineering","category_b":"DB","keywords":["Aggregate function","Algorithm","Context-sensitive help","Eisenstein's criterion","Hoc (programming language)","Information leakage","Microdata Corporation","Numerical analysis","Spectral leakage"],"vec":[1.1742303876,0.5649021643],"nodes":["1681771","1721062","1704011","1689202"]},"4e649f746c007f858492a504ee2da156b3d25e36":{"id":"4e649f746c007f858492a504ee2da156b3d25e36","date":"2018-01-01T12:12:00Z","text":"Employees that spend more time finding relevant data than analyzing it suffer a data discovery problem. The large volume of data in enterprises, and sometimes the lack of knowledge of the schemas aggravates this problem. Similar to how we navigate the Web today, we propose to identify semantic links that assist analysts in their discovery tasks. These links relate tables to each other, to facilitate navigating the schemas. They also relate data to external data sources such as ontologies and dictionaries, to help explain the schema meaning. We materialize the links in an enterprise knowledge graph, where they become available to analysts. The main challenge is how to find pairs of objects that are semantically related. We propose SEMPROP, a DAG of different components that find links based on syntactic and semantic similarities. SEMPROP is commanded by a semantic matcher which leverages word embeddings to find objects that are semantically related. To leverage word embeddings, we introduce coherent groups, a novel technique to combine them which works better than other state of the art alternatives for this problem. We implement SEMPROP as part of a discovery system we are building and conduct user studies, real deployments and a quantitative evaluation to understand the benefits of links for data discovery tasks, as well as the benefits of SEMPROP and coherent groups to find those links.","category_a":"","category_b":"Others","keywords":["Coherence (physics)","Data Sources","Dictionaries as Topic","Dictionary","Discovery system","Graph - visual representation","Knowledge Graph","Ontology (information science)","Physical object","Word embedding","World Wide Web","XML schema","benefit"],"vec":[0.3834376204,-0.1577237903],"nodes":["34568734","1801187","2523205","40394220","1743316","2033016","2168047","1695715","40384618"]},"0d43fe67820c2f2a2e8adda04be04579fa85582c":{"id":"0d43fe67820c2f2a2e8adda04be04579fa85582c","date":"2005-01-01T12:12:00Z","text":"","category_a":"Stream Data Management","category_b":"Others","keywords":["XML"],"vec":[-0.1925470263,-0.0221078554],"nodes":["2362833","1684012","1721062","1704011"]},"23d1255f1a0453ba96c6e6ef054903086f3656df":{"id":"23d1255f1a0453ba96c6e6ef054903086f3656df","date":"2015-01-01T12:12:00Z","text":"Protecting the privacy of individuals in graph structured data while making accurate versions of the data available is one of the most challenging problems in data privacy. Most efforts to date to perform this data release end up mired in complexity, overwhelm the signal with noise, and are not effective for use in practice. In this paper, we introduce a new method which guarantees differential privacy. It specifies a probability distribution over possible outputs that is carefully defined to maximize the utility for the given input, while still providing the required privacy level. The distribution is designed to form a 'ladder', so that each output achieves the highest 'rung' (maximum probability) compared to less preferable outputs. We show how our ladder framework can be applied to problems of counting the number of occurrences of subgraphs, a vital objective in graph analysis, and give algorithms whose cost is comparable to that of computing the count exactly. Our experimental study confirms that our method outperforms existing methods for counting triangles and stars in terms of accuracy, and provides solutions for some problems for which no effective method was previously known. The results of our algorithms can be used to estimate the parameters of suitable graph models, allowing synthetic graphs to be sampled.","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","Data model","Differential privacy","Effective method","Experiment","Graph (abstract data type)","Information privacy","Synthetic data"],"vec":[0.7977400308,0.6094815673],"nodes":["1689591","1709589","1738196","1704011","33285410"]},"0ec2c62121f8543864c84348835883df564aea14":{"id":"0ec2c62121f8543864c84348835883df564aea14","date":"2002-01-01T12:12:00Z","text":"XML employs a tree-structured data model, and, naturally, XML queries specify patterns of selection predicates on multiple elements related by a tree structure. Finding all occurrences of such a twig pattern in an XML database is a core operation for XML query processing. Prior work has typically decomposed the twig pattern into binary structural (parent-child and ancestor-descendant) relationships, and twig matching is achieved by: (i) using structural join algorithms to match the binary relationships against the XML database, and (ii) stitching together these basic matches. A limitation of this approach for matching twig patterns is that intermediate result sizes can get large, even when the input and output sizes are more manageable.In this paper, we propose a novel holistic twig join algorithm, TwigStack, for matching an XML query twig pattern. Our technique uses a chain of linked stacks to compactly represent partial results to root-to-leaf query paths, which are then composed to obtain matches for the twig pattern. When the twig pattern uses only ancestor-descendant relationships between elements, TwigStack is I\/O and CPU optimal among all sequential algorithms that read the entire input: it is linear in the sum of sizes of the input lists and the final result list, but independent of the sizes of intermediate results. We then show how to use (a modification of) B-trees, along with TwigStack, to match query twig patterns in sub-linear time. Finally, we complement our analysis with experimental results on a range of real and synthetic data, and query twig patterns.","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","B-tree","Central processing unit","Data model","Database","Holism","Input\/output","Join (SQL)","Pattern matching","Synthetic data","Time complexity","Tree structure","Twig","Twig (database)","XML","XML database"],"vec":[0.9979775904,0.1061482279],"nodes":["2362833","1721062","1704011"]},"007d73c91a1bf90d72eb59fbdd8791a4b009f363":{"id":"007d73c91a1bf90d72eb59fbdd8791a4b009f363","date":"2012-01-01T12:12:00Z","text":"Many algorithms are available to learn deep hierarchies of features from unlabeled data, especially images. In many cases, these algorithms involve multi-layered networks of features (e.g., neural networks) that are sometimes tricky to train and tune and are difficult to scale up to many machines effectively. Recently, it has been found that K-means clustering can be used as a fast alternative training method. The main advantage of this approach is that it is very fast and easily implemented at large scale. On the other hand, employing this method in practice is not completely trivial: K-means has several limitations, and care must be taken to combine the right ingredients to get the system to work well. This chapter will summarize recent results and technical tricks that are needed to make effective use of K-means clustering for learning large-scale representations of images. We will also connect these results to other well-known algorithms to make clear when K-means can be most useful and convey intuitions about its behavior that are useful for debugging and engineering new systems.","category_a":"Neural Networks: Tricks of the Trade","category_b":"Others","keywords":["Algorithm","Artificial neural network","Cluster analysis","Debugging","K-means clustering","Teaching method"],"vec":[-0.4575633846,0.2172749264],"nodes":["5574038","1701538"]},"45e3f381d16e6d6db5449b7a44b7bdf294a8a822":{"id":"45e3f381d16e6d6db5449b7a44b7bdf294a8a822","date":"2017-01-01T12:12:00Z","text":"We propose a generalization of neural network sequence models. Instead of predicting one symbol at a time, our multi-scale model makes predictions over multiple, potentially overlapping multi-symbol tokens. A variation of the byte-pair encoding (BPE) compression algorithm is used to learn the dictionary of tokens that the model is trained with. When applied to language modelling, our model has the flexibility of character-level models while maintaining many of the performance benefits of word-level models. Our experiments show that this model performs better than a regular LSTM on language modeling tasks, especially for smaller models.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Artificial neural network","Byte","Byte pair encoding","Data compression","Dictionary","Experiment","Language model"],"vec":[-0.4021538789,0.1550248502],"nodes":["3158246","3494481","1777528","1751762"]},"dc4a6027f4b868e544d2659181e52fa2e716a517":{"id":"dc4a6027f4b868e544d2659181e52fa2e716a517","date":"2002-01-01T12:12:00Z","text":"XML queries typically specify patterns of selection predicates on multiple elements that have some speciied tree structured relationships. The primitive tree structured relationships are parent-child and ancestor-descendant, and nding all occurrences of these structural relationships in an XML database is a core operation for XML query processing. In this paper, we develop two families of structural join algorithms for this task: tree-merge and stack-tree. The tree-merge algorithms are a natural extension of traditional merge joins and the recently proposed multi-predicate merge joins, while the stack-tree algorithms have no counterpart in traditional relational join processing. We present experimental results on a range of data and queries using (i) the Timber native XML query engine built on top of SHORE, and (ii) a commercial relational database system. In all cases, our structural join algorithms turn out to be vastly superior to traditional traversal-style algorithms. Structural join algorithms are also much more eecient than using traditional join algorithms, implemented in relational databases, for the same task. Finally, we show that while, in some cases, tree-merge algorithms can have performance comparable to stack-tree algorithms, in many cases they are considerably worse. This behavior is explained by analytical results that demonstrate that, on sorted inputs, the stack-tree algorithms have worst-case I\/O and CPU complexities linear in the sum of the sizes of inputs and output, while the tree-merge algorithms do not have the same guarantee.","category_a":"","category_b":"Others","keywords":["Algorithm","Analysis","Best, worst and average case","Binary tree","CPU (central processing unit of computer system)","Central processing unit","Database","Extensible Markup Language","Join (SQL)","Memory-mapped I\/O","Merge","Pattern matching","Question (inquiry)","Relational algebra","Relational database","Relational database management system","Tree (data structure)","XML","XML database","algorithm"],"vec":[1.0210205113,0.0224380325],"nodes":["1978097","1735239","1721062","2042232","1704011","1683593"]},"69de7827bb2d7508d0f15ce634d9594f8d0bb4b4":{"id":"69de7827bb2d7508d0f15ce634d9594f8d0bb4b4","date":"2005-01-01T12:12:00Z","text":"This paper describes an algorithm that tracks and localizes a helicopter using a ground-based trinocular camera array. The three cameras are placed independently in an arbitrary arrangement that allows each camera to view the helicopter\u2019s flight volume. The helicopter then flies an unplanned path that allows the cameras to self-survey utilizing an algorithm based on structure from motion and bundle adjustment. This yields the camera\u2019s extrinsic parameters allowing for real-time positioning of the helicopter\u2019s position in a camera array based coordinate frame. In fielded experiments, there is less than a 2m RMS tracking error and the update rate of 20Hz is comparable to DGPS update rates. This system has successfully been integrated with an IMU to provide a positioning system for autonomous hovering.","category_a":"FSR","category_b":"Others","keywords":["Algorithm","Autonomous car","Bundle adjustment","Camera resectioning","Experiment","Global Positioning System","Positioning system","Structure from motion"],"vec":[-0.0369259732,-1.1144179052],"nodes":["2753674","40795278","1788158","5574038","1701538","1738444"]},"71439278086b7fc84eedc8aeba377747e00b8403":{"id":"71439278086b7fc84eedc8aeba377747e00b8403","date":"2011-01-01T12:12:00Z","text":"Microblogs are a tremendous repository of user-generated content about world events. However, for people trying to understand events by querying services like Twitter, a chronological log of posts makes it very difficult to get a detailed understanding of an event. In this paper, we present TwitInfo, a system for visualizing and summarizing events on Twitter. TwitInfo allows users to browse a large collection of tweets using a timeline-based display that highlights peaks of high tweet activity. A novel streaming algorithm automatically discovers these peaks and labels them meaningfully using text from the tweets. Users can drill down to subevents, and explore further via geolocation, sentiment, and popular URLs. We contribute a recall-normalized aggregate sentiment visualization to produce more honest sentiment overviews. An evaluation of the system revealed that users were able to reconstruct meaningful summaries of events in a small amount of time. An interview with a Pulitzer Prize-winning journalist suggested that the system would be especially useful for understanding a long-running event and for identifying eyewitnesses. Quantitatively, our system can identify 80-100% of manually labeled peaks, facilitating a relatively complete view of each event studied.","category_a":"CHI","category_b":"HCI","keywords":["Algorithm","Browsing","Event chain methodology","Geolocation","Streaming algorithm","User-generated content"],"vec":[0.4299777717,-0.667249548],"nodes":["40030541","35609041","2259673","1743286","2033016","34205614"]},"cf280435c471ee099148c4eb9eb2e106ccb2b218":{"id":"cf280435c471ee099148c4eb9eb2e106ccb2b218","date":"2017-01-01T12:12:00Z","text":"We introduce HoME: a Household Multimodal Environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, soundbased navigation, robotics, multi-agent learning, and more. We hope HoME better enables artificial agents to learn as humans do: in an interactive, multimodal, and richly contextualized setting.","category_a":"ArXiv","category_b":"Journal","keywords":["Intelligent agent","Multi-agent system","Multimodal interaction","Open-source software","Reinforcement learning","Robotics"],"vec":[-0.3315279992,0.3103753768],"nodes":["33622538","3439053","12679121","2970150","39500693","3367628","1680808","1777528","1760871"]},"1b74e7b2547373ff3df6fc293be7bed49075053d":{"id":"1b74e7b2547373ff3df6fc293be7bed49075053d","date":"2007-01-01T12:12:00Z","text":"Poor quality data is prevalent in databases due to a variety of reasons, including transcription errors, lack of standards for recording database fields, etc. To be able to query and integrate such data, considerable recent work has focused on the record linkage problem, i.e., determine if two entities represented as relational records are approximately the same. Often entities are represented as groups of relational records, rather than individual relational records, e.g., households in a census survey consist of a group of persons. We refer to the problem of determining if two entities represented as groups are approximately the same as group linkage. Intuitively, two groups can be linked to each other if (i) there is high enough similarity between \"matching\" pairs of individual records that constitute the two groups, and (ii) there is a large fraction of such matching record pairs. In this paper, we formalize this intuition and propose a group linkage measure based on bipartite graph matching. Given a data set consisting of a large number of groups, efficiently finding groups with a high group linkage similarity to an input query group requires quickly eliminating the many groups that are unlikely to be desired matches. To enable this task, we present simpler group similarity measures that can be used either during fast pre-processing steps or as approximations to our proposed group linkage measure. These measures can be easily instantiated using SQL, permitting our techniques to be implemented inside the database system itself. We experimentally validate the utility of our measures and techniques using a variety of real and synthetic data sets.","category_a":"Data Engineering","category_b":"DB","keywords":["Approximation","Database","Domain Name System Security Extensions","Entity","Linkage (software)","Preprocessor","SQL","Synthetic data","Transcription (software)"],"vec":[0.9989811218,0.1054666247],"nodes":["1791452","1721062","1784227","1704011"]},"91ff05cf0512789727dd010270e95dfc02301318":{"id":"91ff05cf0512789727dd010270e95dfc02301318","date":"2001-01-01T12:12:00Z","text":"While symbolic parsers can be viewed as deduction systems, this view is less natural for probabilistic parsers. We present a view of parsing as directed hypergraph analysis which naturally covers both symbolic and probabilistic parsing. We illustrate the approach by showing how a dynamic extension of Dijkstra\u2019s algorithm can be used to construct a probabilistic chart parser with an O(n) time bound for arbitrary PCFGs, while preserving as much of the flexibility of symbolic chart parsers as allowed by the inherent ordering of probabilistic dependencies.","category_a":"IWPT","category_b":"Others","keywords":["Algorithm","Chart parser","Dijkstra's algorithm","Natural deduction","Parsing","Stochastic context-free grammar"],"vec":[0.0574669789,-1.1120881501],"nodes":["38666915","1812612"]},"e21a6a56e24f321286a50f2a1811ae66ef6245f2":{"id":"e21a6a56e24f321286a50f2a1811ae66ef6245f2","date":"2011-01-01T12:12:00Z","text":"The spike and slab Restricted Boltzmann Machine (ssRBM) is defined by having both a real valued \u201cslab\u201d variable and a binary \u201cspike\u201d variable associated with each unit in the hidden layer. Earlier work exploring a simple incarnation of the ssRBM model family demonstrated its utility as a model of natural images. In this work, we explore an extension of the ssRBM model \u2013 the \u03bc-ssRBM \u2013 that includes additional terms to the energy function which we use, in part, to address one of the potential drawback of the original ssRBM introduced in (Courville et al., 2010), specifically, the lack of a guarantee that the model defines a valid density over the whole data domain. We find that while it is possible to parametrize the model to guarantee that all conditionals of the model are well defined, loosening this constraint empirically yields better classification performance with the CIFAR-10 image dataset.","category_a":"","category_b":"Others","keywords":["Action potential","Boltzmann machine","Data domain","Man-Machine Systems","Mathematical optimization","Restricted Boltzmann machine","Slab allocation"],"vec":[-0.4085727935,0.4165547399],"nodes":["1760871","32837403","1751762"]},"3fd5307b2705971e543d16e7350651945b257f14":{"id":"3fd5307b2705971e543d16e7350651945b257f14","date":"2004-01-01T12:12:00Z","text":"\u00a9 2004 Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux, Jean-Francois Paiement, Pascal Vincent, Marie Ouimet. Tous droits r\u00e9serv\u00e9s. All rights reserved. Reproduction partielle permise avec citation du document source, incluant la notice \u00a9. Short sections may be quoted without explicit permission, if full credit, including \u00a9 notice, is given to the source. S\u00e9rie Scientifique Scientific Series 2004s-27","category_a":"","category_b":"Others","keywords":["Dimensionality reduction","Epilepsy","Hallermann's Syndrome","JEAN","Lactic acid","Object Pascal","Schizophrenia","citation","newton per square metre"],"vec":[-0.1774646964,-0.2043206028],"nodes":["1751762","2460212","7245737","39651651","1724875","2120888"]},"1683b929bd2c1fd5f326ee9501ef24c2b613b5ef":{"id":"1683b929bd2c1fd5f326ee9501ef24c2b613b5ef","date":"2000-01-01T12:12:00Z","text":"In the Support Vector Machines (SVM) framework, the positive-definite kernel can be seen as representing a fixed similarity measure between two patterns, and a discriminant function is obtained by taking a linear combination of the kernels computed at training examples called support vectors. Here we investigate learning architectures in which the kernel functions can be replaced by more general similarity measures that can have arbitrary internal parameters. The training criterion used in SVMs is not appropriate for this purpose so we adopt the simple criterion that is generally used when training neural networks for classification tasks. Several experiments are performed which show that such Neural Support Vector Networks perform similarly to SVMs while requiring significantly fewer support vectors, even when the similarity measure has no internal parameters.","category_a":"CNN","category_b":"Others","keywords":["Artificial neural network","Discriminant","Eisenstein's criterion","Experiment","Network architecture","Similarity measure","Support vector machine"],"vec":[-0.3686575411,0.3440630043],"nodes":["1724875","1751762"]},"8e0eacf11a22b9705a262e908f17b1704fd21fa7":{"id":"8e0eacf11a22b9705a262e908f17b1704fd21fa7","date":"2016-01-01T12:12:00Z","text":"We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech\u2013two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Artificial neural network","Benchmark (computing)","Data center","Deep learning","Dynamic dispatch","End-to-end encryption","End-to-end principle","Experiment","Graphics processing unit","Pipeline (computing)","Speech recognition","Super Robot Monkey Team Hyperforce Go!","Transcription (software)"],"vec":[-0.3474411911,-0.188877583],"nodes":["2698777","2432216","5697774","1714272","2314133","2301680","1710611","35977287","5574038","2322582","34892314","9695761","3275727","2910729","2893056","34601942","3041255","3081566","1898780","2209076","2013727","1701538","1955694","3168041","3176767","34042420","2638806","2100685","2264597","1747751","1734275","1808053","1719582","2816770","1755465","2045210","2042558"]},"4ca3c1040a4bc0d1ae200d26d1c18fbc6df0e95a":{"id":"4ca3c1040a4bc0d1ae200d26d1c18fbc6df0e95a","date":"2013-01-01T12:12:00Z","text":"Intel has moved to a collaboration model with universities consisting of \"Science and Technology Centers\" (ISTCs). These are located at a \"hub\" university with participation from other universities, contain embedded Intel personnel, and are focused on some research theme. Intel held a national competition for a 5th Science and Technology center in 2012 and selected a proposal from M.I.T. with a theme of \"Big Data\". This paper presents the big data vision of this technology center and the execution plan for the first few years.","category_a":"SIGMOD","category_b":"DB","keywords":["Big data","Ethernet hub","Query plan"],"vec":[0.2445768084,-0.2995907401],"nodes":["1695715","2033016","1719384"]},"77ae592ec975dfb14d494667e0b019af19da9075":{"id":"77ae592ec975dfb14d494667e0b019af19da9075","date":"2012-01-01T12:12:00Z","text":"As data analytics becomes mainstream, and the complexity of the underlying data and computation grows, it will be increasingly important to provide tools that help analysts understand the underlying reasons when they encounter errors in the result. While data provenance has been a large step in providing tools to help debug complex workflows, its current form has limited utility when debugging aggregation operators that compute a single output from a large collection of inputs. Traditional provenance will return the entire input collection, which has very low precision. In contrast, users are seeking precise descriptions of the inputs that caused the errors. We propose a Ranked Provenance System, which identifies subsets of inputs that influenced the output error, describes each subset with human readable predicates and orders them by contribution to the error. In this demonstration, we will present DBWipes, a novel data cleaning system that allows users to execute aggregate queries, and interactively detect, understand, and clean errors in the query results. Conference attendees will explore anomalies in campaign donations from the current US presidential election and in readings from a 54-node sensor deployment.","category_a":"VLDB","category_b":"DB","keywords":["Computation","Debugging","Human-readable medium","Interactivity","Sputter cleaning"],"vec":[0.638435617,-0.4730559198],"nodes":["39708150","2033016","1695715"]},"76765f06161c2f1b153056b86ce665102414ec73":{"id":"76765f06161c2f1b153056b86ce665102414ec73","date":"2011-01-01T12:12:00Z","text":"Microblogs such as Twitter are a tremendous repository of usergenerated content. Increasingly, we see tweets used as data sources for novel applications such as disaster mapping, brand sentiment analysis, and real-time visualizations. In each scenario, the workflow for processing tweets is ad-hoc, and a lot of unnecessary work goes into repeating common data processing patterns. We introduce TweeQL, a stream query processing language that presents a SQL-like query interface for unstructured tweets to generate structured data for downstream applications. We have built several tools on top of TweeQL, most notably TwitInfo, an event timeline generation and exploration interface that summarizes events as they are discussed on Twitter. Our demonstration will allow the audience to interact with both TweeQL and TwitInfo to convey the value of data embedded in tweets.","category_a":"","category_b":"Others","keywords":["Data Sources","Data model","Database","Downstream (software development)","Hoc (programming language)","Interface Device Component","Question (inquiry)","SQL","Sentiment analysis","Structured Query Language","TimeLine Fluoride Releasing Resin"],"vec":[0.4646220327,-0.6628556414],"nodes":["35306555","35609041","2259673","1743286","2033016","31577736"]},"3d6e2d4c1cde661fea2c8b1c93528aa24bb87aff":{"id":"3d6e2d4c1cde661fea2c8b1c93528aa24bb87aff","date":"2015-01-01T12:12:00Z","text":"The notion of heavy hitters\u2014items that make up a large fraction of the population\u2014has been successfully used in a variety of applications across sensor and RFID monitoring, network data analysis, event mining, and more. Yet this notion often fails to capture the semantics we desire when we observe data in the form of correlated pairs. Here, we are interested in items that are conditionally frequent: when a particular item is frequent within the context of its parent item. In this work, we introduce and formalize the notion of conditional heavy hitters to identify such items, with applications in network monitoring and Markov chain modeling. We explore the relationship between conditional heavy hitters and other related notions in the literature, and show analytically and experimentally the usefulness of our approach. We introduce several algorithm variations that allow us to efficiently find conditional heavy hitters for input data with very different characteristics, and provide analytical results for their performance. Finally, we perform experimental evaluations with several synthetic and real datasets to demonstrate the efficacy of our methods and to study the behavior of the proposed algorithms for different types of data.","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Markov chain","Radio-frequency identification","Synthetic data","Utility"],"vec":[0.964610205,0.325287549],"nodes":["1701357","1709589","1725167","1704011"]},"5115f3ff1ac45486a50ad3834a40490f9ca4bcea":{"id":"5115f3ff1ac45486a50ad3834a40490f9ca4bcea","date":"2014-01-01T12:12:00Z","text":"We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer\u2019s input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network\u2019s depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.","category_a":"NIPS","category_b":"ML","keywords":["Activation function","Artificial neural network","Computation","Deep learning","Feedforward neural network","Map","Neural Networks","Piecewise linear continuation","Rectifier"],"vec":[-0.4191277013,0.3790979202],"nodes":["1784667","1996134","1979489","1751762"]},"b3610b7650533631ef7a63adf21ec0e722f4d9c0":{"id":"b3610b7650533631ef7a63adf21ec0e722f4d9c0","date":"2012-01-01T12:12:00Z","text":"In data-mining applications, we are frequently faced with a large fraction of missing entries in the data matrix, which is problematic for most discriminant machine learning algorithms. A solution that we explore in this paper is the use of a generative model (a mixture of Gaussians) to compute the conditional expectation of the missing variables given the observed variables. Since training a Gaussian mixture with many different patterns of missing values can be computationally very expensive, we introduce a spanning-tree based algorithm that significantly speeds up training in these conditions. We also observe that good results can be obtained by using the generative model to fill-in the missing values for a separate discriminant learning algorithm.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Data mining","Discriminant","Generative model","Machine learning","Missing data","Mixture model","Spanning Tree Protocol","Spanning tree"],"vec":[-0.3467739168,0.3554437441],"nodes":["2460212","1760871","1751762"]},"907e35afd9633cd0819e0add854081bae738d320":{"id":"907e35afd9633cd0819e0add854081bae738d320","date":"2016-01-01T12:12:00Z","text":"We study the problem of generalization to a zero-data task. A zero-data task corresponds to a learning problem for which no training data is available and only a description d(y) of the task y is given. Though learning in such a scenario might seem impossible, such tasks can actually be found in the context of a number of fields: for instance, in drug discovery [1], where one needs to decide whether a candidate molecule x is a possible cure for a new disease y, or in the so-called \u201ccold start\u201d problem of recommender systems, where one needs to determine whether a new item y (movie, album, book, etc.) should be recommended to a user x or not. It is also related to the one sample learning problem [2], where only one training example per class is provided in a classification setting. There are at least two approaches to learning zero-data tasks, which correspond to two different views of the problem. We refer to these approaches as the input space view and the model indexation view. The input space view considers the input of the learning problem to be the concatenation of a sample with the corresponding task description [x, d(y)]1. The desired output for the respective task y, noted z(y), can be 1 if sample x is from class y and 0 otherwise for a classification setting, or a value of the dependent variable associated with x in task y, in a regression setting. Then, to obtain an answer for a new task y from a sample x, the input [x, d(y)] is provided to the trained model. The model indexation view considers the model fy(x) for a task y to be a function of its description d(y), i.e. fy(x) = gd(y)(x). For example, gd(y)(x) could be a Gaussian density where the mean is a function of d(y), or it could be a linear classifier (perceptron) for which the weights are a function of gd(y). To train the \u201cmodel of models\u201d gd(y)(x), one can simply optimize its parameters to minimize the average cost for each pair of samples and tasks from the training set 1 T \u2211","category_a":"","category_b":"Others","keywords":["Arabic numeral 0","Cold start","Concatenation","Drug Discovery","Generalization (Psychology)","Linear classifier","Model\u2013view\u2013controller","Neural Network Simulation","Perceptron","Recommender system","Test set"],"vec":[-0.1233794951,0.2329487499],"nodes":["1777528","1761978","1751762"]},"3ec4a5d29a3d897595f8f02aff0984d249c179c4":{"id":"3ec4a5d29a3d897595f8f02aff0984d249c179c4","date":"2012-01-01T12:12:00Z","text":"Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns low-level translationally invariant features which are then given as inputs to multiple, fixed-tree RNNs in order to compose higher order features. RNNs can be seen as combining convolution and pooling into one efficient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during training and testing than comparable architectures such as two-layer CNNs.","category_a":"NIPS","category_b":"ML","keywords":["Artificial neural network","Convolution","Deep learning","High- and low-level","Modality (human\u2013computer interaction)","Outline of object recognition","Recursion","Recursive neural network"],"vec":[-0.423282926,0.0484987511],"nodes":["2166511","2570381","3100049","1812612","1701538"]},"d579adf7a2c5cce3bb17482bb757f15bff45b131":{"id":"d579adf7a2c5cce3bb17482bb757f15bff45b131","date":"2010-01-01T12:12:00Z","text":"We present a new Java-based open source toolkit for phrase-based machine translation. The key innovation provided by the toolkit is to use APIs for integrating new features (\/knowledge sources) into the decoding model and for extracting feature statistics from aligned bitexts. The package includes a number of useful features written to these APIs including features for hierarchical reordering, discriminatively trained linear distortion, and syntax based language models. Other useful utilities packaged with the toolkit include: a conditional phrase extraction system that builds a phrase table just for a specific dataset; and an implementation of MERT that allows for pluggable evaluation metrics for both training and evaluation with built in support for a variety of metrics (e.g., TERp, BLEU, METEOR).","category_a":"NAACL","category_b":"ComLing","keywords":["BLEU","Discriminative model","Distortion","Java","Machine translation","Meteor","Multi-Environment Real-Time","Open-source software","Statistical machine translation"],"vec":[-0.9497675588,-0.431484248],"nodes":["3208422","1947267","1746807","1812612"]},"2579b2066d0fcbeda5498f5053f201b10a8e254b":{"id":"2579b2066d0fcbeda5498f5053f201b10a8e254b","date":"2016-01-01T12:12:00Z","text":"The manual labeling of data is and will remain a costly endeavor. For this reason, semi-supervised learning remains a topic of practical importance. The recently proposed Ladder Network is one such approach that has proven to be very successful. In addition to the supervised objective, the Ladder Network also adds an unsupervised objective corresponding to the reconstruction costs of a stack of denoising autoencoders. Although the empirical results are impressive, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. In order to help elucidate and disentangle the different ingredients in the Ladder Network recipe, this paper presents an extensive experimental investigation of variants of the Ladder Network in which we replace or remove individual components to gain more insight into their relative importance. We find that all of the components are necessary for achieving optimal performance, but they do not contribute equally. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connection, followed by the application of noise, and finally the choice of what we refer to as the \u2018combinator function\u2019 in the decoder path. We also find that as the number of labeled training examples increases, the lateral connections and reconstruction criterion become less important, with most of the improvement in generalization being due to the injection of noise in each layer. Furthermore, we present a new type of combinator function that outperforms the original design in both fullyand semi-supervised tasks, reducing record test error rates on Permutation-Invariant MNIST to 0.57% for the supervised setting, and to 0.97% and 1.0% for semisupervised settings with 1000 and 100 labeled examples respectively.","category_a":"ICML","category_b":"ML","keywords":["Combinatory logic","Eisenstein's criterion","Electronic filter topology","MNIST database","Network architecture","Noise reduction","Semi-supervised learning","Supervised learning","Unsupervised learning"],"vec":[-0.4062052671,0.3817758092],"nodes":["2719055","3275727","2616163","1760871","1751762"]},"2525c025f11aec60cff428271ca851381b92008f":{"id":"2525c025f11aec60cff428271ca851381b92008f","date":"2014-01-01T12:12:00Z","text":"Fine-grained, record-oriented write-ahead logging, as exemplified by systems like ARIES, has been the gold standard for relational database recovery. In this paper, we show that in modern high-throughput transaction processing systems, this is no longer the optimal way to recover a database system. In particular, as transaction throughputs get higher, ARIES-style logging starts to represent a non-trivial fraction of the overall transaction execution time. We propose a lighter weight, coarse-grained command logging technique which only records the transactions that were executed on the database. It then does recovery by starting from a transactionally consistent checkpoint and replaying the commands in the log as if they were new transactions. By avoiding the overhead of fine-grained logging of before and after images (both CPU complexity as well as substantial associated 110), command logging can yield significantly higher throughput at run-time. Recovery times for command logging are higher compared to an ARIEs-style physiological logging approach, but with the advent of high-availability techniques that can mask the outage of a recovering node, recovery speeds have become secondary in importance to run-time performance for most applications. We evaluated our approach on an implementation of TPCC in a main memory database system (VoltDB), and found that command logging can offer 1.5 x higher throughput than a main-memory optimized implementation of ARIEs-style physiological logging.","category_a":"Data Engineering","category_b":"DB","keywords":["Algorithms for Recovery and Isolation Exploiting Semantics","Application checkpointing","Computer data storage","Downtime","High availability","High-throughput computing","In-memory database","Online transaction processing","Overhead projector","Relational database","Run time (program lifecycle phase)","Throughput","Transaction processing","Transaction processing system","VoltDB","Write-ahead logging"],"vec":[0.6797003455,-0.6153503617],"nodes":["37433143","35085622","2033016","1695715"]},"0c23ebb3abf584fa5e0fde558584befc94fb5ea2":{"id":"0c23ebb3abf584fa5e0fde558584befc94fb5ea2","date":"2011-01-01T12:12:00Z","text":"Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-theart performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Artificial neural network","GiST","Natural language","Parse tree","Parsing","Recursion","Recursive neural network","Treebank"],"vec":[-0.4054172099,-0.5044057182],"nodes":["2166511","2585821","1701538","1812612"]},"022029f1211970dbf29824f6510fe6dc7718190e":{"id":"022029f1211970dbf29824f6510fe6dc7718190e","date":"2010-01-01T12:12:00Z","text":"We present a new Java-based open source toolkit for phrase-based machine translation. The key innovation provided by the toolkit is to use APIs for integrating new features (\/knowledge sources) into the decoding model and for extracting feature statistics from aligned bitexts. The package was used to develop a number of useful features written to these APIs including features for hierarchical reordering, discriminatively trained linear distortion, and syntax based language models. Useful utilities distributed with the toolkit include: a conditional phrase extraction system that builds a phrase table just for a specific dataset; and an implementation of MERT that allows for pluggable evaluation metrics for both training and evaluation with built in support for a variety of metrics (e.g., TERp, BLEU, METEOR).","category_a":"","category_b":"Others","keywords":["BLEU","Discriminative model","Distortion","Java","Java Programming Language","Machine translation","Meteor","Multi-Environment Real-Time","Open-source software","PECR gene","Silo (dataset)","Statistical machine translation"],"vec":[-0.951220126,-0.4332000926],"nodes":["3208422","1947267","1746807","1812612"]},"3b98dc29eb5f95470d12d19ae528674129ca0411":{"id":"3b98dc29eb5f95470d12d19ae528674129ca0411","date":"2005-01-01T12:12:00Z","text":"We present a generative probabilistic model for the unsupervised learning of hierarchical natural language syntactic structure. Unlike most previous work, we do not learn a context-free grammar, but rather induce a distributional model of constituents which explicitly relates constituent yields and their linear contexts. Parameter search with EM produces higher quality analyses for human language data than those previously exhibited by unsupervised systems, giving the best published unsupervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher constituent F1 of 71% on non-trivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task. 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.","category_a":"Pattern Recognition","category_b":"Others","keywords":["Automatic Transmitter Identification System (television)","Context-free grammar","Context-free language","Distributional semantics","Grammar induction","Natural language","Parsing","Part-of-speech tagging","Pattern recognition","Statistical model","Treebank","Unsupervised learning"],"vec":[-0.4722157971,-0.7559673213],"nodes":["38666915","1812612"]},"265efd16c16dc942705246a57337d323722003cb":{"id":"265efd16c16dc942705246a57337d323722003cb","date":"2014-01-01T12:12:00Z","text":"First-generation streaming systems did not pay much attention to state management via ACID transactions (e.g., [3, 4]). S-Store is a data management system that combines OLTP transactions with stream processing. To create S-Store, we begin with H-Store, a main-memory transaction processing engine, and add primitives to support streaming. This includes triggers and transaction workflows to implement push-based processing, windows to provide a way to bound the computation, and tables with hidden state to implement scoping for proper isolation. This demo explores the benefits of this approach by showing how a na\u00efve implementation of our benchmarks using only H-Store can yield incorrect results. We also show that by exploiting push-based semantics and our implementation of triggers, we can achieve significant improvement in transaction throughput. We demo two modern applications: (i) leaderboard maintenance for a version of \u201cAmerican Idol\u201d, and (ii) a city-scale bicycle rental scenario.","category_a":"VLDB","category_b":"DB","keywords":["ACID","Benchmark (computing)","Computation","Computer data storage","Database","H-Store","Microsoft Windows","Naivety","NewSQL","Online transaction processing","Scope (computer science)","Software transactional memory","State management","Stream processing","Throughput","Transaction processing","Velocity"],"vec":[0.6329889863,-0.4484627705],"nodes":["2109957","40444545","1746961","2033016","1740962","2960286","1774210","1695715","36083909","1773620","1793745","39049654","2031287"]},"e2b7f37cd97a7907b1b8a41138721ed06a0b76cd":{"id":"e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","date":"2010-01-01T12:12:00Z","text":"We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.","category_a":"Journal of Machine Learning Research","category_b":"Others","keywords":["Algorithm","Autoencoder","Bayesian network","Benchmark (computing)","Bridging (networking)","Deep belief network","Edge detection","Eisenstein's criterion","Experiment","Focus stacking","Noise reduction","Unsupervised learning"],"vec":[-0.4131306995,0.3603412892],"nodes":["1724875","1777528","2453026","1751762","1798462"]},"277fc66c995fe0c5b79e68b328259159cb412c85":{"id":"277fc66c995fe0c5b79e68b328259159cb412c85","date":"2001-01-01T12:12:00Z","text":"","category_a":"DBPL","category_b":"Others","keywords":["XML"],"vec":[-0.1920011999,-0.0128846099],"nodes":["1735239","1708593","1704011","3774354"]},"84458cd529206ec4897b90be191b089e8966ddc4":{"id":"84458cd529206ec4897b90be191b089e8966ddc4","date":"2014-01-01T12:12:00Z","text":"Recursive Neural Networks have recently obtained state of the art performance on several natural language processing tasks. However, because of their feedforward architecture they cannot correctly predict phrase or word labels that are determined by context. This is a problem in tasks such as aspect-specific sentiment classification which tries to, for instance, predict that the word Android is positive in the sentence Android beats iOS. We introduce global belief recursive neural networks (GB-RNNs) which are based on the idea of extending purely feedforward neural networks to include one feedbackward step during inference. This allows phrase level predictions and representations to give feedback to words. We show the effectiveness of this model on the task of contextual sentiment analysis. We also show that dropout can improve RNN training and that a combination of unsupervised and supervised word vector representations performs better than either alone. The feedbackward step improves F1 performance by 3% over the standard RNN on this task, obtains state-of-the-art performance on the SemEval 2013 challenge and can accurately predict the sentiment of specific entities.","category_a":"NIPS","category_b":"ML","keywords":["Android","Artificial neural network","Dropout (neural networks)","Entity","Feedforward neural network","Natural language","Natural language processing","Neural Networks","Recursion","SemEval","Sentiment analysis","Statistical classification","The Sentence","Word embedding","iOS"],"vec":[-0.5805145229,-0.3658241929],"nodes":["2896063","2166511","1812612"]},"00a28138c74869cfb8236a18a4dbe3a896f7a812":{"id":"00a28138c74869cfb8236a18a4dbe3a896f7a812","date":"2013-01-01T12:12:00Z","text":"Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.","category_a":"CoNLL","category_b":"Others","keywords":["Artificial neural network","Entity","Mathematical morphology","Natural language processing","Recursion","Recursive neural network"],"vec":[-0.6289182379,-0.3820000702],"nodes":["1821711","2166511","1812612"]},"3d3b8fe8b5dd2d518f3a186f63553eb26a49968f":{"id":"3d3b8fe8b5dd2d518f3a186f63553eb26a49968f","date":"2015-01-01T12:12:00Z","text":"We introduce a novel training principle for generative probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework generalizes Denoising Auto-Encoders (DAE) and is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution is a conditional distribution that generally involves a small move, so it has fewer dominant modes and is unimodal in the limit of small moves. This simplifies the learning problem, making it less like density estimation and more akin to supervised function approximation, with gradients that can be obtained by backprop. The theorems provided here provide a probabilistic interpretation for denoising autoencoders and generalize them; seen in the context of this framework, auto-encoders that learn with injected noise are a special case of GSNs and can be interpreted as generative models. The theorems also provide an interesting justification for dependency networks and generalized pseudolikelihood and define an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. Experiments validating these theoretical results are conducted on both synthetic datasets and image datasets. The experiments employ a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but that allows training to proceed with backprop through a recurrent neural network with noise injected inside and without the need for layerwise pretraining.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithmic inference","Artificial neural network","Backpropagation","Boltzmann machine","Encoder","Experiment","Generative model","Gibbs sampling","HIPPI","Image gradient","Markov chain","Noise reduction","Recurrent neural network","Sampling (signal processing)","Social network","Stationary process","Synthetic data"],"vec":[-0.2656599783,0.6994374165],"nodes":["1815021","1751762","1685369","2965424","7369640","35097114","1724875"]},"420a94968bd8671c8071d9c735a095ee475d0b20":{"id":"420a94968bd8671c8071d9c735a095ee475d0b20","date":"2005-01-01T12:12:00Z","text":"Kalman filters are a workhorse of robotics and are routinely used in state-estimation problems. However, their performance critically depends on a large number of modeling parameters which can be very difficult to obtain, and are often set via significant manual tweaking and at a great cost of engineering time. In this paper, we propose a method for automatically learning the noise parameters of a Kalman filter. We also demonstrate on a commercial wheeled rover that our Kalman filter\u2019s learned noise covariance parameters\u2014obtained quickly and fully automatically\u2014significantly outperform an earlier, carefully and laboriously hand-designed one.","category_a":"Robotics: Science and Systems","category_b":"Others","keywords":["Kalman filter","Quantum decoherence","Robotics","Tweaking"],"vec":[-0.2977001115,0.2559712208],"nodes":["1689992","5574038","1764963","1701538","1738444"]},"87a937dff0ecffefd8a7e4ca4ce068aca9731a0a":{"id":"87a937dff0ecffefd8a7e4ca4ce068aca9731a0a","date":"2016-01-01T12:12:00Z","text":"Microtask crowdsourcing has enabled dataset advances in social science and machine learning, but existing crowdsourcing schemes are too expensive to scale up with the expanding volume of data. To scale and widen the applicability of crowdsourcing, we present a technique that produces extremely rapid judgments for binary and categorical labels. Rather than punishing all errors, which causes workers to proceed slowly and deliberately, our technique speeds up workers' judgments to the point where errors are acceptable and even expected. We demonstrate that it is possible to rectify these errors by randomizing task order and modeling response latency. We evaluate our technique on a breadth of common labeling tasks such as image verification, word similarity, sentiment analysis and topic classification. Where prior work typically achieves a 0.25x to 1x speedup over fixed majority vote, our approach often achieves an order of magnitude (10x) speedup.","category_a":"CHI","category_b":"HCI","keywords":["Bitwise operation","CAPTCHA","Crowdsourcing","Machine learning","Sentiment analysis","Speedup"],"vec":[0.0699821193,-0.1215155781],"nodes":["35839791","35163655","3371797","40591424","1760364","3216322","35609041"]},"7fee2107608d10a494a469dbf01c807a9a95b09d":{"id":"7fee2107608d10a494a469dbf01c807a9a95b09d","date":"2012-01-01T12:12:00Z","text":"A key step in validating a proposed idea or system is to evaluate over a suitable dataset. However, to this date there have been no useful tools for researchers to understand which datasets have been used for what purpose, or in what prior work. Instead, they have to manually browse through papers to find the suitable datasets and their corresponding URLs, which is laborious and inefficient. To better aid the dataset discovery process, and provide a better understanding of how and where datasets have been used, we propose a framework to effectively identify datasets within the scientific corpus. The key technical challenges are identification of datasets, and discovery of the association between a dataset and the URLs where they can be accessed. Based on this, we have built a user friendly web-based search interface for users to conveniently explore the dataset-paper relationships, and find relevant datasets and their properties.","category_a":"Data Engineering","category_b":"DB","keywords":["Browsing","Text corpus","Usability","Web application","Web search engine"],"vec":[-0.1618752923,0.0226023136],"nodes":["2114053","35313721","1709589","1836901","1704011"]},"27e38351e48fe4b7da2775bf94341738bc4da07e":{"id":"27e38351e48fe4b7da2775bf94341738bc4da07e","date":"2012-01-01T12:12:00Z","text":"Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.","category_a":"EMNLP","category_b":"ComLing","keywords":["Artificial neural network","Experiment","Natural language","Parse tree","Parsing","Propositional calculus","Recursion","Recursive neural network","The Matrix","Word embedding"],"vec":[-0.546673237,-0.4224723484],"nodes":["2166511","2570381","1812612","1701538"]},"5d458a1bff91aa598fcc47711e5cfd7a6dfa559d":{"id":"5d458a1bff91aa598fcc47711e5cfd7a6dfa559d","date":"2004-01-01T12:12:00Z","text":"Data items archived in data warehouses or those that arrive online as streams typically have attributes which take values from multiple hierarchies (e.g., time and geographic location; source and destination IP addresses). Providing an aggregate view of such data is important to summarize, visualize, and analyze. We develop the aggregate view based on certain hierarchically organized sets of large-valued regions (\"heavy hitters\"). Such Hierarchical Heavy Hitters (HHHs) were previously introduced as a crucial aggregation technique in one dimension. In order to analyze the wider range of data warehousing applications and realistic IP data streams, we generalize this problem to multiple dimensions.We identify and study two variants of HHHs for multi-dimensional data, namely the \"overlap\" and \"split\" cases, depending on how an aggregate computed for a child node in the multi-dimensional hierarchy is propagated to its parent element(s). For data warehousing applications, we present offline algorithms that take multiple passes over the data and produce the exact HHHs. For data stream applications, we present online algorithms that find approximate HHHs in one pass, with provable accuracy guarantees.We show experimentally, using real and synthetic data, that our proposed online algorithms yield outputs which are very similar (virtually identical, in many cases) to their offline counterparts. The lattice property of the product of hierarchical dimensions (\"diamond\") is crucially exploited in our online algorithms to track approximate HHHs using only a small, fixed number of statistics per candidate node, regardless of the number of dimensions.","category_a":"SIGMOD","category_b":"DB","keywords":["Aggregate data","Algorithm","Approximation algorithm","Geographic coordinate system","Online algorithm","Provable prime","Synthetic data","Tree (data structure)","Value (ethics)"],"vec":[1.0857864715,0.4138702751],"nodes":["1709589","2096611","1711192","1704011"]},"1600e8c878a2f42c71c753a353b8554b37ecf093":{"id":"1600e8c878a2f42c71c753a353b8554b37ecf093","date":"2005-01-01T12:12:00Z","text":"Order-based queries over XML data include XPath navigation axes such as following-sibling and following. In this paper, we present holistic algorithms that evaluate such order-based queries. An experimental comparison with previous approaches shows the performance benefits of our algorithms.","category_a":"WWW","category_b":"Others","keywords":["Algorithm","Holism","XML","XPath"],"vec":[0.762791768,0.0958006287],"nodes":["3104475","1721062","1704011","1761528"]},"2329a46590b2036d508097143e65c1b77e571e8c":{"id":"2329a46590b2036d508097143e65c1b77e571e8c","date":"2014-01-01T12:12:00Z","text":"We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \u201cphoneme.\u201d Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5\u201900, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Deep learning","Dictionary","End-to-end principle","Graphics processing unit","Pipeline (computing)","Speech recognition","Telephone exchange","Test set"],"vec":[-0.3200959056,-0.0114605326],"nodes":["2893056","1714272","2314133","2301680","2322582","34892314","3168041","2638806","2264597","5574038","1701538"]},"cb3e11ff1f5f03aa58e47e08640647ca133faab2":{"id":"cb3e11ff1f5f03aa58e47e08640647ca133faab2","date":"2006-01-01T12:12:00Z","text":"XML is the de facto standard for data representation and exchange over the Web. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and sensitivity. When querying such XML data, say using XPath, it is important to efficiently identify the data that meet specified constraints on the meta-data. For example, different users may be satisfied with different levels of quality guarantees, or may only have access to different parts of the XML data based on specified security policies. In this paper, we address the problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints, when the meta-data levels are specifically drawn from an ordered domain (e.g., accuracy in [0,1], recency using timestamps, multi-level security, etc.). More specifically, we develop a family of index structures, which we refer to as meta-data indexes, to address this problem. A meta-data index is easily instantiated using a multi-dimensional index structure, such as an R-tree, incorporating novel query and update algorithms. We show that the full meta-data index (FMI), based on associating each XML element with its meta-data level, has a very high update cost for modifying an element's meta-data level. We resolve this problem by designing the inheritance meta-data index (IMI), in which (i) actual meta-data levels are associated only with elements for which this value is explicitly specified, and (ii) inherited meta-data levels and inheritance source nodes are associated with non-leaf nodes of the index structure. We design efficient query (for all XPath axes) and update (of meta-data levels) algorithms for the IMI, and experimentally demonstrate the superiority of the IMI over the FMI using benchmark data sets.","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","Benchmark (computing)","Data (computing)","Internal Market Information System","Multilevel security","Mutual information","R+ tree","R-tree","Tree (data structure)","XML","XPath"],"vec":[1.1427575844,0.0722573932],"nodes":["2690683","1721062","1704011"]},"0b753d3c9fb80145c59f987749ebbde80a0de475":{"id":"0b753d3c9fb80145c59f987749ebbde80a0de475","date":"2012-01-01T12:12:00Z","text":"We describe the Stanford entries to the SANCL 2012 shared task on parsing noncanonical language. Stanford submitted three entries: (i) a self-trained generative constituency parser, (ii) a graph-based dependency parser, and (iii) a stacked dependency parser using the output from the constituency parser as features while parsing. The stacked parser obtained 2nd place in the dependency parsing track. Our overall approach involved exploring techniques which improved performance consistently across domains without using many external resources.","category_a":"","category_b":"Others","keywords":["Parser","Parsing","Shift-reduce parser","emotional dependency","second (number)"],"vec":[-0.1870953943,-0.1940195512],"nodes":[0,0,"35223489","2565228","2166511","1812612"]},"62dcea12832692d2acdb46cb343ab03a04898542":{"id":"62dcea12832692d2acdb46cb343ab03a04898542","date":"2014-01-01T12:12:00Z","text":"","category_a":"ArXiv","category_b":"Journal","keywords":["Acoustic model","Deep learning","Speech recognition","Vocabulary"],"vec":[-0.237605069,-0.0034278784],"nodes":["34961461","2893056","2403705","40537533","1746807","1701538"]},"25eb839f39507fe6983ad3e692b2f8d93a5cb0cc":{"id":"25eb839f39507fe6983ad3e692b2f8d93a5cb0cc","date":"2015-01-01T12:12:00Z","text":"Neural machine translation (NMT) systems have recently achieved results comparable to the state of the art on a few translation tasks, including English\u2192French and English\u2192German. The main purpose of the Montreal Institute for Learning Algorithms (MILA) submission to WMT\u201915 is to evaluate this new approach on a greater variety of language pairs. Furthermore, the human evaluation campaign may help us and the research community to better understand the behaviour of our systems. We use the RNNsearch architecture, which adds an attention mechanism to the encoderdecoder. We also leverage some of the recent developments in NMT, including the use of large vocabularies, unknown word replacement and, to a limited degree, the inclusion of monolingual language models.","category_a":"","category_b":"Others","keywords":["Machine translation","Neural machine translation","Vocabulary","Vocabulary"],"vec":[-0.9308760525,-0.0469857961],"nodes":["34564969","2345617","1979489","1710604","1751762"]},"f71c25642f53c52823aeef5d215993470991cf82":{"id":"f71c25642f53c52823aeef5d215993470991cf82","date":"2013-01-01T12:12:00Z","text":"We investigate the use of deep neural networks for the novel task of class-generic object detection. We show that neural networks originally designed for image recognition can be trained to detect objects within images, regardless of their class, including objects for which no bounding box labels have been provided. In addition, we show that bounding box labels yield a 1% performance increase on the ImageNet recognition challenge.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Computer vision","Deep learning","ImageNet","Minimum bounding box","Object detection"],"vec":[-0.4425392817,-0.2546544287],"nodes":["2570381","5574038","1701538"]},"0d24a0695c9fc669e643bad51d4e14f056329dec":{"id":"0d24a0695c9fc669e643bad51d4e14f056329dec","date":"2016-01-01T12:12:00Z","text":"We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Artificial neural network","BLEU","Conditioning (Psychology)","Discrepancy function","Learning Disorders","Machine translation","Natural language","Natural language generation","Neural Network Simulation","Reinforcement learning","Security token","Structured prediction","Supervised learning","Synthetic data"],"vec":[-0.347420202,0.3771333946],"nodes":["3335364","2616163","36303818","1996705","2054294","1723772","1760871","1751762"]},"14577a88dec08a9199ec4f7a878956a4e55c3dd5":{"id":"14577a88dec08a9199ec4f7a878956a4e55c3dd5","date":"2006-01-01T12:12:00Z","text":"Skew is prevalent in data streams, and should be taken into account by algorithms that analyze the data. The problem of finding \"biased quantiles\"&#8212;that is, approximate quantiles which must be more accurate for more extreme values&#8212;is a framework for summarizing such skewed data on data streams. We present the first deterministic algorithms for answering biased quantiles queries accurately with small&#8212;sublinear in the input size&#8212;space and time bounds in one pass. The space bound is near-optimal, and the amortized update cost is close to constant, making it practical for handling high speed network data streams. We not only demonstrate theoretical properties of the algorithm, but also show it uses less space than existing methods in many practical settings, and is fast to maintain.","category_a":"PODS","category_b":"Others","keywords":["Algorithm","Amortized analysis","Approximation algorithm"],"vec":[0.6875852635,0.2688531453],"nodes":["1709589","2096611","1711192","1704011"]},"13e10f545bd600b5bd8c36b29bcecec4a61d1b7f":{"id":"13e10f545bd600b5bd8c36b29bcecec4a61d1b7f","date":"2008-01-01T12:12:00Z","text":"We consider the task of learning to accurately follow a trajectory in a vehicle such as a car or helicopter. A number of dynamic programming algorithms such as Differential Dynamic Programming (DDP) and Policy Search by Dynamic Programming (PSDP), can efficiently compute non-stationary policies for these tasks --- such policies in general are well-suited to trajectory following since they can easily generate different control actions at different times in order to follow the trajectory. However, a weakness of these algorithms is that their policies are <i>time-indexed<\/i>, in that they apply different policies depending on the current time. This is problematic since 1) the current time may not correspond well to where we are along the trajectory and 2) the uncertainty over states can prevent these algorithms from finding any good policies at all. In this paper we propose a method for <i>space-indexed<\/i> dynamic programming that overcomes both these difficulties. We begin by showing how a dynamical system can be rewritten in terms of a spatial index variable (i.e., how far along the trajectory we are) rather than as a function of time. We then use these space-indexed dynamical systems to derive space-indexed version of the DDP and PSDP algorithms. Finally, we show that these algorithms perform well on a variety of control tasks, both in simulation and on real systems.","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Differential dynamic programming","Dynamic programming","Dynamical system","Simulation","Spatial database","Stationary process"],"vec":[-0.0173273176,-1.1988353186],"nodes":["2551810","5574038","1701538","37555758","1797481"]},"06554235c2c9361a14c0569206b58a355a63f01b":{"id":"06554235c2c9361a14c0569206b58a355a63f01b","date":"2013-01-01T12:12:00Z","text":"This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.","category_a":"NIPS","category_b":"ML","keywords":["Anomaly detection","Modal logic","Test set","Text corpus","Zero"],"vec":[-0.4256723529,-0.2652593135],"nodes":["2166511","2012435","1812612","1701538"]},"304d859dd545ab119ef1a6141d73a9c3797526d4":{"id":"304d859dd545ab119ef1a6141d73a9c3797526d4","date":"2003-01-01T12:12:00Z","text":"We investigate the calculation of A* bounds for sequence and tree models which are the explicit intersection of a set of simpler models or can be bounded by such an intersection. We provide a natural viewpoint which unifies various instances of factored A* models for trees and sequences, some previously known and others novel, including multiple sequence alignment, weighted finitestate transducer composition, and lexicalized statistical parsing. The specific case of parsing with a product of syntactic (PCFG) and semantic (lexical dependency) components is then considered in detail. We show that this factorization gives a modular lexicalized parser which is simpler than comparably accurate non-factored models, and which allows efficient exact inference with large treebank grammars.","category_a":"CAI","category_b":"AI","keywords":["A* search algorithm","Multiple sequence alignment","Parsing","Sequence alignment","Statistical parsing","Stochastic context-free grammar","Transducer","Treebank"],"vec":[-0.3594806149,-0.5199010799],"nodes":["38666915","1812612"]},"211e4b486a85f5250ef68aef8a4422811ec7a932":{"id":"211e4b486a85f5250ef68aef8a4422811ec7a932","date":"2003-01-01T12:12:00Z","text":"Aggregation along hierarchies is a critical summary technique in a large variety of online applications including decision support, and network management (e.g., IP clustering, denial-of-service attack monitoring). Despite the amount of recent study that has been dedicated to online aggregation on sets (e.g., quantiles, hot items), surprisingly little attention has been paid to summarizing hierarchical structure in stream data. The problem we study in this paper is that of finding Hierarchical Heavy Hitters (HHH): given a hierarchy and a fraction \u03c6, we want to find all HHH nodes that have a total number of descendants in the data stream larger than \u03c6 of the total number of elements in the data stream, after discounting the descendant nodes that are HHH nodes. The resulting summary gives a topological \u201ccartogram\u201d of the hierarchical data. We present deterministic and randomized algorithms for finding HHHs, which builds upon existing techniques by incorporating the hierarchy into the algorithms. Our experiments demonstrate several factors of improvement in accuracy over Supported by NSF ITR 0220280 and NSF EIA 02-05116 This author is also with Rutgers University. Supported by NSF CCR 0087022, NSF ITR 0220280 and NSF EIA 02-05116. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and\/or special permission from the Endowment. Proceedings of the 29th VLDB Conference, Berlin, Germany, 2003 the straightforward approach, which is due to making algorithms hierarchy-aware.","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Cluster analysis","Decision support system","Denial-of-service attack","Experiment","Hierarchical database model","IBM Notes","Online aggregation","Randomized algorithm","VLDB"],"vec":[0.9884901085,0.0976802029],"nodes":["1709589","2096611","1711192","1704011"]},"6556cea7f91c1221dd2693aba0d3dd45d2e09e6d":{"id":"6556cea7f91c1221dd2693aba0d3dd45d2e09e6d","date":"2000-01-01T12:12:00Z","text":"Now there is tremendous interest in data warehousing and OLAP applications. OLAP applications typically view data as having multiple logical dimensions (e.g., product, location) with natural hierarchies de ned on each dimension, and analyze the behavior of various measure attributes (e.g., sales, volume) in terms of the dimensions. OLAP queries typically involve hierarchical selections on some of the dimensions (e.g., product is classi ed under the jeans product category, or location is in the north-east region), often aggregating measure attributes (see, e.g., [6]). Cost-based query optimization of such OLAP queries needs good estimates of the selectivity of hierarchical selections. Histograms capture attribute value distribution statistics in a space-e cient fashion. They have been designed to work well for numeric attribute value domains, and have long been used to support cost-based query optimization in databases [11, 9, 2, 4, 10, 5]. Histograms can be used to estimate the selectivity of OLAP queries by modeling the (hierarchical) conditions on a given dimension as a set of hierarchical ranges (i.e., two ranges are either disjoint or one is contained in the other), and using standard range selectivity estimation techniques (see, e.g., [10]). The quality of selectivity estimates obtained using a histogram depends on computing a good solution to the histogram construction problem, and there has been considerable recent e ort in this area (see, e.g., [10, 5]). However, while OLAP queries make extensive use of hierarchical selection conditions, previous works on computing good histograms, for the most part, consider only equality queries when computing the error incurred by a particular choice of histogram bucket boundaries. This mismatch between the nature of OLAP queries, and the class of queries considered when constructing histograms can result in poor selectivity estimates for OLAP queries. In this paper, we address this problem and focus on e ciently computing optimal histograms for the case of hierarchical range queries. W e make the following contributions:","category_a":"PODS","category_b":"Others","keywords":["Attribute\u2013value pair","Database","Online analytical processing","Open road tolling","Program optimization","Query optimization","Range query (data structures)","Selectivity (electronic)"],"vec":[1.0605188734,0.1822645202],"nodes":["1721062","1711192","1704011"]},"142f38642629b9d268999ad876af482177d36697":{"id":"142f38642629b9d268999ad876af482177d36697","date":"2012-01-01T12:12:00Z","text":"Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1","category_a":"ACL","category_b":"ComLing","keywords":["Algorithm","Artificial neural network","Baseline (configuration management)","Machine learning","Natural language processing","Network architecture"],"vec":[-0.9207657156,-0.1485269879],"nodes":["39929781","2166511","1812612","1701538"]},"6ca1898dac153b8cd500c0c2633675b05d3c638c":{"id":"6ca1898dac153b8cd500c0c2633675b05d3c638c","date":"2013-01-01T12:12:00Z","text":"The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.","category_a":"ArXiv","category_b":"Journal","keywords":["Activation function","Artificial neural network","Dropout (neural networks)","Eisenstein's criterion","Ensemble interpretation","Gradient","Multidimensional scaling","Piecewise linear continuation"],"vec":[-0.2600651344,0.5695229753],"nodes":["1923596","34740554","1760871","1751762"]},"2964d30862d0402b0d0ad4a427067f69e4a52130":{"id":"2964d30862d0402b0d0ad4a427067f69e4a52130","date":"2011-01-01T12:12:00Z","text":"We propose a novel regularizer when training an auto-encoder for unsupervised feature extraction. We explicitly encourage the latent representation to contract the input space by regularizing the norm of the Jacobian (analytically) and the Hessian (stochastically) of the encoder\u2019s output with respect to its input, at the training points. While the penalty on the Jacobian\u2019s norm ensures robustness to tiny corruption of samples in the input space, constraining the norm of the Hessian extends this robustness when moving further away from the sample. From a manifold learning perspective, balancing this regularization with the auto-encoder\u2019s reconstruction objective yields a representation that varies most when moving along the data manifold in input space, and is most insensitive in directions orthogonal to the manifold. The second order regularization, using the Hessian, penalizes curvature, and thus favors smooth manifold. We show that our proposed technique, while remaining computationally efficient, yields representations that are significantly better suited for initializing deep architectures than previously proposed approaches, beating state-of-the-art performance on a number of datasets.","category_a":"KDD","category_b":"AI","keywords":["Autoencoder","Computational complexity theory","Deep packet inspection","Encoder","Feature extraction","Hessian","Jacobian matrix and determinant","Matrix regularization","Nonlinear dimensionality reduction"],"vec":[-0.5690536044,0.7254403176],"nodes":["2425018","1935910","1724875","3012669","1751762","2921469","3119801"]},"115808104b2a9c3ab6e2e60582ab7e33b937b754":{"id":"115808104b2a9c3ab6e2e60582ab7e33b937b754","date":"2016-01-01T12:12:00Z","text":"We have seen great progress in basic perceptual tasks such as object recognition and detection. However, AI models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning. Recently the new task of visual question answering (QA) has been proposed to evaluate a model's capacity for deep image understanding. Previous works have established a loose, global association between QA sentences and images. However, many questions and answers, in practice, relate to local regions in the images. We establish a semantic link between textual descriptions and image regions by object-level grounding. It enables a new type of QA with visual answers, in addition to textual answers used in previous work. We study the visual QA tasks in a grounded setting with a large collection of 7W multiple-choice QA pairs. Furthermore, we evaluate human performance and several baseline models on the QA tasks. Finally, we propose a novel LSTM model with spatial attention to tackle the 7W QA tasks.","category_a":"CVPR","category_b":"ComVis","keywords":["Baseline (configuration management)","Computer vision","Google Questions and Answers","High- and low-level","Human reliability","Link relation","Outline of object recognition","Question answering"],"vec":[-0.4396785443,-0.2330448364],"nodes":["2117748","2152186","35609041","3216322"]},"3b2bf65ebee91249d1045709200a51d157b0176e":{"id":"3b2bf65ebee91249d1045709200a51d157b0176e","date":"2008-01-01T12:12:00Z","text":"Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Autoencoder","Benchmark (computing)","Experiment","Generative model","Map","Noise reduction","Nonlinear dimensionality reduction","Statistical classification","Unsupervised learning"],"vec":[-0.5111995445,0.4013765864],"nodes":["1724875","1777528","1751762","1798462"]},"19a2c31570534fbdde8290413ac465b3831e9e99":{"id":"19a2c31570534fbdde8290413ac465b3831e9e99","date":"2005-01-01T12:12:00Z","text":"XML repositories are usually queried both on structure and content. Due to structural heterogeneity of XML, queries are often interpreted approximately and their answers are returned ranked by scores. Computing answer scores in XML is an active area of research that oscillates between pure content scoring such as the wellknown tf*idf and taking structure into account. However, none of the existing proposals fully accounts for structure and combines it with content to score query answers. We propose novel XML scoring methods that are inspired by tf*idf and that account for both structure and content while considering query relaxations. Twig scoring, accounts for the most structure and content and is thus used as our reference method. Path scoring is an approximation that loosens correlations between query nodes hence reducing the amount of time required to manipulate scores during top-k query processing. We propose efficient data structures in order to speed up ranked query processing. We run extensive experiments that validate our scoring methods and that show that path scoring provides very high precision while improving score computation time.","category_a":"VLDB","category_b":"DB","keywords":["Computation","Data structure","Database","Experiment","Tf\u2013idf","Time complexity","Twig","XML","XML schema"],"vec":[1.0985451554,0.0773140322],"nodes":["1736846","1721062","2394972","1704011","1725663"]},"368fe0571a5611c2de27379747adff2066245937":{"id":"368fe0571a5611c2de27379747adff2066245937","date":"2006-01-01T12:12:00Z","text":"Recent works have shown the benefits of keyword proximity search in querying XML documents in addition to text documents. For example, given query keywords over Shakespeare's plays in XML, the user might be interested in knowing how the keywords cooccur. In this paper, we focus on XML trees and define XML keyword, proximity queries to return the (possibly heterogeneous) set of minimum connecting trees (MCTs) of the matches to the individual keywords in the query. We consider efficiently executing keyword proximity queries on labeled trees (XML) in various settings: 1) when the XML database has been preprocessed and 2) when no indices are available on the XML database. We perform a detailed experimental evaluation to study the benefits of our approach and show that our algorithms considerably outperform prior algorithms and other applicable approaches.","category_a":"Knowledge and Data Engineering","category_b":"Others","keywords":["Algorithm","Proximity search (text)","XML","XML database"],"vec":[0.8617682884,0.0800517502],"nodes":["1754970","1721062","1786049","1704011"]},"71898bd32548b98ceecf71bb30b9616e4be40b43":{"id":"71898bd32548b98ceecf71bb30b9616e4be40b43","date":"2009-01-01T12:12:00Z","text":"We present two regression models for the prediction of pairwise preference judgments among MT hypotheses. Both models are based on feature sets that are motivated by textual entailment and incorporate lexical similarity as well as local syntactic features and specific semantic phenomena. One model predicts absolute scores; the other one direct pairwise judgments. We find that both models are competitive with regression models built over the scores of established MT evaluation metrics. Further data analysis clarifies the complementary behavior of the","category_a":"EACL","category_b":"Others","keywords":["Machine translation","Textual entailment"],"vec":[-0.4337845934,-0.1648514598],"nodes":["1708581","1947267","1746807","1812612"]},"41a0c983e59c8fc5100b94c0802d74a64f86c741":{"id":"41a0c983e59c8fc5100b94c0802d74a64f86c741","date":"2010-01-01T12:12:00Z","text":"We investigate a number of approaches to generating Stanford Dependencies, a widely used semantically-oriented dependency representation. We examine algorithms specifically designed for dependency parsing (Nivre, Nivre Eager, Covington, Eisner, and RelEx) as well as dependencies extracted from constituent parse trees created by phrase structure parsers (Charniak, Charniak-Johnson, Bikel, Berkeley and Stanford). We found that phrase structure parsers systematically outperform algorithms designed specifically for dependency parsing. The most accurate method for generating dependencies is the Charniak-Johnson reranking parser, with 89% (labeled) attachment F1 score. The fastest methods are Nivre, Nivre Eager, and Covington. When used with a linear classifier to make local parsing decisions, these methods can parse the entire Penn Treebank development set (section 22) in less than 10 seconds on an Intel Xeon E5520. However, this speed comes with a substantial drop in F1 score (about 76% for labeled attachment) compared to competing methods. By tuning how much of the search space is explored by the Charniak-Johnson parser, we are able to arrive at a balanced configuration that is both fast and nearly as good as the most accurate approaches.","category_a":"LREC","category_b":"Others","keywords":["Algorithm","F1 score","Linear classifier","Parsing","Phrase structure rules","Treebank"],"vec":[-0.4214509445,-0.6760936975],"nodes":["3208422","2241127","1746807","1812612"]},"16ce9e96e5cf0b8fd3aeee137d548ff903711b97":{"id":"16ce9e96e5cf0b8fd3aeee137d548ff903711b97","date":"2008-01-01T12:12:00Z","text":"ii Introduction Welcome to the ACL Workshop on Parsing German, the first of what we hope will be a long and fruitful series of workshops on this topic. German possesses an interesting set of configurational properties on the syntactic level which make it far less flexible with respect to word order than other free word order languages. Analyses of these properties, which have formed a part of the traditional syntax of German since the early 19th century, only re-entered the mainstream of generative linguistics research within the last twenty years or so. In computational linguistics, however, their realization has varied quite widely: \" topological fields \" in HPSG-style analyses, multiple parse trees, special constraints on liberation in constraint-based dependency-style analyses, various hybrid \" deep\/shallow \" approaches, and agnostic parameter estimation over graphs. This variation can also acutely be felt in the annotation of German treebanks. Many corpora have historically elected to annotate only a few of the different senses of the term \" constituent \" inherent to German syntax, resulting in standards that make German appear either more like English or more like Czech. The aim of this workshop was to provide a forum for theoretical discussion as well as a shared task, based on the TIGER and TueBa-D\/Z German treebanks, for these various approaches to make their case on empirical grounds. This combination we believe to be essential to balancing the considerations of what structure merits learning versus the ease with which it can be learned. Both treebanks are annotated collections of German newspaper text on similar topics. They are annotated with POS, morphology, phrase structure, and grammatical functions. TueBa-D\/Z additionally uses topological fields to describe fundamental word order restrictions in German clauses. The treebanks differ significantly in their annotation schemes, however: while TIGER relies on crossing branches to describe long distance relationships, TueBa-D\/Z uses pure tree structures with designated labels for long distance relationships. Additionally, the annotation is TIGER is flat on the phrasal level while TueBa-D\/Z annotates phrasal structure more hierarchically. A report on the results of this year's shared task can be found in the final paper of these proceedings. Abstract This paper presents a method and implementation of parsing German V2 word order by means of constraints that reside in lexical heads. It first describes the design of the underlying parsing engine: the head-corner chart parsing that incorporates a procedure that dynamically enforces word order \u2026","category_a":"","category_b":"Others","keywords":["Annotation","Chart parser","Choose (action)","Computational linguistics","Educational workshop","Equilibrium","Estimation theory","Generative grammar","Head","Head-driven phrase structure grammar","Killzone: Liberation","Languages","Linguistics","Mac OS X 10.4 Tiger","Mathematical morphology","Order by","Parsing","Phrase structure rules","Point of sale","Reside","Text corpus","Treebank","Trees (plant)","emotional dependency"],"vec":[-0.4634067717,-0.6206783455],"nodes":["1804668","1804295",0,0,"2505673","1823231","6230562","3080637","3709887","35384949","1754497","38666915","1875900","1812612","2964780"]},"5da15e6b30920ab20ce461232c00276e3b0f0bef":{"id":"5da15e6b30920ab20ce461232c00276e3b0f0bef","date":"2009-01-01T12:12:00Z","text":"","category_a":"ICML","category_b":"ML","keywords":[],"vec":[-0.1944470357,0.0008867951],"nodes":["1736727","1776908","1688882","1695689","1751762"]},"561008cb23d7a38a00806353ba3389c1b95395af":{"id":"561008cb23d7a38a00806353ba3389c1b95395af","date":"2016-01-01T12:12:00Z","text":"Audio description (AD) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed ADs, which are temporally aligned to full length movies. In addition we also collected and aligned movie scripts used in prior work and compare the two sources of descriptions. We introduce the Large Scale Movie Description Challenge (LSMDC) which contains a parallel corpus of 128,118 sentences aligned to video clips from 200 movies (around 150\u00a0h of video in total). The goal of the challenge is to automatically generate descriptions for the movie clips. First we characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing ADs to scripts, we find that ADs are more visual and describe precisely what is shown rather than what should happen according to the scripts created prior to movie production. Furthermore, we present and compare the results of several teams who participated in the challenges organized in the context of two workshops at ICCV 2015 and ECCV 2016.","category_a":"Journal of Computer Vision","category_b":"Others","keywords":["Audio description","Benchmark (computing)","Computational linguistics","Computer vision","European Conference on Computer Vision","ICCV","Parallel text","Video clip"],"vec":[-0.2588425135,-0.4025329009],"nodes":["34721166","1730844","34849128","1721168","1972076","1777528","1760871","1697100"]},"79d1e429c241d0aa47a2194246256a5bc79585bc":{"id":"79d1e429c241d0aa47a2194246256a5bc79585bc","date":"2014-01-01T12:12:00Z","text":"We present a method to perform first-pass large vocabulary continuous speech recognition using only a neural network and language model. Deep neural network acoustic models are now commonplace in HMM-based speech recognition systems, but building such systems is a complex, domain-specific task. Recent work demonstrated the feasibility of discarding the HMM sequence modeling framework by directly predicting transcript text from audio. This paper extends this approach in two ways. First, we demonstrate that a straightforward recurrent neural network architecture can achieve a high level of accuracy. Second, we propose and evaluate a modified prefix-search decoding algorithm. This approach to decoding enables first-pass speech recognition with a language model, completely unaided by the cumbersome infrastructure of HMM-based systems. Experiments on the Wall Street Journal corpus demonstrate fairly competitive word error rates, and the importance of bi-directional network recurrence.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Artificial neural network","Deep learning","Experiment","Hidden Markov model","High-level programming language","Language model","LiveCode","Network architecture","Recurrent neural network","Speech recognition","The Wall Street Journal","Vocabulary"],"vec":[-0.474416915,-0.1754807426],"nodes":["34961461","2893056","1746807","1701538"]},"35e024714f0702f8b77c8c2a916dedb91931708a":{"id":"35e024714f0702f8b77c8c2a916dedb91931708a","date":"2010-01-01T12:12:00Z","text":"Despite years of speech recognition research, little is known about which words tend to be misrecognized and why. Previous work has shown that errors increase for infrequent words, short words, and very loud or fast speech, but many other presumed causes of error (e.g., nearby disfluencies, turn-initial words, phonetic neighborhood density) have never been carefully tested. The reasons for the huge differences found in error rates between speakers also remain largely mysterious. Using a mixed-effects regression model, we investigate these and other factors by analyzing the errors of two state-of-the-art recognizers on conversational speech. Words with higher error rates include those with extreme prosodic characteristics, those occurring turninitially or as discourse markers, and doubly confusable pairs: acoustically similar words that also have similar language model probabilities. Words preceding disfluent interruption points (first repetition tokens and words before fragments) also have higher error rates. Finally, even after accounting for other factors, speaker differences cause enormous variance in error rates, suggesting that speaker error rate variance is not fully explained by differences in word choice, fluency, or prosodic characteristics. We also propose that doubly confusable pairs, rather than high neighborhood density, may better explain phonetic neighborhood errors in human speech processing. 2009 Elsevier B.V. All rights reserved.","category_a":"Speech Communication","category_b":"Others","keywords":["Finite-state machine","Interruption science","Language model","Speech processing","Speech recognition"],"vec":[-0.7067228093,-0.5211263119],"nodes":["1991315","1746807","1812612"]},"0808bb50993547a533ea5254e0454024d98c5e2f":{"id":"0808bb50993547a533ea5254e0454024d98c5e2f","date":"2015-01-01T12:12:00Z","text":"In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.","category_a":"NIPS","category_b":"ML","keywords":["Artificial neural network","Autoencoder","Calculus of variations","High- and low-level","Latent variable model","Natural language","Recurrent neural network","Spatial variability"],"vec":[-0.4410820627,0.442255223],"nodes":["8270717","2182706","35094433","2957685","1760871","1751762"]},"5e3b036c44f0c3b0b5eb1e99ef78644dcabcf2f0":{"id":"5e3b036c44f0c3b0b5eb1e99ef78644dcabcf2f0","date":"2015-01-01T12:12:00Z","text":"","category_a":"ArXiv","category_b":"Journal","keywords":["Audio description"],"vec":[-0.1927221876,0.0015180582],"nodes":["1685369","1730844","1979489","2482072","1972076","1777528","1760871"]},"7da23d5b7c26dc1d6ac98682a74a8dd6a18bf309":{"id":"7da23d5b7c26dc1d6ac98682a74a8dd6a18bf309","date":"2012-01-01T12:12:00Z","text":"We present a probabilistic approach for learning to interpret temporal phrases given only a corpus of utterances and the times they reference. While most approaches to the task have used regular expressions and similar linear pattern interpretation rules, the possibility of phrasal embedding and modification in time expressions motivates our use of a compositional grammar of time expressions. This grammar is used to construct a latent parse which evaluates to the time the phrase would represent, as a logical parse might evaluate to a concrete entity. In this way, we can employ a loosely supervised EM-style bootstrapping approach to learn these latent parses while capturing both syntactic uncertainty and pragmatic ambiguity in a probabilistic framework. We achieve an accuracy of 72% on an adapted TempEval-2 task \u2013 comparable to state of the art systems.","category_a":"HLT-NAACL","category_b":"ComLing","keywords":["Latent semantic analysis","Parsing","Regular expression","Speech corpus","The Times"],"vec":[-0.5798344382,-0.4039794778],"nodes":["32301760","1812612","1746807"]},"1eb654f295d0e98e71bfbbe18c8a0cad3f6d15e9":{"id":"1eb654f295d0e98e71bfbbe18c8a0cad3f6d15e9","date":"2014-01-01T12:12:00Z","text":"The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences.","category_a":"EMNLP","category_b":"ComLing","keywords":["Artificial neural network","Concatenation","Machine translation","Neural machine translation"],"vec":[-1.1148422078,0.1517555167],"nodes":["2409581","3335364","3158246","1979489","1751762"]},"135c89b491f82bd4fd7de175ac778207f598342b":{"id":"135c89b491f82bd4fd7de175ac778207f598342b","date":"2014-01-01T12:12:00Z","text":"Neural language models learn word representations that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models. We show that translation-based embeddings outperform those learned by cutting-edge monolingual models at single-language tasks requiring knowledge of conceptual similarity and\/or syntactic role. The findings suggest that, while monolingual models learn information about how concepts are related, neural-translation models better capture their true ontological status. It is well known that word representations can be learned from the distributional patterns in corpora. Originally, such representations were constructed by counting word co-occurrences, so that the features in one word\u2019s representation corresponded to other words [11, 17]. Neural language models, an alternative means to learn word representations, use language data to optimise (latent) features with respect to a language modelling objective. The objective can be to predict either the next word given the initial words of a sentence [4, 14, 8], or simply a nearby word given a single cue word [13, 15]. The representations learned by neural models (sometimes called embeddings) generally outperform those acquired by co-occurrence counting models when applied to NLP tasks [3]. Despite these clear results, it is not well understood how the architecture of neural models affects the information encoded in their embeddings. Here, we explore this question by considering the embeddings learned by architectures with a very different objective function to monolingual language models: neural machine translation models. We show that translation-based embeddings outperform monolingual embeddings on two types of task: those that require knowledge of conceptual similarity (rather than simply association or relatedness), and those that require knowledge of syntactic role. We discuss what the findings indicate about the information content of different embeddings, and suggest how this content might emerge as a consequence of the translation objective. 1 Learning embeddings from language data Both neural language models and translation models learn real-valued embeddings (of specified dimension) for words in some pre-specified vocabulary, V , covering many or all words in their training corpus. At each training step, a \u2018score\u2019 for the current training example (or batch) is computed based on the embeddings in their current state. This score is compared to the model\u2019s objective function, and the error is backpropagated to update both the model weights (affecting how the score is computed from the embeddings) and the embedding features. At the end of this process, the embeddings should encode information that enables the model to optimally satisfy its objective. 1.1 Monolingual models In the original neural language model [4] and subsequent variants [8], each training example consists of n subsequent words, of which the model is trained to predict the n-th word given the first n \u2212 1 ar X iv :1 41 0. 07 18 v2 [ cs .C L ] 1 3 N ov 2 01 4 1 words. The model first represents the input as an ordered sequence of embeddings, which it transforms into a single fixed length \u2018hidden\u2019 representation by, e.g., concatenation and non-linear projection. Based on this representation, a probability distribution is computed over the vocabulary, from which the model can sample a guess at the next word. The model weights and embeddings are updated to maximise the probability of correct guesses for all sentences in the training corpus. More recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer [13, 15]. Given a single word in the corpus, these models simply predict which other words will occur nearby. For each word w in V , a list of training cases (w, c) : c \u2208 V is extracted from the training corpus. For instance, in the skipgram approach [13], for each \u2018cue word\u2019 w the \u2018context words\u2019 c are sampled from windows either side of tokens of w in the corpus (with c more likely to be sampled if it occurs closer to w).1 For each w in V , the model initialises both a cue-embedding, representing the w when it occurs as a cue-word, and a context-embedding, used when w occurs as a context-word. For a cue word w, the model can use the corresponding cueembedding and all context-embeddings to compute a probability distribution over V that reflects the probability of a word occurring in the context of w. When a training example (w, c) is observed, the model updates both the cue-word embedding of w and the context-word embeddings in order to increase the conditional probability of c. 1.2 Translation-based embeddings Neural translation models generate an appropriate sentence in their target language St given a sentence Ss in their source language [see, e.g., 16, 6]. In doing so, they learn distinct sets of embeddings for the vocabularies Vs and Vt in the source and target languages respectively. Observing a training case (Ss, St), such a model represents Ss as an ordered sequence of embeddings of words from Vs. The sequence for Ss is then encoded into a single representation RS .2 Finally, by referencing the embeddings in Vt, RS and a representation of what has been generated thus far, the model decodes a sentence in the target language word by word. If at any stage the decoded word does not match the corresponding word in the training target St, the error is recorded. The weights and embeddings in the model, which together parameterise the encoding and decoding process, are updated based on the accumulated error once the sentence decoding is complete. Although neural translation models can differ in low-level architecture [7, 2], the translation objective exerts similar pressure on the embeddings in all cases. The source language embeddings must be such that the model can combine them to form single representations for ordered sequences of multiple words (which in turn must enable the decoding process). The target language embeddings must facilitate the process of decoding these representations into correct target-language sentences. 2 Comparing Mono-lingual and Translation-based Embeddings To learn translation-based embeddings, we trained both the RNN encoder-decoder [RNNenc, 7] and the RNN Search architectures [2] on a 300m word corpus of English-French sentence pairs. We conducted all experiments with the resulting (English) source embeddings from these models. For comparison, we trained a monolingual skipgram model [13] and its Glove variant [15] for the same number of epochs on the English half of the bilingual corpus. We also extracted embeddings from a full-sentence language model [CW, 8] trained for several months on a larger 1bn word corpus. As in previous studies [1, 5, 3], we evaluate embeddings by calculating pairwise (cosine) distances and correlating these distances with (gold-standard) human judgements. Table 1 shows the correlations of different model embeddings with three such gold-standard resources, WordSim-353 [1], MEN [5] and SimLex-999 [10]. Interestingly, translation embeddings perform best on SimLex-999, while the two sets of monolingual embeddings perform better on modelling the MEN and WordSim353. To interpret these results, it should be noted that SimLex-999 evaluation quantifies conceptual similarity (dog wolf ), whereas MEN and WordSim-353 (despite its name) quantify more general relatedness (dog collar) [10]. The results seem to indicate that translation-based embeddings better capture similarity, while monolingual embeddings better capture relatedness. 1 Subsequent variants use different algorithms for selecting the (w, c) from the training corpus [9, 12] Alternatively, subsequences (phrases) of Ss may be encoded at this stage in place of the whole sentence [2].","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Artificial neural network","Batch processing","Code","Concatenation","Creational pattern","Display resolution","Experiment","High- and low-level","Language model","Machine translation","Microsoft Windows","N-gram","Natural language processing","Neural machine translation","Nonlinear system","Self-information","Text corpus","The Sentence","Vocabulary","Word embedding"],"vec":[-1.2035683809,0.1298111229],"nodes":["38909889","1979489","34564969","16284940","1751762"]},"3e1638d1ce6ec0d18c82fb3235a521db0a559da2":{"id":"3e1638d1ce6ec0d18c82fb3235a521db0a559da2","date":"2011-01-01T12:12:00Z","text":"Regularization is a well studied problem in the context of neural networks. It is usually used to improve the generalization performance when the number of input samples is relatively small or heavily contaminated with noise. The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay, output smoothing that are used to avoid overfitting during the training of the considered model. From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991). Using Bishop\u2019s approximation (Bishop, 1995) of the objective function when a restricted type of noise is added to the input of a parametric function, we derive the higher order terms of the Taylor expansion and analyze the coefficients of the regularization terms induced by the noisy input. In particular we study the effect of penalizing the Hessian of the mapping function with respect to the input in terms of generalization performance. We also show how we can control independently this coefficient by explicitly penalizing the Jacobian of the mapping function on corrupted inputs.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Coefficient","Early stopping","Hessian","Jacobian matrix and determinant","Loss function","Matrix regularization","Morgan","Objective-C","Overfitting","Parametric model","Perturbation theory","Point of View (computer hardware company)","Smoothing"],"vec":[-0.2624011721,0.5541111252],"nodes":["2425018","3119801","1751762","1724875"]},"3b118bbb7c33eb8ca4605170e1bff2d459488df6":{"id":"3b118bbb7c33eb8ca4605170e1bff2d459488df6","date":"2004-01-01T12:12:00Z","text":"We describe an interesting application of the principle of local learning to density estimation. Locally weighted fitting of a Gaussian with a regularized full covariance matrix yields a density estimator which displays improved behavior in the case where much of the probability mass is concentrated along a low dimensional manifold. While the proposed estimator is not guaranteed to integrate to 1 with a finite sample size, we prove asymptotic convergence to the true density. Experimental results illustrating the advantages of this estimator over classic non-parametric estimators are presented.","category_a":"","category_b":"Others","keywords":["Sample Size","manifold"],"vec":[-0.3002637394,0.5003334753],"nodes":["1751762","1724875"]},"ecdd236a1a31afcb2a3480a4c7dd5aa65b44551d":{"id":"ecdd236a1a31afcb2a3480a4c7dd5aa65b44551d","date":"2011-01-01T12:12:00Z","text":"Microblogs such as Twitter provide a valuable corpus of diverse user-generated content. While the data extracted from Twitter is generally timely and accurate, the process by which developers currently extract structured data from the tweet stream is ad-hoc and requires reimplementation of common data manipulation primitives. In this paper, we present two systems for extracting structure from and querying Twitter-embedded data. The first, TweeQL, provides a streaming SQL-like interface to the Twitter API, making common tweet processing tasks simpler. The second, TwitInfo, shows how end-users can interact with and understand aggregated data from the tweet stream (as well as showcasing the power of the TweeQL language). Together these systems show the richness of content that can be extracted from Twitter.","category_a":"","category_b":"Others","keywords":["Application programming interface","Body of uterus","Data model","Hoc (programming language)","SQL","Structured Query Language","Text corpus","User-generated content"],"vec":[0.4383285354,-0.6328729213],"nodes":["9121681","14895280","35609041","2259673","1743286","2033016","31577736","35306555"]},"8b358a216f83edb259decd68127722a356cc8adf":{"id":"8b358a216f83edb259decd68127722a356cc8adf","date":"2017-01-01T12:12:00Z","text":"It is commonly assumed that language refers to high-level visual concepts while leaving low-level visual processing unaffected. This view dominates the current literature in computational models for language-vision tasks, where visual and linguistic inputs are mostly processed independently before being fused into a single representation. In this paper, we deviate from this classic pipeline and propose to modulate the entire visual processing by a linguistic input. Specifically, we introduce Conditional Batch Normalization (CBN) as an efficient mechanism to modulate convolutional feature maps by a linguistic embedding. We apply CBN to a pre-trained Residual Network (ResNet), leading to the MODulatEd ResNet (MODERN) architecture, and show that this significantly improves strong baselines on two visual question answering tasks. Our ablation study confirms that modulating from the early stages of the visual processing is beneficial.","category_a":"NIPS","category_b":"ML","keywords":["Computational model","Flow network","High- and low-level","Modulation","Question answering"],"vec":[-0.3198838279,0.3153064315],"nodes":["1952531","3367628","38544526","1777528","1721354","1760871"]},"374668781c1aa05391a900cc2841eb31c7bbce6a":{"id":"374668781c1aa05391a900cc2841eb31c7bbce6a","date":"2013-01-01T12:12:00Z","text":"The notion of heavy hitters-items that make up a large fraction of the population - has been successfully used in a variety of applications across sensor and RFID monitoring, network data analysis, event mining, and more. Yet this notion often fails to capture the semantics we desire when we observe data in the form of correlated pairs. Here, we are interested in items that are conditionally frequent: when a particular item is frequent within the context of its parent item. In this work, we introduce and formalize the notion of Conditional Heavy Hitters to identify such items, with applications in network monitoring, and Markov chain modeling. We introduce several streaming algorithms that allow us to find conditional heavy hitters efficiently, and provide analytical results. Different algorithms are successful for different input characteristics. We perform experimental evaluations to demonstrate the efficacy of our methods, and to study which algorithms are most suited for different types of data.","category_a":"ICDE","category_b":"DB","keywords":["Algorithm","Markov chain","Radio-frequency identification","Streaming algorithm"],"vec":[0.941808368,0.3057116611],"nodes":["1701357","1725167","1709589","1704011"]},"260697ac804faf844241de404bab6e9460223c2f":{"id":"260697ac804faf844241de404bab6e9460223c2f","date":"2009-01-01T12:12:00Z","text":"Conditional functional dependencies (CFDs) have recently been proposed as extensions of classical functional dependencies that apply to a certain subset of the relation, as specified by a <i>pattern tableau<\/i>. Calculating the support and confidence of a CFD (i.e., the size of the applicable subset and the extent to which it satisfies the CFD)gives valuable information about data semantics and data quality. While computing the support is easier, computing the confidence exactly is expensive if the relation is large, and estimating it from a random sample of the relation is unreliable unless the sample is large.\n We study how to efficiently estimate the confidence of a CFD with a small number of passes (one or two) over the input using small space. Our solutions are based on a variety of sampling and sketching techniques, and apply when the pattern tableau is known in advance, and also the harder case when this is given after the data have been seen. We analyze our algorithms, and show that they can guarantee a small additive error; we also show that relative errors guarantees are not possible. We demonstrate the power of these methods empirically, with a detailed study using both real and synthetic data. These experiments show that it is possible to estimate the CFD confidence very accurately with summaries which are much smaller than the size of the data they represent.","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","Conditional entropy","Data quality","Experiment","Functional dependency","Method of analytic tableaux","Sampling (signal processing)","Synthetic data"],"vec":[0.3993135008,0.5334185228],"nodes":["1709589","1688834","2096611","2097772","1704011","1702286"]},"0a832c69048ca30d4f528d221ba9899590a45f34":{"id":"0a832c69048ca30d4f528d221ba9899590a45f34","date":"2007-01-01T12:12:00Z","text":"We show a number of improvements in the use of Hidden Conditional Random Fields (HCRFs) for phone classification on the TIMIT and Switchboard corpora. We first show that the use of regularization effectively prevents overfitting, improving over other methods such as early stopping. We then show that HCRFs are able to make use of non-independent features in phone classification, at least with small numbers of mixture components, while HMMs degrade due to their strong independence assumptions. Finally, we successfully apply Maximum a Posteriori adaptation to HCRFs, decreasing the phone classification error rate in the Switchboard corpus by around 1% -5% given only small amounts of adaptation data.","category_a":"ASRU","category_b":"Others","keywords":["Conditional random field","Early stopping","Matrix regularization","Overfitting","TIMIT","Telephone exchange","Text corpus"],"vec":[-0.2664260929,0.2794313189],"nodes":["2305450","3300609","1812612","1746807"]},"0e18e9dcfceedabcf14e6a615862c53ce8c0293d":{"id":"0e18e9dcfceedabcf14e6a615862c53ce8c0293d","date":"2009-01-01T12:12:00Z","text":"Linking constructions involving { (DE) are ubiquitous in Chinese, and can be translated into English in many different ways. This is a major source of machine translation error, even when syntaxsensitive translation models are used. This paper explores how getting more information about the syntactic, semantic, and discourse context of uses of { (DE) can facilitate producing an appropriate English translation strategy. We describe a finergrained classification of { (DE) constructions in Chinese NPs, construct a corpus of annotated examples, and then train a log-linear classifier, which contains linguistically inspired features. We use the DE classifier to preprocess MT data by explicitly labeling { (DE) constructions, as well as reordering phrases, and show that our approach provides significant BLEU point gains on MT02 (+1.24), MT03 (+0.88) and MT05 (+1.49) on a phrasedbased system. The improvement persists when a hierarchical reordering model is applied.","category_a":"EACL","category_b":"Others","keywords":["BLEU","De Morgan's laws","Linear classifier","Log-linear model","Machine translation","Preprocessor"],"vec":[-0.8357137416,-0.3058071565],"nodes":["2923276","1746807","1812612"]},"03f34688ef4ee4239464633784235387e9bff4bb":{"id":"03f34688ef4ee4239464633784235387e9bff4bb","date":"2013-01-01T12:12:00Z","text":"Unsupervised learning of representations has been found useful in many applications and benefits from several advantages, e.g., where there are many unlabeled exemples and few labeled ones (semi-supervised learning), or where the unlabeled or labeled examples are from a distribution different but related to the one of interest (self-taught learning, multi-task learning, and domain adaptation). Some of these algorithms have successfully been used to learn a hierarchy of features, i.e., to build a deep architecture, either as initialization for a supervised predictor, or as a generative model. Deep learning algorithms can yield representations that are more abstract and better disentangle the hidden factors of variation underlying the unknown generating distribution, i.e., to capture invariances and discover non-local structure in that distribution. This chapter reviews the main motivations and ideas behind deep learning algorithms and their representation-learning components, as well as recent results in this area, and proposes a vision of challenges and hopes on the road ahead.","category_a":"Handbook on Neural Information Processing","category_b":"Others","keywords":["Algorithm","Branch predictor","Computer multitasking","Deep learning","Domain adaptation","Generative model","Initialization (programming)","Machine learning","Multi-task learning","Semi-supervised learning","Supervised learning","Unsupervised learning"],"vec":[-0.4570381649,0.3888347841],"nodes":["1751762","1760871"]},"bf1d7df019f8c5355758a65002dedc9690539819":{"id":"bf1d7df019f8c5355758a65002dedc9690539819","date":"1999-01-01T12:12:00Z","text":"With the explosion of the Internet, LDAP directories and XML, there is an ever greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes\/dimensions, with correlations between the dimensions. EEective query optimization in this context requires good selectivity estimates. In this paper, we use multi-dimensional count-suux trees as the basic framework for sub-string selectivity estimation. Given the enormous size of these trees for large databases, we develop a space and time eecient probabilis-tic algorithm to construct multi-dimensional pruned count-suux trees directly. We then present two techniques to obtain good estimates for a given multi-dimensional sub-string matching query, using a pruned count-suux tree. The rst one, called GNO (for Greedy Non-Overlap), generalizes the greedy parsing suggested by Krishnan et al. 9] for one-dimensional substring selectivity estimation. The second one, called MO (for Maximal Overlap), uses all maximal multi-dimensional substrings of the query for estimation; these multi-dimensional substrings help to capture the correlation that may exist between strings Supported in part by NSF under grant IDM9877060. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and\/or special permission from the Endowment. in the multiple dimensions. We demonstrate experimentally, using real data sets, that MO is substantially superior to GNO in the quality of the estimate.","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Database","Greedy algorithm","IBM Notes","Joint Task Force-Global Network Operations","Lightweight Directory Access Protocol","Overlap\u2013add method","Parsing","Program optimization","Query optimization","Selectivity (electronic)","String searching algorithm","Substring","VLDB","XML"],"vec":[1.0343990886,0.0880590896],"nodes":["1735239","1821764","1736021","1704011"]},"376eec3b84d31d672a3633ed06498fc9a9776f93":{"id":"376eec3b84d31d672a3633ed06498fc9a9776f93","date":"2002-01-01T12:12:00Z","text":"We present an improved method for clustering in the presence of very limited supervisory information, given as pairwise instance constraints. By allowing instance-level constraints to have spacelevel inductive implications, we are able to successfully incorporate constraints for a wide range of data set types. Our method greatly improves on the previously studied constrained -means algorithm, generally requiring less than half as many constraints to achieve a given accuracy on a range of real-world data, while also being more robust when over-constrained. We additionally discuss an active learning algorithm which increases the value of constraints even further.","category_a":"ICML","category_b":"ML","keywords":["Active learning (machine learning)","Algorithm","Cluster analysis"],"vec":[0.1755750264,0.3412226342],"nodes":["38666915","2833700","1812612"]},"7184a8b7be5ae88bca900542b0ab2c27b019f0ce":{"id":"7184a8b7be5ae88bca900542b0ab2c27b019f0ce","date":"2012-01-01T12:12:00Z","text":"In this paper, we present a supervised method to improve the multiple pitch estimation accuracy of the non-negative matrix factorization (NMF) algorithm. The idea is to extend the sparse NMF framework by incorporating pitch information present in time-aligned musical scores in order to extract features that enforce the separability between pitch labels. We introduce two discriminative criteria that maximize inter-class scatter and quantify the predictive potential of a given decomposition using logistic regressors. Those criteria are applied to both the latent variable and the deterministic autoencoder views of NMF, and we devise efficient update rules for each. We evaluate our method on three polyphonic datasets of piano recordings and orchestral instrument mixes. Both models greatly enhance the quality of the basis spectra learned by NMF and the accuracy of multiple pitch estimation.","category_a":"ISMIR","category_b":"Others","keywords":["Algorithm","Autoencoder","Common Criteria","Latent variable","Linear separability","Non-negative matrix factorization","Pitch detection algorithm","Sparse matrix","Stellar classification"],"vec":[-0.2290616911,-0.1108502599],"nodes":["2488222","1751762","1724875"]},"bbb6bef6b2e48f5088f9bc0fd7cf7c07d514ea2a":{"id":"bbb6bef6b2e48f5088f9bc0fd7cf7c07d514ea2a","date":"2015-01-01T12:12:00Z","text":"In this work, we introduce a dataset of video annotated with high quality natural language phrases describing the visual content in a given segment of time. Our dataset is based on the Descriptive Video Service (DVS) that is now encoded on many digital media products such as DVDs. DVS is an audio narration describing the visual elements and actions in a movie for the visually impaired. It is temporally aligned with the movie and mixed with the original movie soundtrack. We describe an automatic DVS segmentation and alignment method for movies, that enables us to scale up the collection of a DVS-derived dataset with minimal human intervention. Using this method, we have collected the largest DVS-derived dataset for video description of which we are aware. Our dataset currently includes over 84.6 hours of paired video\/sentences from 92 DVDs and is growing.","category_a":"ArXiv","category_b":"Journal","keywords":["Audio description","Descriptive Video Service","Digital media","Display resolution","Natural language","Temporal logic"],"vec":[-0.2330924361,-0.4441691035],"nodes":["1730844","1972076","1777528","1760871"]},"16cfb80ff6321fa0e277e0316398742475dcdc84":{"id":"16cfb80ff6321fa0e277e0316398742475dcdc84","date":"2003-01-01T12:12:00Z","text":"XML path queries form the basis of complex filtering of XML data. Most current XML path query processing techniques can be divided in two groups. Navigation-based algorithms compute results by analyzing an input document one tag at a time. In contrast, index-based algorithms take advantage of precomputed numbering schemes over the input XML document. In this paper we introduce a new indexbased technique, Index-Filter, to answer multiple XML path queries. Index-Filter uses indexes built over the document tags to avoid processing large portions of the input document that are guaranteed not to be part of any match. We analyze Index-Filter and compare it against Y-Filter, a stateof-the-art navigation-based technique. We show that both techniques have their advantages, and we discuss the scenarios under which each technique is superior to the other one. In particular, we show that while most XML path query processing techniques work off SAX events, in some cases it pays off to preprocess the input document, augmenting it with auxiliary information that can be used to evaluate the queries faster. We present experimental results over real and synthetic XML documents that validate our claims.","category_a":"ICDE","category_b":"DB","keywords":["Algorithm","Database","Precomputation","Preprocessor","Synthetic data","XML","XPath"],"vec":[0.877220538,0.0560210978],"nodes":["2362833","1684012","1721062","1704011"]},"7509931a7cafe9b0a61337d141d79d76eadaaeed":{"id":"7509931a7cafe9b0a61337d141d79d76eadaaeed","date":"2002-01-01T12:12:00Z","text":"Data Warehousing and OLAP applications typically view data an having multiple logical dimensions (e.g., product, location) with natural hierarchies defined on each dimension. OLAP queries usually involve hierarchical selections on some of the dimensions, and often aggregate measure attributes (e.g., sales, volume). Accurately estimating the distribution of measure attributes, under hierarchical selections, is important in a variety of scenarios, including approximate query evaluation and cost-based optimization of queries.In this paper, we propose fast (near linear time) algorithms for the problem of approximating the distribution of measure attributes with hierarchies defined on them, using histograms. Our algorithms are based on dynamic programming and a novel notion of sparse intervals that we introduce, and are the first <i>practical<\/i> algorithms for this problem. They effectively trade space for construction time without compromising histogram accuracy. We complement our analytical contributions with an experimental evaluation using real data sets, demonstrating the superiority of our approach.","category_a":"PODS","category_b":"Others","keywords":["Algorithm","Approximation algorithm","Dynamic programming","Online analytical processing","Program optimization","Sparse matrix","Time complexity"],"vec":[1.0608384744,0.1937256363],"nodes":["1679363","1721062","1704011"]},"65ad0e876216ea034b7958f016456e32666bc5c6":{"id":"65ad0e876216ea034b7958f016456e32666bc5c6","date":"2013-01-01T12:12:00Z","text":"Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive\/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.","category_a":"","category_b":"Others","keywords":["Parsing","Phrases","Recursion","Sentiment analysis","Treebank","Trees (plant)","sentence"],"vec":[-0.6296105505,-0.7462758945],"nodes":["2166511","24590005","1856239","1964541","1812612","34699434","2205340"]},"2982952a884a8d27fbcd800c010d8f4af564181b":{"id":"2982952a884a8d27fbcd800c010d8f4af564181b","date":"2003-01-01T12:12:00Z","text":"","category_a":"HLT-NAACL","category_b":"ComLing","keywords":[],"vec":[-0.1953673624,-0.0047646908],"nodes":["1812612","38666915"]},"17f5c7411eeeeedf25b0db99a9130aa353aee4ba":{"id":"17f5c7411eeeeedf25b0db99a9130aa353aee4ba","date":"2016-01-01T12:12:00Z","text":"We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and backoff n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger questionanswer pair corpus and from pretrained word embeddings.","category_a":"AAAI","category_b":"AI","keywords":["Artificial neural network","Backoff","Dialog system","Encoder","End-to-end principle","Generative model","Interaction","N-gram","Question answering","Recurrent neural network","Text corpus"],"vec":[-0.3941400334,-0.0869535],"nodes":["35224828","2041695","1751762","1760871","1723772"]},"0899c2c3c31cd555425008d38a1a671f3e37724a":{"id":"0899c2c3c31cd555425008d38a1a671f3e37724a","date":"2008-01-01T12:12:00Z","text":"Validation of multi-column schema matchings is essential for successful database integration. This task is especially difficult when the databases to be integrated contain little overlapping data, as is often the case in practice (e.g., customer bases of different companies). Based on the intuition that values present in different columns related by a schema matching will have similar \";semantic type\";, and that this can be captured using distributions over values (\";statistical types\";), we develop a method for validating 1-1 and compositional schema matchings. Our technique is based on three key technical ideas. First, we propose a generic measure for comparing two columns matched by a schema matching, based on a notion of information-theoretic discrepancy that generalizes the standard geometric discrepancy; this provides the basis for 1:1 matching. Second, we present an algorithm for \";splitting\"; the string values in a column to identify substrings that are likely to match with the values in another column; this enables (multi-column) l:m schema matching. Third, our technique provides an invalidation certificate if it fails to validate a schema matching. We complement our conceptual and algorithmic contributions with an experimental study that demonstrates the effectiveness and efficiency of our technique on a variety of database schemas and data sets.","category_a":"Data Engineering","category_b":"DB","keywords":["1:1 pixel mapping","Algorithm","Column (database)","Database","Database schema","Discrepancy function","Experiment","Heterogeneous database system","Information theory","Matching (graph theory)","Substring","Value (ethics)"],"vec":[0.9982503205,0.105732396],"nodes":["21526180","1721062","1704011","1699730","1747652"]},"87f34d2e08f9f079c189bdb1db1a4d750080a8cc":{"id":"87f34d2e08f9f079c189bdb1db1a4d750080a8cc","date":"2008-01-01T12:12:00Z","text":"We study selectivity estimation techniques for set similarity queries. A wide variety of similarity measures for sets have been proposed in the past. In this work we concentrate on the class of weighted similarity measures (e.g., TF\/IDF and BM25 cosine similarity and variants) and design selectivity estimators based on a priori constructed samples. First, we study the pitfalls associated with straightforward applications of random sampling, and argue that care needs to be taken in how the samples are constructed; uniform random sampling yields very low accuracy, while query sensitive realtime sampling is more expensive than exact solutions (both in CPU and I\/O cost). We show how to build robust samples a priori, based on existing synopses for distinct value estimation. We prove the accuracy of our technique theoretically, and verify its performance experimentally. Our algorithm is orders of magnitude faster than exact solutions and has very small space overhead.","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Central processing unit","Cosine similarity","Sampling (signal processing)","Selectivity (electronic)","Tf\u2013idf"],"vec":[0.1045135834,0.5117302096],"nodes":["1836901","1805402","1721062","1704011"]},"071b16f25117fb6133480c6259227d54fc2a5ea0":{"id":"071b16f25117fb6133480c6259227d54fc2a5ea0","date":"2014-01-01T12:12:00Z","text":"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder\u2013decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder\u2013decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Encoder","Machine translation","Neural machine translation","Statistical machine translation"],"vec":[-1.0876930817,0.157781958],"nodes":["3335364","1979489","1751762"]},"5f4abcc6526ca79fb0d5df14527a9a5ce1c12621":{"id":"5f4abcc6526ca79fb0d5df14527a9a5ce1c12621","date":"2006-01-01T12:12:00Z","text":"XML is widely recognized as the data interchange standard of tomorrow because of its ability to represent data from a variety of sources. Hence, XML is likely to be the format through which data from multiple sources is integrated. In this article, we study the problem of integrating XML data sources through correlations realized as join operations. A challenging aspect of this operation is the XML document structure. Two documents might convey approximately or exactly the same information but may be quite different in structure. Consequently, an approximate match in structure, in addition to content, has to be folded into the join operation. We quantify an approximate match in structure and content for pairs of XML documents using well defined notions of distance. We show how notions of distance that have metric properties can be incorporated in a framework for joins between XML data sources and introduce the idea of reference sets to facilitate this operation. Intuitively, a reference set consists of data elements used to project the data space. We characterize what constitutes a good choice of a reference set, and we propose sampling-based algorithms to identify them. We then instantiate our join framework using the tree edit distance between a pair of trees. We next turn our attention to utilizing well known index structures to improve the performance of approximate XML join operations. We present a methodology enabling adaptation of index structures for this problem, and we instantiate it in terms of the R-tree. We demonstrate the practical utility of our solutions using large collections of real and synthetic XML data sets, varying parameters of interest, and highlighting the performance benefits of our approach.","category_a":"ACM Trans. Database Syst.","category_b":"Others","keywords":["Algorithm","Approximation algorithm","Dataspaces","Document","Edit distance","Graph edit distance","Interchange circuit","R+ tree","R-tree","Sampling (signal processing)","Synthetic data","XML","XML database"],"vec":[1.0267334853,0.0550265431],"nodes":["1679363","1735239","1721062","1704011","1689202"]},"5cceeb8a36f62cce92056b8e055ec9aaa7c3074c":{"id":"5cceeb8a36f62cce92056b8e055ec9aaa7c3074c","date":"2008-01-01T12:12:00Z","text":"How can the development of ideas in a scientific field be studied over time? We apply unsupervised topic modeling to the ACL Anthology to analyze historical trends in the field of Computational Linguistics from 1978 to 2006. We induce topic clusters using Latent Dirichlet Allocation, and examine the strength of each topic over time. Our methods find trends in the field including the rise of probabilistic methods starting in 1988, a steady increase in applications, and a sharp decline of research in semantics and understanding between 1978 and 2001, possibly rising again after 2001. We also introduce a model of the diversity of ideas, topic entropy, using it to show that COLING is a more diverse conference than ACL, but that both conferences as well as EMNLP are becoming broader over time. Finally, we apply Jensen-Shannon divergence of topic distributions to show that all three conferences are converging in the topics they cover.","category_a":"EMNLP","category_b":"ComLing","keywords":["Computational linguistics","Empirical Methods in Natural Language Processing","Jensen's inequality","Kullback\u2013Leibler divergence","Latent Dirichlet allocation","Topic model","Unsupervised learning"],"vec":[-0.2014518086,-0.1437650277],"nodes":["40510209","1746807","1812612"]},"44d6ffe6535f75ccfd4a71a72ac2982077afcb1c":{"id":"44d6ffe6535f75ccfd4a71a72ac2982077afcb1c","date":"2009-01-01T12:12:00Z","text":"Existing evaluation metrics for machine translation lack crucial robustness: their correlations with human quality judgments vary considerably across languages and genres. We believe that the main reason is their inability to properly capture meaning: A good translation candidate means the same thing as the reference translation, regardless of formulation. We propose a metric that evaluates MT output based on a rich set of features motivated by textual entailment, such as lexical-semantic (in-)compatibility and argument structure overlap. We compare this metric against a combination metric of four state-of-theart scores (BLEU, NIST, TER, and METEOR) in two different settings. The combination metric outperforms the individual scores, but is bested by the entailment-based metric. Combining the entailment and traditional features yields further improvements.","category_a":"CNLP","category_b":"Others","keywords":["BLEU","Latent semantic analysis","Machine translation","Meteor","Textual entailment"],"vec":[-0.9731529724,-0.2298506987],"nodes":["1708581","1947267","1746807","1812612"]},"1297074f0e7d6c663c55f7580ca603473842eec9":{"id":"1297074f0e7d6c663c55f7580ca603473842eec9","date":"2005-01-01T12:12:00Z","text":"We present a Chinese word segmentation system submitted to the closed track of Sighan bakeoff 2005. Our segmenter was built using a conditional random field sequence model that provides a framework to use a large number of linguistic features such as character identity, morphological and character reduplication features. Because our morphological features were extracted from the training corpora automatically, our system was not biased toward any particular variety of Mandarin. Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers. Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).","category_a":"CNLP","category_b":"Others","keywords":["Conditional random field","Overfitting","Super Robot Monkey Team Hyperforce Go!","Text corpus","Text segmentation"],"vec":[-0.4594876084,-0.2891427273],"nodes":["39034442","2923276","40147028","1746807","1812612"]},"2009b3c05dd9084a0a1c609abcedd81713ed7150":{"id":"2009b3c05dd9084a0a1c609abcedd81713ed7150","date":"2013-01-01T12:12:00Z","text":"Data lineage is a key component of provenance that helps scientists track and query relationships between input and output data. While current systems readily support lineage relationships at the file or data array level, finer-grained support at an array-cell level is impractical due to the lack of support for user defined operators and the high runtime and storage overhead to store such lineage. We interviewed scientists in several domains to identify a set of common semantics that can be leveraged to efficiently store fine-grained lineage. We use the insights to define lineage representations that efficiently capture common locality properties in the lineage data, and a set of APIs so operator developers can easily export lineage information from user defined operators. Finally, we introduce two benchmarks derived from astronomy and genomics, and show that our techniques can reduce lineage query costs by up to 10&#x00D7; while incuring substantially less impact on workflow runtime and storage.","category_a":"ICDE","category_b":"DB","keywords":["Application programming interface","Benchmark (computing)","Data lineage","Database","Input\/output","Locality of reference"],"vec":[0.6911784969,-0.5152143099],"nodes":["39708150","2033016","1695715"]},"5d363ec087445a226c8f64db03dd7efaa132e854":{"id":"5d363ec087445a226c8f64db03dd7efaa132e854","date":"2012-01-01T12:12:00Z","text":"We compare the recently proposed Discriminative Restricted Boltzmann Machine to the classical Support Vector Machine on a challenging classification task consisting in identifying weapon classes from audio signals. The three weapon classes considered in this work (mortar, rocket and rocket-propelled grenade), are difficult to reliably classify with standard techniques since they tend to have similar acoustic signatures. In addition, specificities of the data available in this study makes it challenging to rigorously compare classifiers, and we address methodological issues arising from this situation. Experiments show good classification accuracy that could make these techniques suitable for fielding on autonomous devices. Discriminative Restricted Boltzmann Machines appear to yield better accuracy than Support Vector Machines, and are less sensitive to the choice of signal preprocessing and model hyperparameters. This last property is especially appealing in such a task where the lack of data makes model validation difficult.","category_a":"Computational Intelligence","category_b":"Others","keywords":["Autonomous car","Boltzmann machine","Data pre-processing","Digital signature","Experiment","Mortar methods","Restricted Boltzmann machine","Statistical classification","Support vector machine","Type signature"],"vec":[-0.1721487519,0.2378194856],"nodes":["1751762","2748188","2460212","1777528","2751789","2011665","2373952"]},"6701889c81ad460f53a4d84361cd3d37b4e02743":{"id":"6701889c81ad460f53a4d84361cd3d37b4e02743","date":"2017-01-01T12:12:00Z","text":"We introduce GuessWhat?!, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions. Higher-level image understanding, like spatial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial baselines of the introduced tasks.","category_a":"CVPR","category_b":"ComVis","keywords":["Computer vision","Deep learning","Dialog system","Modal logic","Spatial\u2013temporal reasoning","Testbed"],"vec":[-0.332619069,-0.1210347367],"nodes":["1952531","3367628","2251963","1721354","1777528","1760871"]},"a4ab5ad02c4ed463e1e8a5c06e8b176ef3dea2ea":{"id":"a4ab5ad02c4ed463e1e8a5c06e8b176ef3dea2ea","date":"2011-01-01T12:12:00Z","text":"Sparse coding is a proven principle for learning compact representations of images. However, sparse coding by itself often leads to very redundant dictionaries. With images, this often takes the form of similar edge detectors which are replicated many times at various positions, scales and orientations. An immediate consequence of this observation is that the estimation of the dictionary components is not statistically efficient. We propose a factored model in which factors of variation (e.g. position, scale and orientation) are untangled from the underlying Gabor-like filters. There is so much redundancy in sparse codes for natural images that our model requires only a single dictionary element (a Gabor-like edge detector) to outperform standard sparse coding. Our model scales naturally to arbitrary-sized images while achieving much greater statistical efficiency during learning. We validate this claim with a number of experiments showing, in part, superior compression of out-of-sample data using a sparse coding dictionary learned with only a single image.","category_a":"ArXiv","category_b":"Journal","keywords":["Autostereogram","Dictionary","Edge detection","Experiment","Grammar-based code","Neural coding","Sparse approximation","Sparse matrix"],"vec":[-0.3473601817,0.3476772472],"nodes":["32837403","1760871","1751762"]},"528c397b0e523c9a257d67bb05409b83b4cdd33d":{"id":"528c397b0e523c9a257d67bb05409b83b4cdd33d","date":"2017-01-01T12:12:00Z","text":"Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the \u201clong tail\u201d of this distribution requires enormous amounts of data. Representations of rare words trained directly on end tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end-to-end for the downstream task. We show that this improves results against baselines where embeddings are trained on the end task for reading comprehension, recognizing textual entailment and language modeling.","category_a":"ArXiv","category_b":"Journal","keywords":["Downstream (software development)","End-to-end principle","Language model","Long tail","Natural language","On the fly","Textual entailment","Vocabulary","Zipf's law"],"vec":[-0.5996894345,-0.3456876983],"nodes":["3335364","3448428","40569328","1864353","1724875","1751762"]},"39371e655d27527c74b6d18080dcd3107273ba8f":{"id":"39371e655d27527c74b6d18080dcd3107273ba8f","date":"2017-01-01T12:12:00Z","text":"Many analysis and machine learning tasks require the availability of marginal statistics on multidimensional datasets while providing strong privacy guarantees for the data subjects. Applications for these statistics range from finding correlations in the data to fitting sophisticated prediction models. In this paper, we provide a set of algorithms for materializing marginal statistics under the strong model of local differential privacy. We prove the first tight theoretical bounds on the accuracy of marginals compiled under each approach, perform empirical evaluation to confirm these bounds, and evaluate them for tasks such as modeling and correlation testing. Our results show that releasing information based on (local) Fourier transformations of the input is preferable to alternatives based directly on (local) marginals.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Differential privacy","Fast Fourier transform","Machine learning","Software testing"],"vec":[1.1372626899,0.5901948449],"nodes":["40415081","1709589","1704011"]},"55657836eb554d4d42ba94db5baa274478a56d8a":{"id":"55657836eb554d4d42ba94db5baa274478a56d8a","date":"2010-01-01T12:12:00Z","text":"We investigate the problem of estimating the density function of multivariate binary data. In particular, we focus on models for which computing the estimated probability of any data point is tractable. In such a setting, previous work has mostly concentrated on mixture modeling approaches. We argue that for the problem of tractable density estimation, the restricted Boltzmann machine (RBM) provides a competitive framework for multivariate binary density modeling. With this in mind, we also generalize the RBM framework and present the restricted Boltzmann forest (RBForest), which replaces the binary variables in the hidden layer of RBMs with groups of tree-structured binary variables. This extension allows us to obtain models that have more modeling capacity but remain tractable. In experiments on several data sets, we demonstrate the competitiveness of this approach and study some of its properties.","category_a":"Neural Computation","category_b":"ML","keywords":["Binary data","Boltzmann machine","Concentrate Dosage Form","Data point","Experiment","Man-Machine Systems","Mixture model","Restricted Boltzmann machine"],"vec":[-0.2034017518,0.4818214482],"nodes":["1777528","1751762","34999784"]},"0c881ea63ff12d85bc3192ce61f37abf701fdf38":{"id":"0c881ea63ff12d85bc3192ce61f37abf701fdf38","date":"2010-01-01T12:12:00Z","text":"We propose a semi-supervised model which segments and annotates images using very few labeled images and a large unaligned text corpus to relate image regions to text labels. Given photos of a sports event, all that is necessary to provide a pixel-level labeling of objects and background is a set of newspaper articles about this sport and one to five labeled images. Our model is motivated by the observation that words in text corpora share certain context and feature similarities with visual objects. We describe images using visual words, a new region-based representation. The proposed model is based on kernelized canonical correlation analysis which finds a mapping between visual and textual words by projecting them into a latent meaning space. Kernels are derived from context and adjective features inside the respective visual and textual domains. We apply our method to a challenging dataset and rely on articles of the New York Times for textual features. Our model outperforms the state-of-the-art in annotation. In segmentation it compares favorably with other methods that use significantly more labeled training data.","category_a":"CVPR","category_b":"ComVis","keywords":["Kernel method","Pixel","Semi-supervised learning","Test set","Text corpus","The New York Times","Visual Objects"],"vec":[-0.3777920714,-0.3934352406],"nodes":["2166511","3216322"]},"62a4a6abc2a52889db51b56ef082d02784266985":{"id":"62a4a6abc2a52889db51b56ef082d02784266985","date":"2006-01-01T12:12:00Z","text":"We propose a novel algorithm for inducing semantic taxonomies. Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns. By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word\u2019s coordinate terms to help in determining its hypernyms, and vice versa. We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1). We add 10, 000 novel synsets to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classifiers. Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs.","category_a":"ACL","category_b":"ComLing","keywords":["Algorithm","Approximation error","Knowledge Search","Mathematical induction","Synonym ring","WordNet"],"vec":[-0.5309675059,-0.5555710814],"nodes":["40187504","1746807","1701538"]},"043e91679d7def20ece119117757ae7178d9eec9":{"id":"043e91679d7def20ece119117757ae7178d9eec9","date":"2016-01-01T12:12:00Z","text":"While O(n) methods for parsing probabilistic context-free grammars (PCFGs) are well known, a tabular parsing framework for arbitrary PCFGs which allows for botton-up, topdown, and other parsing strategies, has not yet been provided. This paper presents such an algorithm, and shows its correctness and advantages over prior work. The paper finishes by bringing out the connections between the algorithm and work on hypergraphs, which permits us to extend the presented Viterbi (best parse) algorithm to an inside (total probability) algorithm.","category_a":"","category_b":"Others","keywords":["Algorithm","Chart parser","Context-free grammar","Context-free language","Correctness (computer science)","License","Parser","Parsing","Stochastic context-free grammar","Table (information)","algorithm"],"vec":[0.0996794489,-1.1427330757],"nodes":["38666915","1812612"]},"26196511e307ec89466af06751a66ee2d95b6305":{"id":"26196511e307ec89466af06751a66ee2d95b6305","date":"2008-01-01T12:12:00Z","text":"Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon\u2019s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.","category_a":"EMNLP","category_b":"ComLing","keywords":["Algorithm","Amazon Mechanical Turk","Machine learning","Natural language","Natural language processing","Textual entailment","The Turk","Word sense","Word-sense disambiguation","World Wide Web"],"vec":[-0.6176385896,-0.3291990523],"nodes":["40187504","7870761","1746807","1701538"]},"fd7447a976968cd4190c65edef8482f4f8e0cab9":{"id":"fd7447a976968cd4190c65edef8482f4f8e0cab9","date":"2016-01-01T12:12:00Z","text":"Neural machine translation has become a major alternative to widely used phrase-based statistical machine translation. We notice however that much of research on neural machine translation has focused on European languages despite its language agnostic nature. In this paper, we apply neural machine translation to the task of Arabic translation (Ar\u2194En) and compare it against a standard phrase-based translation system. We run extensive comparison using various configurations in preprocessing Arabic script and show that the phrase-based and neural translation systems perform comparably to each other and that proper preprocessing of Arabic script has a similar effect on both of the systems. We however observe that the neural machine translation significantly outperform the phrase-based system on an out-of-domain test set, making it attractive for real-world deployment.","category_a":"ArXiv","category_b":"Journal","keywords":["Data pre-processing","Language-independent specification","Machine translation","Neural machine translation","Preprocessor","Software deployment","Statistical machine translation","Test set"],"vec":[-0.9860223871,0.0524753981],"nodes":["2634674","1979489","1696645","1760871"]},"d07a50bb119b594a6f8f9d9c574cbdfdfaa586d3":{"id":"d07a50bb119b594a6f8f9d9c574cbdfdfaa586d3","date":"2016-01-01T12:12:00Z","text":"BigDAWG is a polystore system designed to work on complex problems that naturally span across different processing or storage engines. BigDAWG provides an architecture that supports diverse database systems working with different data models, support for the competing notions of location transparency and semantic completeness via islands of information and a middleware that provides a uniform multi\u2013island interface. In this article, we describe the current architecture of BigDAWG, its application on the MIMIC II medical dataset, and our plans for the mechanics of cross-system queries. During the presentation, we will also deliver a brief demonstration of the current version of BigDAWG.","category_a":"ArXiv","category_b":"Journal","keywords":["Data model","Database","Database engine","MIMIC Simulator","Middleware","Span and div"],"vec":[0.6046377143,-0.4056521585],"nodes":["1867298","38449161","1787375","3257323","2033016","2682150","1695715"]},"45923f515e07b0250643350f8e4c19a74593dc91":{"id":"45923f515e07b0250643350f8e4c19a74593dc91","date":"2005-01-01T12:12:00Z","text":"XML is now widely used and management of XML data has become important. To this end, there has been work on the native management of XML data in a database to utilize the different capabilities of such a system like transaction management and indexing structures. At the heart of such a native XML database is the query evaluator, which provides access methods specifically tailored for XML data manipulation. The design of efficient access methods is the topic of this thesis. The most frequently used operation in an XML database is called structural join. Almost all XML queries contain at least one structural join. The structural join returns matches to a pattern from an XML document. We introduce a new efficient family of algorithms to address this task. These algorithms use a stack data structure that exploits the hierarchy of XML in favor of performance. We then develop variants that permit the combination of other operators, including projection, set difference, and universal quantification, with the structural join operation for greater efficiency. An important value provided by XML is the seamless representation of text and structured data. Querying the text with regard to the structure yields fast and accurate results. However, standard database query paradigms are not suitable for querying text. We introduce the TIX algebra for this purpose, and develop new access methods capable of efficiently computing and combining scores associated with intermediate results. In such applications, one is typically interested in only a few results with the highest scores. We develop new access methods to find results","category_a":"","category_b":"Others","keywords":["Algorithm","Data model","Data structure","Database","Eval","Extensible Markup Language","Join (SQL)","Question (inquiry)","Stack (abstract data type)","Universal quantification","XML","XML database","algorithm"],"vec":[1.1326749806,0.053451499],"nodes":["1978097","2042232","39931057","1704011","35041863","1721062",0,"40165899","2693100","40233765","1919291","31653820","39503788","3301260","2768028","38386922","2519906",0,"2822695","39540394","37513291","2755994","1718694","1683593"]},"7466d1a809d69435f29cebdd651029566eb702f7":{"id":"7466d1a809d69435f29cebdd651029566eb702f7","date":"2011-01-01T12:12:00Z","text":"The problem of privately releasing data is to provide a version of a dataset without revealing sensitive information about the individuals who contribute to the data. The model of differential privacy allows such private release while providing strong guarantees on the output. A basic mechanism achieves differential privacy by adding noise to the frequency counts in the contingency tables (or, a subset of the count data cube) derived from the dataset. However, when the dataset is sparse in its underlying space, as is the case for most multi-attribute relations, then the effect of adding noise is to vastly increase the size of the published data: it implicitly creates a huge number of dummy data points to mask the true data, making it almost impossible to work with. We present techniques to overcome this roadblock and allow efficient private release of sparse data, while maintaining the guarantees of differential privacy. Our approach is to release a compact summary of the noisy data. Generating the noisy data and then summarizing it would still be very costly, so we show how to shortcut this step, and instead directly generate the summary from the input data, without materializing the vast intermediate noisy data. We instantiate this outline for a variety of sampling and filtering methods, and show how to use the resulting summary for approximate, private, query answering. Our experimental study shows that this is an effective, practical solution: in some examples we generate output that is 1000 times smaller than the naive method, in less than 1% of the time while achieving comparable and occasionally improved utility over the costly materialization approach.","category_a":"ArXiv","category_b":"Journal","keywords":["Approximation algorithm","Contingency table","Count data","Data cube","Data point","Differential privacy","Dummy variable (statistics)","Experiment","Information sensitivity","Keyboard shortcut","Naivety","Sampling (signal processing)","Simplicial complex","Sparse","Sparse matrix"],"vec":[0.8556711027,0.6723242523],"nodes":["1709589","1738196","1704011","36323928"]},"c05d7d5ad5b154e5118e8e996a26898b6854eb6e":{"id":"c05d7d5ad5b154e5118e8e996a26898b6854eb6e","date":"2009-01-01T12:12:00Z","text":"We develop a framework for minimizing the communication overhead of monitoring global system parameters in IP networks and sensor networks. A global system predicate is defined as a conjunction of the local properties of different network elements. A typical example is to identify the time windows when the outbound traffic from each network element exceeds a predefined threshold. Our main idea is to optimize the scheduling of local event reporting across network elements for a given network traffic load and local event frequencies. The system architecture consists of N distributed network elements coordinated by a central monitoring station. Each network element monitors a set of local properties and the central station is responsible for identifying the status of global parameters registered in the system. We design an optimal algorithm, the partition and rank (PAR) scheme, when the local events are independent; whereas, when they are dependent, we show that the problem is NP-complete and develop two efficient heuristics: the PAR for dependent events (PAR-D) and adaptive (Ada) algorithms, which adapt well to changing network conditions, and outperform the current state of the art techniques in terms of communication cost.","category_a":"Knowledge and Data Engineering","category_b":"Others","keywords":["Ada","Algorithm","Heuristic","Microsoft Windows","NP-completeness","Network traffic control","Outbound laptop","Scheduling (computing)","Systems architecture"],"vec":[0.0153880722,-1.0773297969],"nodes":["3075508","1721062","40606259","1737487","1704011"]},"8c8b44029fbdac1572ae47b8eaab3929c9987098":{"id":"8c8b44029fbdac1572ae47b8eaab3929c9987098","date":"2008-01-01T12:12:00Z","text":"Online Transaction Processing (OLTP) databases include a suite of features - disk-resident B-trees and heap files, locking-based concurrency control, support for multi-threading - that were optimized for computer technology of the late 1970's. Advances in modern processors, memories, and networks mean that today's computers are vastly different from those of 30 years ago, such that many OLTP databases will now fit in main memory, and most OLTP transactions can be processed in milliseconds or less. Yet database architecture has changed little.\n Based on this observation, we look at some interesting variants of conventional database systems that one might build that exploit recent hardware trends, and speculate on their performance through a detailed instruction-level breakdown of the major components involved in a transaction processing database system (Shore) running a subset of TPC-C. Rather than simply profiling Shore, we progressively modified it so that after every feature removal or optimization, we had a (faster) working system that fully ran our workload. Overall, we identify overheads and optimizations that explain a total difference of about a factor of 20x in raw performance. We also show that there is no single \"high pole in the tent\" in modern (memory resident) database systems, but that substantial time is spent in logging, latching, locking, B-tree, and buffer management operations.","category_a":"SIGMOD","category_b":"DB","keywords":["B+ tree","B-tree","Central processing unit","Computer","Computer data storage","Concurrency (computer science)","Concurrency control","Database","IBM Tivoli Storage Productivity Center","Lock (computer science)","Online transaction processing","Program optimization","Project Looking Glass","Relational database management system","Thread (computing)","Transaction processing"],"vec":[0.6680103084,-0.6093384647],"nodes":["2367532","2254232","2033016","1695715"]},"5272c3e3c06481eb705241a6ed45f1d3905a1459":{"id":"5272c3e3c06481eb705241a6ed45f1d3905a1459","date":"2010-01-01T12:12:00Z","text":"In this paper, we propose a new benchmark for scientific data management systems called SS-DB. This benchmark, loosely modeled on an astronomy workload, is intended to simulate applications that manipulate array-oriented data through relatively sophisticated user-defined functions. SS-DB is representative of the processing performed in a number of scientific domains in addition to astronomy, including earth science, oceanography, and medical image analysis. The benchmark includes three types of operations: (i) manipulation of raw imagery, including processing pixels to extract geo-spatial observations; (ii) manipulation of observations, including spatial aggregation and grouping into related sets; and (iii) manipulation of groups, including a number of relatively complex geometric","category_a":"","category_b":"Others","keywords":["Academic Medical Centers","Astronomy","Benchmark (computing)","Database","Earth Sciences","Image analysis","Medical image computing","Medical imaging","Oceanography","Pixel","User-defined function","Workload","database management software"],"vec":[0.4030352165,-0.2631557143],"nodes":["1680925","32228637","34531502","40260531","2033016","1695715","2031287","10234433"]},"5edf85b3068a0c95d5901623c7fd793ac1c92b38":{"id":"5edf85b3068a0c95d5901623c7fd793ac1c92b38","date":"2014-01-01T12:12:00Z","text":"We study strategies for scalable multi-label annotation, or for efficiently acquiring multiple labels from humans for a collection of items. We propose an algorithm that exploits correlation, hierarchy, and sparsity of the label distribution. A case study of labeling 200 objects using 20,000 images demonstrates the effectiveness of our approach. The algorithm results in up to 6x reduction in human computation time compared to the naive method of querying a human annotator for the presence of every object in every image.","category_a":"CHI","category_b":"HCI","keywords":["Algorithm","Computation","Human-based computation","Multi-label classification","Naivety","Scalability","Sparse matrix","Time complexity"],"vec":[-0.0278705944,-0.1427086539],"nodes":["8342699","2192178","2285165","35609041","39668247","3216322"]},"43fc0302e46e91ffcbae49a2aae1a32dd0e927d9":{"id":"43fc0302e46e91ffcbae49a2aae1a32dd0e927d9","date":"2001-01-01T12:12:00Z","text":"String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string queries directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string processing capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on generating short substrings of length q, called q-grams, and processing them using standard methods available in the DBMS. The proposed technique enables various approximate string processing methods in a DBMS, for example approximate (sub)string selections and joins, and can even be used with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers.","category_a":"IEEE Data Eng. Bull.","category_b":"DB","keywords":["Approximation algorithm","Comparison of programming languages (string functions)","Database","Edit distance","Grams","Relational operator","Substring","User-defined function"],"vec":[0.9925315612,0.0703727155],"nodes":["1684012","2942126","1735239","1721062","1711192","2029251","1704011"]},"0d3fc5945c3ee608af29080c00032b9afd232c36":{"id":"0d3fc5945c3ee608af29080c00032b9afd232c36","date":"2015-01-01T12:12:00Z","text":"Neural machine translation (NMT) systems have recently achieved results comparable to the state of the art on a few translation tasks, including English\u2192French and English\u2192German. The main purpose of the Montreal Institute for Learning Algorithms (MILA) submission to WMT\u201915 is to evaluate this new approach on a greater variety of language pairs. Furthermore, the human evaluation campaign may help us and the research community to better understand the behaviour of our systems. We use the RNNsearch architecture, which adds an attention mechanism to the encoderdecoder. We also leverage some of the recent developments in NMT, including the use of large vocabularies, unknown word replacement and, to a limited degree, the inclusion of monolingual language models.","category_a":"EMNLP","category_b":"ComLing","keywords":["Machine translation","Neural machine translation","Vocabulary"],"vec":[-0.9291593919,-0.0458012768],"nodes":["34564969","2345617","1979489","1710604","1751762"]},"5fc69f93422b11c944b2d53e9d2f93295eca3d19":{"id":"5fc69f93422b11c944b2d53e9d2f93295eca3d19","date":"2010-01-01T12:12:00Z","text":"Autonomous helicopter flight is widely regarded to be a highly challenging control problem. Despite this fact, human experts can reliably fly helicopters through a wide range of maneuvers, including aerobatic maneuvers at the edge of the helicopter\u2019s capabilities. We present apprenticeship learning algorithms, which leverage expert demonstrations to efficiently learn good controllers for tasks being demonstrated by an expert. These apprenticeship learning algorithms have enabled us to significantly extend the state of the art in autonomous helicopter aerobatics. Our experimental results include the first autonomous execution of a wide range of maneuvers, including but not limited to in-place flips, in-place rolls, loops and hurricanes, and even auto-rotation landings, chaos and tic-tocs, which only exceptional human pilots can perform. Our results also include complete airshows, which require autonomous transitions between many of these maneuvers. Our controllers perform as well as, and often even better than, our expert pilot.","category_a":"I. J. Robotics Res.","category_b":"Others","keywords":["Algorithm","Apprenticeship learning","Autonomous car","In-place algorithm","Legal expert system","Machine learning"],"vec":[-0.0341932655,-1.2744032765],"nodes":["1689992","5574038","1701538"]},"20a599c5f152c3f135b9b55a667e64c93ec8d477":{"id":"20a599c5f152c3f135b9b55a667e64c93ec8d477","date":"2016-01-01T12:12:00Z","text":"We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT\u201915 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.","category_a":"HLT-NAACL","category_b":"ComLing","keywords":["Machine translation","Neural machine translation"],"vec":[-1.0602441875,0.1643866351],"nodes":["2345617","1979489","1751762"]},"10c51faad92ff64e93c3688f62edf9756dca20ad":{"id":"10c51faad92ff64e93c3688f62edf9756dca20ad","date":"2003-01-01T12:12:00Z","text":"We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP\/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-theart. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize. In the early 1990s, as probabilistic methods swept NLP, parsing work revived the investigation of probabilistic context-free grammars (PCFGs) (Booth and Thomson, 1973; Baker, 1979). However, early results on the utility of PCFGs for parse disambiguation and language modeling were somewhat disappointing. A conviction arose that lexicalized PCFGs (where head words annotate phrasal nodes) were the key tool for high performance PCFG parsing. This approach was congruent with the great success of word -gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (Ford et al., 1982; Hindle and Rooth, 1993). In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman, 1995; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001). However, several results have brought into question how large a role lexicalization plays in such parsers. Johnson (1998) showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category. The Penn treebank covering PCFG is a poor tool for parsing because the context-freedom assumptions it embodies are far too strong, and weakening them in this way makes the model much better. More recently, Gildea (2001) discusses how taking the bilexical probabilities out of a good current lexicalized PCFG parser hurts performance hardly at all: by at most 0.5% for test text from the same domain as the training data, and not at all for test text from a different domain.1 But it is precisely these bilexical dependencies that backed the intuition that lexicalized PCFGs should be very successful, for example in Hindle and Rooth\u2019s demonstration from PP attachment. We take this as a reflection of the fundamental sparseness of the lexical dependency information available in the Penn Treebank. As a speech person would say, one million words of training data just isn\u2019t enough. Even for topics central to the treebank\u2019s Wall Street Journal text, such as stocks, many very plausible dependencies occur only once, for example stocks stabilized, while many others occur not at all, for example stocks skyrocketed.2 The best-performing lexicalized PCFGs have increasingly made use of subcategorization3 of the There are minor differences, but all the current best-known lexicalized PCFGs employ both monolexical statistics, which describe the phrasal categories of arguments and adjuncts that appear around a head lexical item, and bilexical statistics, or dependencies, which describe the likelihood of a head word taking as a dependent a phrase headed by a certain other word. This observation motivates various classor similaritybased approaches to combating sparseness, and this remains a promising avenue of work, but success in this area has proven somewhat elusive, and, at any rate, current lexicalized PCFGs do simply use exact word matches if available, and interpolate with syntactic category-based estimates when they are not. In this paper we use the term subcategorization in the original general sense of Chomsky (1965), for where a syntactic catcategories appearing in the Penn treebank. Charniak (2000) shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and Collins (1999) uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating \u201cbase NPs\u201d from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject NP. While he gives incomplete experimental results as to their efficacy, we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization. In this paper, we show that the parsing performance that can be achieved by an unlexicalized PCFG is far higher than has previously been demonstrated, and is, indeed, much higher than community wisdom has thought possible. We describe several simple, linguistically motivated annotations which do much to close the gap between a vanilla PCFG and state-of-the-art lexicalized models. Specifically, we construct an unlexicalized PCFG which outperforms the lexicalized PCFGs of Magerman (1995) and Collins (1996) (though not more recent models, such as Charniak (1997) or Collins (1999)). One benefit of this result is a much-strengthened lower bound on the capacity of an unlexicalized PCFG. To the extent that no such strong baseline has been provided, the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing, rather than looking critically at where lexicalized probabilities are both needed to make the right decision and available in the training data. Secondly, this result affirms the value of linguistic analysis for feature discovery. The result has other uses and advantages: an unlexicalized PCFG is easier to interpet, reason about, and improve than the more complex lexicalized models. The grammar representation is much more compact, no longer requiring large structures that store lexicalized probabilities. The parsing algorithms have lower asymptotic complexity4 and have much smaller grammar constants. An unlexicalized PCFG parser is much egory is divided into several subcategories, for example dividing verb phrases into finite and non-finite verb phrases, rather than in the modern restricted usage where the term refers only to the syntactic argument frames of predicators. 4 vs. for a naive implementation, or vs. if using the clever approach of Eisner and Satta (1999). simpler to build and optimize, including both standard code optimization techniques and the investigation of methods for search space pruning (Caraballo and Charniak, 1998; Charniak et al., 1998). It is not our goal to argue against the use of lexicalized probabilities in high-performance probabilistic parsing. It has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities, and a parser should make use of such information where possible. We focus here on using unlexicalized, structural context because we feel that this information has been underexploited and underappreciated. We see this investigation as only one part of the foundation for state-of-the-art parsing which employs both lexical and structural conditioning. 1 Experimental Setup To facilitate comparison with previous work, we trained our models on sections 2\u201321 of the WSJ section of the Penn treebank. We used the first 20 files (393 sentences) of section 22 as a development set (devset). This set is small enough that there is noticeable variance in individual results, but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill-climb. All of section 23 was used as a test set for the final model. For each model, input trees were annotated or transformed in some way, as in Johnson (1998). Given a set of transformed trees, we viewed the local trees as grammar rewrite rules in the standard way, and used (unsmoothed) maximum-likelihood estimates for rule probabilities.5 To parse the grammar, we used a simple array-based Java implementation of a generalized CKY parser, which, for our final best model, was able to exhaustively parse all sentences in section 23 in 1GB of memory, taking approximately 3 sec for average length sentences.6 The tagging probabilities were smoothed to accommodate unknown words. The quantity was estimated as follows: words were split into one of several categories \"! ! , based on capitalization, suffix, digit, and other character features. For each of these categories, we took the maximum-likelihood estimate of # $ \"! ! . This distribution was used as a prior against which observed taggings, if any, were taken, giving &%(' ) +*, .-","category_a":"ACL","category_b":"ComLing","keywords":["Algorithm","Baseline (configuration management)","Computational complexity theory","Context-free grammar","Context-free language","Gigabyte","Interpolation","Java","LR parser","Language model","Machine learning","Naivety","Natural language processing","Neural coding","Parsing","Phrasal template","Program optimization","Rewrite (programming)","Rewriting","Silent Hill: Downpour","Smoothing","Speech recognition","Stochastic context-free grammar","Test set","The Wall Street Journal","Treebank","Word-sense disambiguation"],"vec":[-0.4444700956,-0.5701894079],"nodes":["38666915","1812612"]},"01cbebbbcb973ae2d1d32a49fa1f1e0738153ba9":{"id":"01cbebbbcb973ae2d1d32a49fa1f1e0738153ba9","date":"2016-01-01T12:12:00Z","text":"We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity sub-networks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies the modules and input features for which the DCN\u2019s output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar performance.","category_a":"ICML","category_b":"ML","keywords":["Artificial neural network","Computation","Convolutional neural network","Dynamic circuit network","Gradient","MNIST database"],"vec":[-0.0765919781,0.050419661],"nodes":["2634674","2482072","2348758","3262347","1777528","1760871"]},"8c3679bab6b379a3f487c68f631f55bb18292bc0":{"id":"8c3679bab6b379a3f487c68f631f55bb18292bc0","date":"2016-01-01T12:12:00Z","text":"Convolutional Neural Networks (CNNs) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition (ASR). Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models\/Gaussian Mixture Models (HMMs\/GMMs) have achieved the state-of-the-art in various benchmarks. Meanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural Networks (RNNs), which is proposed for labeling unsegmented sequences, makes it feasible to train an \u2018end-to-end\u2019 speech recognition system instead of hybrid settings. However, RNNs are computationally expensive and sometimes difficult to train. In this paper, inspired by the advantages of both CNNs and the CTC approach, we propose an end-to-end speech framework for sequence labeling, by combining hierarchical CNNs with CTC directly without recurrent connections. By evaluating the approach on the TIMIT phoneme recognition task, we show that the proposed model is not only computationally efficient, but also competitive with the existing baseline systems. Moreover, we argue that CNNs have the capability to model temporal correlations with appropriate context information.","category_a":"INTERSPEECH","category_b":"Others","keywords":["Analysis of algorithms","Baseline (configuration management)","Computational complexity theory","Connectionism","Convolutional neural network","End-to-end principle","Hidden Markov model","Neural Networks","Recurrent neural network","Sequence labeling","Speech recognition","TIMIT"],"vec":[-0.3606288149,0.3657497142],"nodes":["1720117","2719055","39728786","35097114","40201308","1751762","1760871"]},"133eacaf0ad25b8364cb4510007d9363298e8adf":{"id":"133eacaf0ad25b8364cb4510007d9363298e8adf","date":"2009-01-01T12:12:00Z","text":"There is currently considerable enthusiasm around the MapReduce (MR) paradigm for large-scale data analysis [17]. Although the basic control flow of this framework has existed in parallel SQL database management systems (DBMS) for over 20 years, some have called MR a dramatically new computing model [8, 17]. In this paper, we describe and compare both paradigms. Furthermore, we evaluate both kinds of systems in terms of performance and development complexity. To this end, we define a benchmark consisting of a collection of tasks that we have run on an open source version of MR as well as on two parallel DBMSs. For each task, we measure each system's performance for various degrees of parallelism on a cluster of 100 nodes. Our results reveal some interesting trade-offs. Although the process to load data into and tune the execution of parallel DBMSs took much longer than the MR system, the observed performance of these DBMSs was strikingly better. We speculate about the causes of the dramatic performance difference and consider implementation concepts that future systems should take from both kinds of architectures.","category_a":"SIGMOD","category_b":"DB","keywords":["Benchmark (computing)","Control flow","Database","MapReduce","Open-source software","Parallel computing","Performance","SQL"],"vec":[0.6666377586,-0.5066433454],"nodes":["1774210","12067489","6850164","2254232","1765659","2033016","1695715"]},"1ec8bcda29ae8159b0beb989fef08f6c332e4993":{"id":"1ec8bcda29ae8159b0beb989fef08f6c332e4993","date":"2000-01-01T12:12:00Z","text":"As our world gets more networked, ever increasing amounts of information are being stored in LDAP directories. While LDAP directories have considerable exibility in the modeling and retrieval of information for network applications, the notion of schema they provide for enabling consistent and coherent representation of directory information is rather weak. In this paper, we propose an expressive notion of bounding-schemas for LDAP directories, and illustrate their practical utility. Bounding-schemas are based on lower bound and upper bound speciications for the content and structure of an LDAP directory. Given a bounding-schema speciication, we present algorithms to eeciently determine: (i) if an LDAP directory is legal w.r.t. the bounding-schema, and (ii) if directory insertions and deletions preserve legality. Finally, we show that the notion of bounding-schemas has wider applicability, beyond the speciic context of LDAP directories.","category_a":"EDBT","category_b":"Others","keywords":["Algorithm","Lightweight Directory Access Protocol","Minimum bounding box","Our World"],"vec":[0.8462562702,0.0364947238],"nodes":["1736846","1735239","1708593","1704011"]},"2dfbd860542f79240f8f58cc042db1dfa562053a":{"id":"2dfbd860542f79240f8f58cc042db1dfa562053a","date":"2010-01-01T12:12:00Z","text":"The relentless pace at which textual data are generated on-line necessitates novel paradigms for their understanding and exploration. To this end, we introduce a methodology for discovering strong entity associations in all the slices (meta-data value restrictions) of a document collection. Since related documents mention approximately the same group of core entities (people, locations, etc.), the groups of coupled entities discovered can be used to expose themes in the document collection. We devise and evaluate algorithms capable of addressing two flavors of our core problem: algorithm THR-ENT for computing all sufficiently strong entity associations and algorithm TOP-ENT for computing the top-k strongest entity associations, for each slice of the document collection.","category_a":"ICDE","category_b":"DB","keywords":["Algorithm","Archive","Document","Entity","Entity\u2013relationship model","Little Big Adventure","Online and offline","Text corpus","Theme (computing)","Value restriction"],"vec":[0.8067739272,0.0117927458],"nodes":["1796624","1761975","1721062","1704011"]},"506ccb2fe5b756b35381b750fb996d014fbc8014":{"id":"506ccb2fe5b756b35381b750fb996d014fbc8014","date":"2008-01-01T12:12:00Z","text":"Minimum error rate training (MERT) is a widely used learning procedure for statistical machine translation models. We contrast three search strategies for MERT: Powell\u2019s method, the variant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell\u2019s method and coordinate descent.","category_a":"ACL","category_b":"ComLing","keywords":["BLEU","Coordinate descent","Machine translation","Matrix regularization","Moses","Multi-Environment Real-Time","Powell's method","Statistical machine translation","Test set"],"vec":[-0.3213467671,0.2959726115],"nodes":["3208422","1746807","1812612"]},"fe994dc0a8d9a365c5dc34e573dbf7c481b978df":{"id":"fe994dc0a8d9a365c5dc34e573dbf7c481b978df","date":"2007-01-01T12:12:00Z","text":"Disambiguation H. Tolga Ilhan, Sepandar D. Kamvar, Dan Klein, Christopher D. Manning and Kristina Toutanova Computer S ien e Department Stanford University Stanford, CA 94305-9040, USA Abstra t The Stanford-CS224N system is an ensemble of simple lassi ers. The rst-tier systems are heterogeneous, onsisting primarily of naive-Bayes variants, but also in luding ve tor spa e, memory-based, and other lassi er types. These simple lassi ers are ombined by a se ond-tier lassi er, whi h variously uses majority voting, weighted voting, or a maximum entropy model. Results from Senseval-2 lexi al sample tasks indi ate that, while the individual lassi ers perform at a level omparable to middles oring team's systems, the ombination a hieves high performan e. In this paper, we dis uss both our system and lessons learned from its behavior. 1 Introdu tion The problem of supervised word sense disambiguation (wsd) has been approa hed using many di erent lassi ation algorithms, in luding naive Bayes, de ision trees, de ision lists, and memory-based learners. While it is unquestionable that ertain algorithms are better suited to the wsd problem than others (for a omparison, see Mooney (1996)), it seems to be the ase that, given similar features as input, various algorithms do not behave dramati ally di erently. This was seen in the Senseval-2 results where a large fra tion of the systems had s ores lustered in a fairly narrow region. We began building our system with 23 supervised wsd systems, ea h submitted by a student taking the natural language pro essing ourse (CS224N) at Stanford University. Students were free to implement whatever wsd This paper is based on work supported in part by the National S ien e Foundation under Grants IIS-0085896 and IIS-9982226, by an NSF Graduate Fellowship, and by the Resear h Collaboration between NTT Communiation S ien e Laboratories, Nippon Telegraph and Telephone Corporation and CSLI, Stanford University. Entropy Cross Validation Voting","category_a":"","category_b":"Others","keywords":["Algorithm","Cross Reactions","Distributed Interactive Simulation","IBM Notes","Multitier architecture","Naive Bayes classifier","Naivety","Natural language","Nipponia nippon","Principle of maximum entropy","Stanford University centers and institutes","Status Epilepticus","Tor Messenger","Trees (plant)","Word sense","Word-sense disambiguation","algorithm","funding grant"],"vec":[-0.1033599451,-0.3191208021],"nodes":["32227558","2833700","38666915","1812612","3259253"]},"7b4cd55ce7d7558eecb8d39f25a05cfd01ce4715":{"id":"7b4cd55ce7d7558eecb8d39f25a05cfd01ce4715","date":"2017-01-01T12:12:00Z","text":"We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.","category_a":"Computer Speech & Language","category_b":"Others","keywords":["Bag-of-words model","Encoder","Experiment","Machine translation","Neural machine translation","Nonlinear system","Type system","Word embedding"],"vec":[-0.9601637946,-0.0800797239],"nodes":["40663606","1979489","1751762"]},"9f5f5ecf0ee423f8493e3a600c6af9a7f8720d8e":{"id":"9f5f5ecf0ee423f8493e3a600c6af9a7f8720d8e","date":"2005-01-01T12:12:00Z","text":"In this paper we present the <i>Threshold Join Algorithm (TJA)<\/i>, which is an efficient TOP-k query processing algorithm for distributed sensor networks. The objective of a top-k query is to find the <i>k<\/i> highest ranked answers to a user defined similarity function. The evaluation of such a query in a sensor network environment is associated with the transfer of data over an extremely expensive communication medium. <i>TJA<\/i> uses a non-uniform threshold on the queried attribute in order to minimize the number of tuples that have to be transferred towards the querying node. Additionally, <i>TJA<\/i> resolves queries in the network rather than in a centralized fashion, which minimizes even more the consumption of bandwidth and delay. Our preliminary experimental results, using our trace driven simulator, show that TJA is both practical and efficient.","category_a":"DMSN","category_b":"Others","keywords":["Algorithm","Bandwidth (signal processing)","Centralisation","Database","Join (SQL)","Relational algebra","Sensor","Similarity measure","Simulation"],"vec":[0.6875413175,0.0137036314],"nodes":["2605417","3104475","1736832","1685532","1761528","1699100","1721062","1704011"]},"af5a145735128e1aa4782f8eb8981ff5e3292b2a":{"id":"af5a145735128e1aa4782f8eb8981ff5e3292b2a","date":"1999-01-01T12:12:00Z","text":"Much of the data we deal with every day is organized hierarchically: file systems, library classification schemes and yellow page categories are salient examples. Business data too, benefits from a hierarchical organization, and indeed the hierarchical data model was quite prevalent thirty years ago. Due to the recently increased importance of X.500\/LDAP directories, which are hierarchical, and the prevalence of aggregation hierarchies in datacubes, there is now renewed interest in the hierarchical organization of data. In this paper, we develop a framework for a modern hierarchical data model, substantially improved from the original version by taking advantage of the lessons learned in the relational database context. We argue that this new hierarchical data model has many benefits with respect to the ubiquitous flat relational data model. key words: hierarchy, X.500, LDAP, directories, white pages, data warehousing","category_a":"","category_b":"Others","keywords":["Categories","Data model","Hierarchical database model","Library classification","Lightweight Directory Access Protocol","Published Directory","Relational database","Relational model","Web Services Discovery","X.500","benefit"],"vec":[0.4590695953,-0.0873125039],"nodes":["1735239","1708593","1704011"]},"54636e41ef8dc33f818aec7b9b02731eb75650d6":{"id":"54636e41ef8dc33f818aec7b9b02731eb75650d6","date":"2005-01-01T12:12:00Z","text":"The inherent order within the XML document-centric data model is typically exposed through positional predicates defined over the XPath navigation axes. Although processing algorithms for each axis have already been proposed, the incorporation of positional predicates in them has received very little attention. In this paper, we present techniques that leverage the power of existing, state of the art methods, to efficiently support positional predicates as well. Our preliminary experimental comparisons with alternative approaches reveal the performance benefits of the proposed techniques.","category_a":"XSym","category_b":"Others","keywords":["Algorithm","Data model","XML","XPath"],"vec":[0.8268555221,0.0353012776],"nodes":["3104475","1721062","1704011","1761528"]},"0bdaf705c237a53b8db50dbdb62f1451774cb2a3":{"id":"0bdaf705c237a53b8db50dbdb62f1451774cb2a3","date":"2003-01-01T12:12:00Z","text":"We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.","category_a":"HLT-NAACL","category_b":"ComLing","keywords":["A* search algorithm","Algorithm","Best, worst and average case","Cubic function","Parsing","Precomputation","Speedup","Stochastic context-free grammar","Table (information)","Time complexity","Treebank","Viterbi algorithm"],"vec":[0.1176918987,-1.1635506016],"nodes":["38666915","1812612"]},"78c91d969c55a4a61184f81001c376810cdbd541":{"id":"78c91d969c55a4a61184f81001c376810cdbd541","date":"2011-01-01T12:12:00Z","text":"We introduce the spike and slab Restricted Boltzmann Machine, characterized by having both a real-valued vector, the slab, and a binary variable, the spike, associated with each unit in the hidden layer. The model possesses some practical properties such as being amenable to Block Gibbs sampling as well as being capable of generating similar latent representations of the data to the recently introduced mean and covariance Restricted Boltzmann Machine. We illustrate how the spike and slab Restricted Boltzmann Machine achieves competitive performance on the CIFAR-10 object recognition task.","category_a":"AISTATS","category_b":"ML","keywords":["Binary data","Boltzmann machine","Gibbs sampling","Outline of object recognition","Restricted Boltzmann machine","Sampling (signal processing)","Slab allocation","The Spike (1997)"],"vec":[-0.3628413902,0.3745639857],"nodes":["1760871","32837403","1751762"]},"5a5d6b0a9a7036c5417a618925c8e03d786e7d74":{"id":"5a5d6b0a9a7036c5417a618925c8e03d786e7d74","date":"2006-01-01T12:12:00Z","text":"At AT&amp;T Labs-Research, we have been developing a prototype system called SPIDER to efficiently support flexible string matching of attribute values in large databases. SPIDER has been used in AT&T, both as a key component of an operational portal for matching customer names and addresses, and for a variety of ad hoc data quality analyses. In this talk, we report on experiences with SPIDER.","category_a":"SIGMOD","category_b":"DB","keywords":["Data quality","Database","Experience","Hoc (programming language)","String searching algorithm"],"vec":[0.9750359425,0.1032531871],"nodes":["1721062","2440762","1704011"]},"7cc937645ddee844b7171b88b587a8d9af98c9bc":{"id":"7cc937645ddee844b7171b88b587a8d9af98c9bc","date":"2000-01-01T12:12:00Z","text":"As databases have expanded in scope from storing purely business data to include XML documents, product catalogs, e-mail messages, and directory data, it has become increasingly important to search databases based on wild-card string matching: prefix matching, for example, is more common (and useful) than exact matching, for such data. In many cases, matches need to be on multiple attributes\/dimensions, with correlations between the dimensions. Traditional multi-dimensional index structures, designed with (fixed length) numeric data in mind, are not suitable for matching unbounded length string data.\nIn this paper, we describe a general technique for adapting a multi-dimensional index structure for wild-card indexing of unbounded length string data. The key ideas are (a) a carefully developed mapping function from strings to rational numbers, (b) representing an unbounded length string in an index leaf page by a fixed length offset to an external key, and (c) storing multiple elided tries, one per dimension, in an index page to prune search during traversal of index pages. These basic ideas affect all index algorithms. In this paper, we present efficient algorithms for different types of string matching.\nWhile our technique is applicable to a wide range of multi-dimensional index structures, we instantiate our generic techniques by adapting the 2-dimensional R-tree to string data. We demonstrate the space effectiveness and time benefits of using the string R-tree both analytically and experimentally.","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","Database","Email","Home page","R+ tree","R-tree","String searching algorithm","XML"],"vec":[1.0727417188,0.0635942624],"nodes":["1735239","1721062","1704011"]},"db82c8e83d2ef333250126655511498b8114b18e":{"id":"db82c8e83d2ef333250126655511498b8114b18e","date":"2005-01-01T12:12:00Z","text":"We present a system for deciding whether a given sentence can be inferred from text. Each sentence is represented as a directed graph (extracted from a dependency parser) in which the nodes represent words or phrases, and the links represent syntactic and semantic relationships. We develop a learned graph matching model to approximate entailment by the amount of the sentence\u2019s semantic content which is contained in the text. We present results on the Recognizing Textual Entailment dataset (Dagan et al., 2005), and show that our approach outperforms BagOf-Words and TF-IDF models. In addition, we explore common sources of errors in our approach and how to remedy them.","category_a":"EMNLP","category_b":"ComLing","keywords":["Directed graph","Matching (graph theory)","Parsing","Textual entailment","Tf\u2013idf","The Sentence"],"vec":[-0.389460643,-0.3585867954],"nodes":["1761880","1701538","1812612"]},"02537a4d8781435a40f768f479a68e7a613dd26f":{"id":"02537a4d8781435a40f768f479a68e7a613dd26f","date":"2001-01-01T12:12:00Z","text":"This paper discusses ensembles of simple but heterogeneous classifiers for word-sense disambiguation, examining the Stanford-CS224N system entered in the SENSEVAL-2 English lexical sample task. First-order classifiers are combined by a second-order classifier, which variously uses majority voting, weighted voting, or a maximum entropy model. While individual first-order classifiers perform comparably to middle-scoring teams\u2019 systems, the combination achieves high performance. We discuss trade-offs and empirical performance. Finally, we present an analysis of the combination, examining how ensemble performance depends on error independence and task difficulty.","category_a":"SENSEVAL","category_b":"Others","keywords":["Error detection and correction","Principle of maximum entropy","Word sense","Word-sense disambiguation"],"vec":[-0.3782380158,-0.4932249418],"nodes":["38666915","3259253","6286797","2833700","1812612"]},"0bfbdafdfbcc268860fe54ae4d8f08d487bcc762":{"id":"0bfbdafdfbcc268860fe54ae4d8f08d487bcc762","date":"2006-01-01T12:12:00Z","text":"Autonomous helicopter flight is widely regarded to be a highly challenging control problem. This paper presents the first successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward flip and sideways roll at low speed, tail-in funnel, and nose-in funnel. Our experimental results significantly extend the state of the art in autonomous helicopter flight. We used the following approach: First we had a pilot fly the helicopter to help us find a helicopter dynamics model and a reward (cost) function. Then we used a reinforcement learning (optimal control) algorithm to find a controller that is optimized for the resulting model and reward function. More specifically, we used differential dynamic programming (DDP), an extension of the linear quadratic regulator (LQR).","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Autonomous car","Differential dynamic programming","Dynamic programming","Loss function","Optimal control","Reinforcement learning"],"vec":[-0.0357272157,-1.2631106992],"nodes":["1689992","5574038","39100828","1701538"]},"273121804df13771df2a6890f50e54e7ec419f75":{"id":"273121804df13771df2a6890f50e54e7ec419f75","date":"2004-01-01T12:12:00Z","text":"Data Cleaning is an important process that has been at the center of research interest in recent years. An important end goal of effective data cleaning is to identify the relational tuple or tuples that are \u201cmost related\u201d to a given query tuple. Various techniques have been proposed in the literature for efficiently identifying approximate matches to a query string against a single attribute of a relation. In addition to constructing a ranking (i.e., ordering) of these matches, the techniques often associate, with each match, scores that quantify the extent of the match. Since multiple attributes could exist in the query tuple, issuing approximate match operations for each of them separately will effectively create a number of ranked lists of the relation tuples. Merging these lists to identify a final ranking and scoring, and returning the top-K tuples, is a challenging task. In this paper, we adapt the well-known footrule distance (for merging ranked lists) to effectively deal with scores. We study efficient algorithms to merge rankings, and produce the top-K tuples, in a declarative way. Since techniques for approximately matching a query string against a single attribute in a relation are typically best deployed in a database, we introduce and describe two novel algorithms for this problem and we provide SQL specifications for them. Our experimental case study, using real application data along with a realization of our proposed techniques on a commercial data base system, highlights the benefits of the proposed algorithms and attests to the overall effectiveness and practicality of our approach.","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Approximation algorithm","Database","Foreach loop","Query string","SQL","Sputter cleaning","Whole Earth 'Lectronic Link"],"vec":[0.9990549111,0.1054405763],"nodes":["1679363","1721062","2440762","1704011"]},"620d6ebb2e7604f6dd838210a6847239bc2670dd":{"id":"620d6ebb2e7604f6dd838210a6847239bc2670dd","date":"2002-01-01T12:12:00Z","text":"XML permits repeated and missing sub-elements, and missing attributes. We discuss the consequent implications on grouping, both with respect to specification and with respect to implementation. The techniques described here have been implemented in the TIMBER native XML database system being developed at the University of Michigan.","category_a":"EDBT","category_b":"Others","keywords":["XML","XML database","XSLT\/Muenchian grouping"],"vec":[0.6489953104,0.0016397968],"nodes":["2822695","1978097","1735239","1708593","2183125","1704011","1683593"]},"019902292dff81eae20f3e87970dd7a1151d9405":{"id":"019902292dff81eae20f3e87970dd7a1151d9405","date":"1998-01-01T12:12:00Z","text":"The database research community is rightly proud of success in basic research, and its remarkable record of technology transfer. Now the field needs to radically broaden its research focus to attack the issues of capturing, storing, analyzing, and presenting the vast array of online data. The database research community should embrace a broader research agenda &#8212; broadening the definition of database management to embrace all the content of the Web and other online data stores, and rethinking our fundamental assumptions in light of technology shifts. To accelerate this transition, we recommend changing the way research results are evaluated and presented. In particular, we advocate encouraging more speculative and long-range work, moving conferences to a poster format, and publishing all research literature on the Web.","category_a":"SIGMOD","category_b":"DB","keywords":["Data store","Scientific literature","Speculative execution","World Wide Web"],"vec":[0.3995499845,-0.1918470109],"nodes":["1737944","6943123","1705257","1765659","1712149","1695250","19967209","39334739","1695576","1735239","3142223","1740962","5151034","2886859","1695715","1742391"]},"26cb14c9d22cf946314d685fe3541ef9f641e429":{"id":"26cb14c9d22cf946314d685fe3541ef9f641e429","date":"2012-01-01T12:12:00Z","text":"Full end-to-end text recognition in natural images is a challenging problem that has received much attention recently. Traditional systems in this area have relied on elaborate models incorporating carefully hand-engineered features or large amounts of prior knowledge. In this paper, we take a different route and combine the representational power of large, multilayer neural networks together with recent developments in unsupervised feature learning, which allows us to use a common framework to train highly-accurate text detector and character recognizer modules. Then, using only simple off-the-shelf methods, we integrate these two modules into a full end-to-end, lexicon-driven, scene text recognition system that achieves state-of-the-art performance on standard benchmarks, namely Street View Text and ICDAR 2003.","category_a":"ICPR","category_b":"Others","keywords":["Artificial neural network","Convolutional neural network","End-to-end principle","Feature learning","Finite-state machine","Google Street View","International Conference on Document Analysis and Recognition","Lexicon","Optical character recognition"],"vec":[-0.5201756767,-0.3879429791],"nodes":["1685072","25629078","5574038","1701538"]},"0d75052f1d7350fa035a35566555ce7b65d1cd2f":{"id":"0d75052f1d7350fa035a35566555ce7b65d1cd2f","date":"2016-01-01T12:12:00Z","text":"The task of associating images and videos with a natural language description has attracted a great amount of attention recently. The state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvements. Instead of proposing new models, this work investigates performances that an oracle can obtain. In order to disentangle the contribution from visual model from the language model, our oracle assumes that highquality visual concept extractor is available and focuses only on the language part. We demonstrate the construction of such oracles on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). Surprisingly, despite the simplicity of the model and the training procedure, we show that current state-of-the-art models fall short when being compared with the learned oracle. Furthermore, it suggests the inability of current models in capturing important visual concepts in captioning tasks.","category_a":"BMVC","category_b":"Others","keywords":["Extractor (mathematics)","Language model","Natural language","Performance","Random oracle","Visual modeling","Voice activity detection"],"vec":[-0.2794066993,0.0656140871],"nodes":["1685369","2482072","1979489","1732563","1751762"]},"0d6aa509c98b197a1f410f31e6fa760fde40d8a1":{"id":"0d6aa509c98b197a1f410f31e6fa760fde40d8a1","date":"2008-01-01T12:12:00Z","text":"Database research is expanding, with major efforts in system architecture, new languages, cloud services, mobile and virtual worlds, and interplay between structure and text.","category_a":"Commun. ACM","category_b":"Journal","keywords":["Cloud computing","Systems architecture","Virtual world"],"vec":[0.2961737906,-0.123798764],"nodes":["1680081","1728318","1737944","1759010","1703347","1728620","3030274","1697597","1712149","1695250","1710965","1692259","2046257","1770962","1695576","1684197","1695692","1691108","2033016","3139601","1693070","39143781","1709145","1770124","1695715","7934073","1751591"]},"19150b001031cc6d964e83cd28553004f653cc24":{"id":"19150b001031cc6d964e83cd28553004f653cc24","date":"2016-01-01T12:12:00Z","text":"Visual relationships capture a wide variety of interactions between pairs of objects in images (e.g. \u201cman riding bicycle\u201d and \u201cman pushing bicycle\u201d). Consequently, the set of possible relationships is extremely large and it is difficult to obtain sufficient training examples for all possible relationships. Because of this limitation, previous work on visual relationship detection has concentrated on predicting only a handful of relationships. Though most relationships are infrequent, their objects (e.g. \u201cman\u201d and \u201cbicycle\u201d) and predicates (e.g. \u201criding\u201d and \u201cpushing\u201d) independently occur more frequently. We propose a model that uses this insight to train visual models for objects and predicates individually and later combines them together to predict multiple relationships per image. We improve on prior work by leveraging language priors from semantic word embeddings to finetune the likelihood of a predicted relationship. Our model can scale to predict thousands of types of relationships from a few examples. Additionally, we localize the objects in the predicted relationships as bounding boxes in the image. We further demonstrate that understanding relationships can improve content based image retrieval.","category_a":"ECCV","category_b":"ComVis","keywords":["Content-based image retrieval","Image retrieval","Interaction","Visual Basic[.NET]"],"vec":[-0.4346315983,-0.3897251036],"nodes":["1830034","2580593","35609041","3216322"]},"1d8bfe40212695fdd05fd1e297f9584a30f477c4":{"id":"1d8bfe40212695fdd05fd1e297f9584a30f477c4","date":"2010-01-01T12:12:00Z","text":"Anonymization of social networks before they are published or shared has become an important research question. Recent work on anonymizing social networks has looked at privacy preserving techniques for publishing a single instance of the network. However, social networks evolve and a single instance is inadequate for analyzing the evolution of the social network or for performing any longitudinal data analysis. We study the problem of repeatedly publishing social network data as the network evolves, while preserving privacy of users. Publishing multiple instances of the same network independently has privacy risks, since stitching the information together may allow an adversary to identify users in the networks.\n We propose methods to anonymize a dynamic network such that the privacy of users is preserved when new nodes and edges are added to the published network. These methods make use of link prediction algorithms to model the evolution of the social network. Using this predicted graph to perform group-based anonymization, the loss in privacy caused by new edges can be reduced. We evaluate the privacy loss on publishing multiple social network instances using our methods.","category_a":"WWW","category_b":"Others","keywords":["Adversary (cryptography)","Algorithm","Anonymous web browsing","Data anonymization","Image stitching","Privacy","Single-instance storage","Social network"],"vec":[1.2640926411,0.6930157225],"nodes":["1738297","1709589","1778687","1704011"]},"411b6492957d5367ef0df2ac926793fec0ca74ed":{"id":"411b6492957d5367ef0df2ac926793fec0ca74ed","date":"2003-01-01T12:12:00Z","text":"XML data integration tools are facing a variety of challenges for their efficient and effective operation. Among these is the requirement to handle a variety of inconsistencies or mistakes present in the data sets. In this paper we study the problem of integrating XML data sources through index assisted join operations, using notions of approximate match in the structure and content of XML documents as the join predicate. We show how a well known and widely deployed index structure, namely the R-tree, can be adopted to improve the performance of such operations. We propose novel search and join algorithms for R-trees adopted to index XML document collections. We also propose novel optimization objectives for R-tree construction, making R-trees better suited for this application.","category_a":"ICDE","category_b":"DB","keywords":["Algorithm","Join (SQL)","Program optimization","R* tree","R+ tree","R-tree","XML","XML database"],"vec":[1.0585752063,0.0876194569],"nodes":["1679363","1721062","1704011","1689202"]},"2f7ad26514bce4df6c8ebc42c90383ef3a974df4":{"id":"2f7ad26514bce4df6c8ebc42c90383ef3a974df4","date":"2013-01-01T12:12:00Z","text":"Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library\u2019s architecture, and a description of how the Pylearn2 community functions socially.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Application programming interface","Extensibility","Library","Machine learning"],"vec":[-0.3388597913,0.2537670226],"nodes":["34740554","1923596","3087941","3074927","1778734","1996134","32837403","3227028","1751762"]},"7a7a2658df5d66541305962d4c9d43078adadac6":{"id":"7a7a2658df5d66541305962d4c9d43078adadac6","date":"2013-01-01T12:12:00Z","text":"This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient matrix-vector product to avoid explicitly storing the natural gradient metric L. This metric is shown to be the expected second derivative of the log-partition function (under the model distribution), or equivalently, the covariance of the vector of partial derivatives of the energy function. We evaluate our method on the task of joint-training a 3-layer Deep Boltzmann Machine and show that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Boltzmann machine","Gradient","Hessian","Information geometry","Mathematical optimization","Matrix multiplication","Newton","Partition function (mathematics)"],"vec":[-0.3403652819,0.32114126],"nodes":["2755582","1996134","1760871","1751762"]},"161ba68b34bf2ec77384c9f1de04e9631d08af50":{"id":"161ba68b34bf2ec77384c9f1de04e9631d08af50","date":"2011-01-01T12:12:00Z","text":"Two learning algorithms were recently proposed \u2013 Herding and Fast Persistent Contrastive Divergence (FPCD) \u2013 which share the following interesting characteristic: they exploit changes in the model parameters while sampling in order to escape modes and mix better, during the sampling process that is part of the learning algorithm. We justify such approaches as ways to escape modes while approximately keeping the same asymptotic distribution of the Markov chain. In that spirit, we extend FPCD using an idea borrowed from Herding in order to obtain a pure sampling algorithm which we call the Rates-FPCD sampler. Interestingly, this sampler can improve the model as we collect more samples, since it optimizes a lower bound on the log-likelihood of the training data. We provide empirical evidence that this new algorithm displays substantially better and more robust mixing than Gibbs sampling.","category_a":"Neural Computation","category_b":"ML","keywords":["Algorithm","Gibbs sampling","Kullback\u2013Leibler divergence","Machine learning","Markov chain","Restricted Boltzmann machine","Sampling (signal processing)","Sampling in order","Test set"],"vec":[-0.2644144475,0.5780097474],"nodes":["1967465","1751762","1724875"]},"06d0a9697a0f0242dbdeeff08ec5266b74bfe457":{"id":"06d0a9697a0f0242dbdeeff08ec5266b74bfe457","date":"2002-01-01T12:12:00Z","text":"We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models. Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efficient, exact inference.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Generative model","Natural language","Parsing","Stochastic context-free grammar"],"vec":[-0.3990489908,-0.5388194345],"nodes":["38666915","1812612"]},"7584a7f5e9ace396f2d9725cd9d9072a27c1b5a6":{"id":"7584a7f5e9ace396f2d9725cd9d9072a27c1b5a6","date":"2009-01-01T12:12:00Z","text":"Due to the overwhelming flow of information in many data stream applications, data outsourcing is a natural and effective paradigm for individual businesses to address the issue of scale. In the standard data outsourcing model, the data owner outsources streaming data to one or more third-party servers, which answer queries posed by a potentially large number of clients on the data owner's behalf. Data outsourcing intrinsically raises issues of trust, making outsourced query assurance on data streams a problem with important practical implications. Existing solutions proposed in this model all build upon cryptographic primitives such as signatures and collision-resistant hash functions, which only work for certain types of queries, for example, simple selection\/aggregation queries.\n In this article, we consider another common type of queries, namely, &#8220;GROUP BY, SUM&#8221; queries, which previous techniques fail to support. Our new solutions are not based on cryptographic primitives, but instead use algebraic and probabilistic techniques to compute a small synopsis on the true query result, which is then communicated to the client so as to verify the correctness of the query result returned by the server. The synopsis uses a constant amount of space irrespective of the result size, has an extremely small probability of failure, and can be maintained using no extra space when the query result changes as elements stream by. We then generalize our synopsis to allow some tolerance on the number of erroneous groups, in order to support semantic load shedding on the server. When the number of erroneous groups is indeed tolerable, the synopsis can be strengthened so that we can locate and even correct these errors. Finally, we implement our techniques and perform an empirical evaluation using live network traffic.","category_a":"ACM Trans. Database Syst.","category_b":"Others","keywords":["Collision resistance","Correctness (computer science)","Cryptographic primitive","Cryptography","Hash function","Load Shedding","Network traffic control","Outsourcing","Programming paradigm","Server (computing)","Streaming media","Type signature","Video synopsis"],"vec":[0.9111367509,0.110936626],"nodes":["1683893","3245752","1709589","1836901","1715771","1704011"]},"683e5c3112c63aabaedafb7ba24f994baf07e0a4":{"id":"683e5c3112c63aabaedafb7ba24f994baf07e0a4","date":"2015-01-01T12:12:00Z","text":"The ability to accurately represent word vectors to capture syntactic and semantic similarity is central to Natural language processing. Thus, there is rising interest in vector space word embeddings and their use especially given recent methods for their fast estimation at very large scale. However almost all recent works assume a single representation for each word type, completely ignoring polysemy which eventually leads to errors.","category_a":"","category_b":"Others","keywords":["Microsoft Word for Mac","Natural language processing","Semantic similarity","Word embedding"],"vec":[-0.3545004394,-0.1648877127],"nodes":["38232028","12692625","39929781","2166511","1812612"]},"2d28108e25ba7b43f831210db064b1582d265503":{"id":"2d28108e25ba7b43f831210db064b1582d265503","date":"2003-01-01T12:12:00Z","text":"We present a simple, easily implemented spectral learning algorithm that applies equally whether we have no supervisory information, pairwise link constraints, or labeled examples. In the unsupervised case, it performs consistently with other spectral clustering algorithms. In the supervised case, our approach achieves high accuracy on the categorization of thousands of documents given only a few dozen labeled training documents for the 20 Newsgroups data set. Furthermore, its classification accuracy increases with the addition of unlabeled documents, demonstrating effective use of unlabeled data.","category_a":"CAI","category_b":"AI","keywords":["Algorithm","Categorization","Cluster analysis","Machine learning","Spectral clustering","Unsupervised learning"],"vec":[-0.1322788569,-0.1555687475],"nodes":["2833700","38666915","1812612"]},"67bee729d046662c6ebd9d3d695823c9d820343a":{"id":"67bee729d046662c6ebd9d3d695823c9d820343a","date":"2016-01-01T12:12:00Z","text":"Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale questionanswer corpora available. In this paper we present the 30M Factoid QuestionAnswer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the questiongeneration model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear to be indistinguishable from real human-generated questions.","category_a":"ACL","category_b":"ComLing","keywords":["Artificial neural network","Baseline (configuration management)","Evaluation of machine translation","Freebase","Knowledge base","Machine learning","Machine translation","Natural language","Network architecture","Neural Networks","Question answering","Recurrent neural network","Supervised learning","Text corpus"],"vec":[-0.7125168112,-0.1411699608],"nodes":["35224828","2309743","1854385","3103594","2251963","1760871","1751762"]},"07c43a3ff15f2104022f2b1ca8ec4128a930b414":{"id":"07c43a3ff15f2104022f2b1ca8ec4128a930b414","date":"2012-01-01T12:12:00Z","text":"We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.","category_a":"ICML","category_b":"ML","keywords":["Artificial neural network","Language model","Medical transcription","Recurrent neural network","Statistical model"],"vec":[-0.3512131341,0.2908046261],"nodes":["2488222","1751762","1724875"]},"0e48b9fc023866b55219b26ec40ad88633020d2e":{"id":"0e48b9fc023866b55219b26ec40ad88633020d2e","date":"2001-01-01T12:12:00Z","text":"Estimating insurance premia from data is a difficult regression problem for several reasons: the large number of variables, many of which are .discrete, and the very peculiar shape of the noise distribution, asymmetric with fat tails, with a large majority zeros and a few unreliable and very large values. We compare several machine learning methods for estimating insurance premia, and test them on a large data base of car insurance policies. We find that function approximation methods that do not optimize a squared loss, like Support Vector Machines regression, do not work well in this context. Compared methods include decision trees and generalized linear models. The best results are obtained with a mixture of experts, which better identifies the least and most risky contracts, and allows to reduce the median premium by charging more to the most risky customers.","category_a":"NIPS","category_b":"ML","keywords":["Database","Decision tree","Generalized linear model","Linear model","Machine learning","Mean squared error","Support vector machine","Tails","Value (ethics)"],"vec":[-0.3690599814,0.3371242647],"nodes":["2748188","1751762","1724875","2864754","2701388","3046458","34199641"]},"0e6c65b8c366d6f925b8221e01aceddbf931ef19":{"id":"0e6c65b8c366d6f925b8221e01aceddbf931ef19","date":"2003-01-01T12:12:00Z","text":"XML has become ubiquitous, and XML data has to be managed in databases. The current industry standard is to map XML data into relational tables and store this information in a relational database. Such mappings create both expressive power problems and performance problems.In the T<sc>IMBER<\/sc> [7] project we are exploring the issues involved in storing XML in native format. We believe that the key intellectual contribution of this system is a comprehensive set-at-a-time query processing ability in a native XML store, with all the standard components of relational query processing, including algebraic rewriting and a cost-based optimizer.","category_a":"SIGMOD","category_b":"DB","keywords":["Database","Expressive power (computer science)","F-algebra","Relational database","Rewriting","Technical standard","XML"],"vec":[0.8672509598,0.0059521377],"nodes":["2822695","1978097","1801163","1735239","1708593","2183125","2042232","1704011","3301260","1683593","1746900"]},"81b3b3fe994a9eda6d3f9d2149aa4492d1933975":{"id":"81b3b3fe994a9eda6d3f9d2149aa4492d1933975","date":"2010-01-01T12:12:00Z","text":"Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases only partly address the problem at the cost of huge feature spaces and sparseness. To address this, we introduce a recursive neural network architecture for jointly parsing natural language and learning vector space representations for variable-sized inputs. At the core of our architecture are context-sensitive recursive neural networks (CRNN). These networks can induce distributed feature representations for unseen phrases and provide syntactic information to accurately predict phrase structure trees. Most excitingly, the representation of each phrase also captures semantic information: For instance, the phrases \u201cdecline to comment\u201d and \u201cwould not disclose the terms\u201d are close by in the induced embedding space. Our current system achieves an unlabeled bracketing F-measure of 92.1% on the Wall Street Journal dataset for sentences up to length 15.","category_a":"","category_b":"Others","keywords":["Artificial neural network","Biological Neural Networks","Categories","Etoposide","Linguistics","Natural language","Network architecture","Neural Network Simulation","Neural Tube Defects","Neural coding","Parsing","Phrase structure rules","Phrases","Recursion","Recursive neural network","The Wall Street Journal","Trees (plant)","sentence"],"vec":[-0.5406132118,-0.4789232866],"nodes":["2166511","1812612","1701538"]},"5075636b9e0425358204211706adde1ecb5ba60b":{"id":"5075636b9e0425358204211706adde1ecb5ba60b","date":"2005-01-01T12:12:00Z","text":"Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Artificial neural network","Convex optimization","Layer (electronics)","Linear classifier","Machine learning","Neural Networks","Optimization problem","Program optimization","Weight function"],"vec":[-0.3647248733,0.3931187561],"nodes":["1751762","7245737","1724875","2460212","1755928"]},"be15028db5dc7f50cbca96d9e3d264802f385873":{"id":"be15028db5dc7f50cbca96d9e3d264802f385873","date":"2006-01-01T12:12:00Z","text":"Data quality is a serious concern in every data management application, and a variety of quality measures have been proposed, e.g., accuracy, freshness and completeness, to capture common sources of data quality degradation. We identify and focus attention on a novel measure, column heterogeneity, that seeks to quantify the data quality problems that can arise when merging data from different sources. We identify desiderata that a column heterogeneity measure should intuitively satisfy, and describe our technique to quantify database column heterogeneity based on using a novel combination of cluster entropy and soft clustering. Finally, we present detailed experimental results, using diverse data sets of different types, to demonstrate that our approach provides a robust mechanism for identifying and quantifying database column heterogeneity.","category_a":"ICDM","category_b":"Others","keywords":["Cluster analysis","Data quality","Database","Replay attack"],"vec":[0.9982051543,0.105754279],"nodes":["21526180","1721062","1693070","1704011","1747652"]},"17a40c553bdef82ccfc1bcc978e8e9d23230bfe5":{"id":"17a40c553bdef82ccfc1bcc978e8e9d23230bfe5","date":"2012-01-01T12:12:00Z","text":"Machine learning is everywhere in today\u2019s NLP, but by and large machine learning amounts to numerical optimization of weights for human designed representations and features. The goal of deep learning is to explore how computers can take advantage of data to develop features and representations appropriate for complex interpretation tasks. This tutorial aims to cover the basic motivation, ideas, models and learning algorithms in deep learning for natural language processing. Recently, these methods have been shown to perform very well on various NLP tasks such as language modeling, POS tagging, named entity recognition, sentiment analysis and paraphrase detection, among others. The most attractive quality of these techniques is that they can perform well without any external hand-designed resources or time-intensive feature engineering. Despite these advantages, many researchers in NLP are not familiar with these methods. Our focus is on insight and understanding, using graphical illustrations and simple, intuitive derivations. The goal of the tutorial is to make the inner workings of these techniques transparent, intuitive and their results interpretable, rather than black boxes labeled \u201dmagic here\u201d. The first part of the tutorial presents the basics of neural networks, neural word vectors, several simple models based on local windows and the math and algorithms of training via backpropagation. In this section applications include language modeling and POS tagging. In the second section we present recursive neural networks which can learn structured tree outputs as well as vector representations for phrases and sentences. We cover both equations as well as applications. We show how training can be achieved by a modified version of the backpropagation algorithm introduced before. These modifications allow the algorithm to work on tree structures. Applications include sentiment analysis and paraphrase detection. We also draw connections to recent work in semantic compositionality in vector spaces. The principle goal, again, is to make these methods appear intuitive and interpretable rather than mathematically confusing. By this point in the tutorial, the audience members should have a clear understanding of how to build a deep learning system for word-, sentenceand document-level tasks. The last part of the tutorial gives a general overview of the different applications of deep learning in NLP, including bag of words models. We will provide a discussion of NLP-oriented issues in modeling, interpretation, representational power, and optimization.","category_a":"HLT-NAACL","category_b":"ComLing","keywords":["Algorithm","Artificial neural network","Backpropagation","Bag-of-words model","Black box","Computer","Deep learning","Feature engineering","Interpretation (logic)","Language model","Machine learning","Mathematical optimization","Microsoft Windows","Named-entity recognition","Natural language processing","Numerical analysis","Part-of-speech tagging","Point of sale","Program optimization","Recursion","Recursive neural network","Sentiment analysis","Tree structure","Word embedding"],"vec":[-0.4449937418,-0.3361721606],"nodes":["2166511","1812612"]},"098abe1eee6208b693daafe7183f017f9e71e409":{"id":"098abe1eee6208b693daafe7183f017f9e71e409","date":"2009-01-01T12:12:00Z","text":"Given an image, we propose a hierarchical generative model that classifies the overall scene, recognizes and segments each object component, as well as annotates the image with a list of tags. To our knowledge, this is the first model that performs all three tasks in one coherent framework. For instance, a scene of a `polo game' consists of several visual objects such as `human', `horse', `grass', etc. In addition, it can be further annotated with a list of more abstract (e.g. `dusk') or visually less salient (e.g. `saddle') tags. Our generative model jointly explains images through a visual model and a textual model. Visually relevant objects are represented by regions and patches, while visually irrelevant textual annotations are influenced directly by the overall scene class. We propose a fully automatic learning framework that is able to learn robust scene models from noisy Web data such as images and user tags from Flickr.com. We demonstrate the effectiveness of our framework by automatically classifying, annotating and segmenting images from eight classes depicting sport scenes. In all three tasks, our model significantly outperforms state-of-the-art algorithms.","category_a":"CVPR","category_b":"ComVis","keywords":["Algorithm","Coherence (physics)","Flickr","Generative model","Image segmentation","Patch (computing)","Relevance","Visual Objects","Visual modeling"],"vec":[-0.2690071712,-0.5339791122],"nodes":["33642044","2166511","3216322"]},"73f31354cc9058ddc2e47a1c585b753e1592c1bf":{"id":"73f31354cc9058ddc2e47a1c585b753e1592c1bf","date":"2015-01-01T12:12:00Z","text":"This paper presents BigDAWG, a reference implementation of a new architecture for \u201cBig Data\u201d applications. Such applications not only call for large-scale analytics, but also for real-time streaming support, smaller analytics at interactive speeds, data visualization, and cross-storage-system queries. Guided by the principle that \u201cone size does not fit all\u201d, we build on top of a variety of storage engines, each designed for a specialized use case. To illustrate the promise of this approach, we demonstrate its effectiveness on a hospital application using data from an intensive care unit (ICU). This complex application serves the needs of doctors and researchers and provides real-time support for streams of patient data. It showcases novel approaches for querying across multiple storage engines, data visualization, and scalable real-time analytics.","category_a":"VLDB","category_b":"DB","keywords":["Big data","Data visualization","Database engine","International Components for Unicode","Reference implementation","Scalability"],"vec":[0.6170044323,-0.3751028258],"nodes":["1787375","38449161","1695715","1718134","2109957","1867298","1803140","1686294","3257323","1746961","2033016","1740962","2682150","2115757","33282330","1773620","3039341","2031287"]},"d2ac9f33f16b421c3c5f38fddb8cdd8d30d7f886":{"id":"d2ac9f33f16b421c3c5f38fddb8cdd8d30d7f886","date":"2001-01-01T12:12:00Z","text":"While methods for parsing probabilistic context-free grammars (PCFGs) are well known, a tabular parsing framework for arbitrary PCFGs which allows for botton-up, topdown, and other parsing strategies, has not yet been provided. This paper presents such an algorithm, and shows its correctness and advantages over prior work. The paper finishes by bringing out the connections between the algorithm and work on hypergraphs, which permits us to extend the presented Viterbi (best parse) algorithm to an inside (total probability) algorithm.","category_a":"","category_b":"Others","keywords":["Algorithm","Chart parser","Context-free grammar","Context-free language","Correctness (computer science)","License","Parser","Parsing","Stochastic context-free grammar","Table (information)","algorithm"],"vec":[0.0999265946,-1.1431385624],"nodes":["38666915","1812612"]},"855d0f722d75cc56a66a00ede18ace96bafee6bd":{"id":"855d0f722d75cc56a66a00ede18ace96bafee6bd","date":"2012-01-01T12:12:00Z","text":"Theano is a linear algebra compiler that optimizes a user\u2019s symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano\u2019s performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","C++","Compiler","Computation","High- and low-level","Linear algebra","Machine learning","Recurrent neural network","Theano (software)"],"vec":[-0.3280485537,0.2351352455],"nodes":["3227028","3087941","1996134","32837403","34740554","40128194","14362225","1923596","1751762"]},"1e3d36a9184feec956371fe3fb03eb34747e4844":{"id":"1e3d36a9184feec956371fe3fb03eb34747e4844","date":"2010-01-01T12:12:00Z","text":"Robust object detection is a critical skill for robotic applications in complex environments like homes and offices. In this paper we propose a method for using multiple cameras to simultaneously view an object from multiple angles and at high resolutions. We show that our probabilistic method for combining the camera views, which can be used with many choices of single-image object detector, can significantly improve accuracy for detecting objects from many viewpoints. We also present our own single-image object detection method that uses large synthetic datasets for training. Using a distributed, parallel learning algorithm, we train from very large datasets (up to 100 million image patches). The resulting object detector achieves high performance on its own, but also benefits substantially from using multiple camera views. Our experimental results validate our system in realistic conditions and demonstrates significant performance gains over using standard single-image classifiers, raising accuracy from 0.86 area-under-curve to 0.97.","category_a":"Robotics and Automation","category_b":"AI","keywords":["Algorithm","Autostereogram","Object detection","Robot","Robotics","Synthetic data"],"vec":[-0.3608092927,-0.118526503],"nodes":["5574038","1701538"]},"180920cd652b27cd9207fb797107b19d46e1100a":{"id":"180920cd652b27cd9207fb797107b19d46e1100a","date":"2010-01-01T12:12:00Z","text":"","category_a":"Encyclopedia of Machine Learning","category_b":"Others","keywords":["Autonomous car","Reinforcement learning"],"vec":[-0.1977254589,-0.005169326],"nodes":["5574038","1689992","1701538"]},"0b4d3e59a0107f0dad22e74054bab1cf1ad9c32e":{"id":"0b4d3e59a0107f0dad22e74054bab1cf1ad9c32e","date":"2016-01-01T12:12:00Z","text":"Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \u201cWhat vehicle is the person riding?\u201d, computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) to answer correctly that \u201cthe person is riding a horse-drawn carriage.\u201d In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 108K images where each image has an average of $$35$$ 35 objects, $$26$$ 26 attributes, and $$21$$ 21 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answer pairs.","category_a":"Journal of Computer Vision","category_b":"Others","keywords":["Cognition","Computer","Computer vision","Crowdsourcing","Interaction","Question answering","Synonym ring","Whole genome sequencing","WordNet"],"vec":[-0.3975955648,-0.3816101033],"nodes":["2580593","2117748","2152186","2143297","35163655","40591424","3371797","1944225","33642044","1760364","35609041","3216322"]},"25241d105a39e9933c03298579248fc0874352d5":{"id":"25241d105a39e9933c03298579248fc0874352d5","date":"2001-01-01T12:12:00Z","text":"We describe efficient algorithms for accurately estimating the number of matches of a small node-labeled tree, i.e., a twig, in a large node-labeled tree, using a summary data structure. This problem is of interest for queries on XML and other hierarchical data, to provide query feedback and for costbased query optimization. Our summary data structure scalably representsapproximate frequency information about twiglets (i.e., small twigs) in the data tree. Given a twig query, the number of matches is estimated by creating a set of query twiglets, and combining two complementary approaches: Set Hashing, used to estimate the number of matches of each query twiglet, and Maximal Overlap, used to combine the query twiglet estimates into an estimate for the twig query. We propose several estimation algorithms that apply these approaches on query twiglets formed using variations on different twiglet decomposition techniques. We present an extensive experimental evaluation using several real XML data sets, with a variety of twig queries. Our results demonstrate that accurate and robust estimates can be achieved, even with limited space.","category_a":"ICDE","category_b":"DB","keywords":["Algorithm","Data structure","Hierarchical database model","Overlap\u2013add method","Program optimization","Query optimization","Twig","Twig (database)","XML"],"vec":[1.0311476774,0.1439277312],"nodes":["39507820","1735239","2096611","1721062","1711192","1736021","1704011"]},"1e6f61c574fad2460d06d6f52295922f5c99857a":{"id":"1e6f61c574fad2460d06d6f52295922f5c99857a","date":"2002-01-01T12:12:00Z","text":"We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published unsupervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on nontrivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task.","category_a":"ACL","category_b":"ComLing","keywords":["Automatic Transmitter Identification System (television)","Distributional semantics","Grammar induction","Natural language","Parsing","Part-of-speech tagging","Treebank"],"vec":[-0.4736913025,-0.7575620825],"nodes":["38666915","1812612"]},"2337ff38e6cfb09e28c0958f07e2090c993ef6e8":{"id":"2337ff38e6cfb09e28c0958f07e2090c993ef6e8","date":"2009-01-01T12:12:00Z","text":"For many pattern recognition tasks, the ideal input feature would be invariant to multiple confounding properties (such as illumination and viewing angle, in computer vision applications). Recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, it is difficult to evaluate the learned features by any means other than using them in a classifier. In this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different input transformations. We find that stacked autoencoders learn modestly increasingly invariant features with depth when trained on natural images. We find that convolutional deep belief networks learn substantially more invariant features in each layer. These results further justify the use of \u201cdeep\u201d vs. \u201cshallower\u201d representations, but suggest that mechanisms beyond merely stacking one autoencoder on top of another may be important for achieving invariance. Our evaluation metrics can also be used to evaluate future work in deep learning, and thus help the development of future algorithms.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Autoencoder","Bayesian network","Computer vision","Convolutional Deep Belief Networks","Deep belief network","Deep learning","Focus stacking","Pattern recognition","Statistical classification","Unsupervised learning"],"vec":[-0.4110506951,0.2634036458],"nodes":["34740554","2827616","34927843","1697141","1701538"]},"296bbeb2c43b183fb7d93c8a309b2c576f8b5fc7":{"id":"296bbeb2c43b183fb7d93c8a309b2c576f8b5fc7","date":"2006-01-01T12:12:00Z","text":"We claim and present arguments to the effect that a large class of manifold learning algorithms that are essentially local and can be framed as kernel learning algorithms will suffer from the curse of dimensionality, at the dimension of the true underlying manifold. This observation invites an exploration of nonlocal manifold learning algorithms that attempt to discover shared structure in the tangent planes at different positions. A training criterion for such an algorithm is proposed, and experiments estimating a tangent plane prediction function are presented, showing its advantages with respect to local manifold learning algorithms: it is able to generalize very far from training data (on learning handwritten character image rotations), where local nonparametric methods fail.","category_a":"Neural Computation","category_b":"ML","keywords":["Algorithm","Curse of dimensionality","Eisenstein's criterion","Experiment","Machine learning","Nonlinear dimensionality reduction","Test set","algorithm","manifold"],"vec":[-0.4433631161,0.5335897395],"nodes":["1751762","1690311","1777528"]},"70325f4d1f7502bd06fa54484535b36c8b0c1290":{"id":"70325f4d1f7502bd06fa54484535b36c8b0c1290","date":"2013-01-01T12:12:00Z","text":"A central problem in releasing aggregate information about sensitive data is to do so accurately while providing a privacy guarantee on the output. Recent work focuses on the class of linear queries, which include basic counting queries, data cubes, and contingency tables. The goal is to maximize the utility of their output, while giving a rigorous privacy guarantee. Most results follow a common template: pick a &#x201C;strategy&#x201D; set of linear queries to apply to the data, then use the noisy answers to these queries to reconstruct the queries of interest. This entails either picking a strategy set that is hoped to be good for the queries, or performing a costly search over the space of all possible strategies. In this paper, we propose a new approach that balances accuracy and efficiency: we show how to improve the accuracy of a given query set by answering some strategy queries more accurately than others. This leads to an efficient optimal noise allocation for many popular strategies, including wavelets, hierarchies, Fourier coefficients and more. For the important case of marginal queries we show that this strictly improves on previous methods, both analytically and empirically. Our results also extend to ensuring that the returned query answers are consistent with an (unknown) data set at minimal extra cost in terms of time and noise.","category_a":"ICDE","category_b":"DB","keywords":["3D lookup table","Coefficient","Contingency table","Strategic management","Wavelet"],"vec":[0.9582591722,0.1930301315],"nodes":["1757584","1709589","1738196","1704011"]},"908f7db1b21067ee9f9442bccce4787d9ad5634f":{"id":"908f7db1b21067ee9f9442bccce4787d9ad5634f","date":"2003-01-01T12:12:00Z","text":"1. MOTIVATION A large number of useful databases are currently accessible over the Web and within corporate networks. In addition to being frequently updated, this collection of databases tends to be highly dynamic: new databases appear often, and databases (just like Web sites) also disappear. In this environment, the goal of providing flexible, timely and declarative query access over all these databases remains elusive.","category_a":"SIGMOD","category_b":"DB","keywords":["Database","WEB","World Wide Web"],"vec":[0.3907250828,-0.0806359215],"nodes":["1721062","1704011"]},"16c7d4559b977d10b2a4d73fc3a9a972367e6f11":{"id":"16c7d4559b977d10b2a4d73fc3a9a972367e6f11","date":"2013-01-01T12:12:00Z","text":"We present Positive Diversity Tuning, a newmethod for tuningmachine translation models specifically for improved performance during system combination. System combination gains are often limited by the fact that the translations produced by the different component systems are too similar to each other. We propose a method for reducing excess cross-system similarity by optimizing a joint objective that simultaneously rewards models for producing translations that are similar to reference translations, while also punishing them for translations that are too similar to those produced by other systems. The formulation of the Positive Diversity objective is easy to implement and allows for its quick integration with most machine translation tuning pipelines. We find that individual systems tuned on the same data to Positive Diversity can be even more diverse than systems built using different data sets, while still obtaining good BLEU scores. When these individual systems are used together for system combination, our approach allows for significant gains of 0.8 BLEU even when the combination is performed using a small number of otherwise identical individual systems.","category_a":"ACL","category_b":"ComLing","keywords":["BLEU","Cross-validation (statistics)","Machine translation","Pipeline (computing)"],"vec":[-0.7911304431,-0.3372188218],"nodes":["3208422","1812612","1746807"]},"272665e79d39862fb014377f01b31fd3f91d9fb8":{"id":"272665e79d39862fb014377f01b31fd3f91d9fb8","date":"2017-01-01T12:12:00Z","text":"It has been postulated that a good representation is one that disentangles the underlying explanatory factors of variation. However, it remains an open question what kind of training framework could potentially achieve that. Whereas most previous work focuses on the static setting (e.g., with images), we postulate that some of the causal factors could be discovered if the learner is allowed to interact with its environment. The agent can experiment with different actions and observe their effects. More specifically, we hypothesize that some of these factors correspond to aspects of the environment which are independently controllable, i.e., that there exists a policy and a learnable feature for each such aspect of the environment, such that this policy can yield changes in that feature with minimal changes to other features that explain the statistical variations in the observed data. We propose a specific objective function to find such factors, and verify experimentally that it can indeed disentangle independently controllable aspects of the environment without any extrinsic reward signal.","category_a":"ArXiv","category_b":"Journal","keywords":["Causality","Experiment","Loss function","Optimization problem"],"vec":[-0.2414221732,0.2968235664],"nodes":["27281907","2416433","26958176","24026338","2895922","1777528","1723772","1724729","1751762"]},"5394ab5bf4b66bfb52f111525d6141a3226ba883":{"id":"5394ab5bf4b66bfb52f111525d6141a3226ba883","date":"2007-01-01T12:12:00Z","text":"This document contains a list of open problems and research directions that have been suggested by participants at the IITK Workshop on Algorithms for Data Streams. Many of the questions were discussed at the workshop or were posed during presentations. Further details, including videos of discussion sections, can be found at http:\/\/www.cse.iitk.ac.in\/users\/sganguly\/workshop.html . Please send any comments\/corrections regarding this document to andrewm@ucsd.edu. Workshop Speakers. Pankaj Agarwal Surender Baswana Amit Chakarabarti Graham Cormode Sudipto Guha Piotr Indyk T. S. Jayram Ravi Kannan Sampath Kannan Ravi Kumar Stefano Leonardi Yossi Matias Michael Mahoney Andrew McGregor S. Muthukrishnan Rajeev Raman Nicole Schweikardt D. Sivakumar Christian Sohler Divesh Srivastava Martin Strauss Subhash Suri Srikanta Tirthapura","category_a":"","category_b":"Others","keywords":["Blochmannia endosymbiont of Camponotus (Colobopsis) leonardi","Raman scattering","Subhash Suri","Yossi Matias","algorithm"],"vec":[-0.0824529163,-0.1761608158],"nodes":[0,"39583947","1748594",0,"1709589","1679363","1688317","2545727","1705496","1693864","40657248","1695327","1745572","33736078","40422605","1711192","1685583","1691736","1765251","1701715","1704011","40532808","1688025","1679836"]},"bd4ddde7f9af802805fac652b44146698f0bda6e":{"id":"bd4ddde7f9af802805fac652b44146698f0bda6e","date":"2010-01-01T12:12:00Z","text":"The distance-dependent Chinese Restaurant Process (dd-CRP) is a flexible class of distributions over partitions which was recently introduced by [1, 2]. In their description and experiments Blei and Frazier focus on the sequential setting such as clustering over time. Their Gibbs sampler, while general in nature, does not explicitly handle the case of non-sequential (also called spatial) clustering. In this case further details are needed for a correct implementation. We introduce the Gibbs sampler for spatial clustering with the dd-CRP. For simplicity, we focus on infinite Gaussian mixture models (IGMM) [9].","category_a":"","category_b":"Others","keywords":["Cluster analysis","Experiment","Gaussian elimination","Gibbs sampling","Mixture model","Sampling (signal processing)","statistical cluster"],"vec":[0.1516567955,0.3926327839],"nodes":["2166511","1812612"]},"266228767a0d4aed021754b17947c926f4f8881b":{"id":"266228767a0d4aed021754b17947c926f4f8881b","date":"2016-01-01T12:12:00Z","text":"With thousands of data sources spread across multiple databases and data lakes, modern organizations face a <i>data discovery<\/i> challenge. Analysts spend more time finding relevant data to answer the questions at hand than analyzing it.\n In this paper we introduce a data discovery system that facilitates locating relevant data among thousands of data sources. We represent data sources succinctly through <i>signatures<\/i>, and then create <i>search paths<\/i> that permit quick execution of a set of data discovery primitives used for finding relevant data. We have built a prototype that is being used to solve data discovery challenges of two big organizations.","category_a":"SIGMOD","category_b":"DB","keywords":["Database","Discovery system"],"vec":[0.5798696188,-0.4551486935],"nodes":["34568734","2034349","2033016","1695715"]},"534f6ea4ce0127e5da7f1cafb6334b59ad15b83f":{"id":"534f6ea4ce0127e5da7f1cafb6334b59ad15b83f","date":"2007-01-01T12:12:00Z","text":"Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Artificial neural network","Experiment","Feedforward neural network","Machine learning","Support vector machine"],"vec":[-0.3335556725,0.3242907672],"nodes":["1777528","1761978","1760871","32837403","1751762"]},"03cb609fcfce6c60cbe3eb0dd8254069bf6d7573":{"id":"03cb609fcfce6c60cbe3eb0dd8254069bf6d7573","date":"2006-01-01T12:12:00Z","text":"Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Artificial neural network","Bayesian network","Causality","Deep belief network","Experiment","Generative model","Gradient","Greedy algorithm","High- and low-level","Initialization (programming)","Layer (electronics)","Maxima and minima","Nonlinear system","Optimization problem","Program optimization","Unsupervised learning"],"vec":[-0.3865848575,0.6500022506],"nodes":["1751762","3087941","32384143","1777528"]},"3df87d901a0e3de3bd6a0c17c5683d753fbeaf64":{"id":"3df87d901a0e3de3bd6a0c17c5683d753fbeaf64","date":"2006-01-01T12:12:00Z","text":"Learning distributed representations of symbols (e.g. words) has been used in several Natural Language Processing systems. Such representations can capture semantic or syntactic similarities between words, which permit to fight the curse of dimensionality when considering sequences of such words. Unfortunately, because these representations are learned only for a previously determined vocabulary of words, it is not clear how to obtain representations for new words. We present here an approach which gets around this problem by considering the distributed representations as predictions from low-level or domain-knowledge features of words. We report experiments on a Part Of Speech tagging task, which demonstrates the success of this approach in learning meaningful representations and in providing improved accuracy, especially for new words.","category_a":"","category_b":"Others","keywords":["Artificial neural network","Curse of dimensionality","Experiment","Generalization (Psychology)","High- and low-level","Natural language","Natural language processing","Oscillator representation","Part-of-speech tagging","Protologism","Vocabulary","Vocabulary"],"vec":[-0.6184876118,-0.3829705269],"nodes":["1777528","1751762"]},"7797d9e0e444246ba61d7e16cbbd654b07c117e8":{"id":"7797d9e0e444246ba61d7e16cbbd654b07c117e8","date":"2017-01-01T12:12:00Z","text":"Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive\/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.","category_a":"","category_b":"Others","keywords":["Deep learning","Natural language processing","Parsing","Phrases","Robotics","Robotics","Sentiment analysis","Treebank","Trees (plant)","sentence"],"vec":[-0.6285205649,-0.745815448],"nodes":["7736730","38666915","2166511","34201153","40383658"]},"0add4da8389bd54958221def3ea16575ee9a007f":{"id":"0add4da8389bd54958221def3ea16575ee9a007f","date":"2013-01-01T12:12:00Z","text":"We investigate the problem of transforming an input sequence into a high-dimensional output sequence in order to transcribe polyphonic audio music into symbolic notation. We introduce a probabilistic model based on a recurrent neural network that is able to learn realistic output distributions given the input and we devise an efficient algorithm to search for the global mode of that distribution. The resulting method produces musically plausible transcriptions even under high levels of noise and drastically outperforms previous state-of- the-art approaches on five datasets of synthesized sounds and real recordings, approximately halving the test error rate.","category_a":"Acoustics, Speech and Signal Processing","category_b":"Others","keywords":["Algorithm","Artificial neural network","Division by two","Recurrent neural network","Statistical model","Transduction (machine learning)"],"vec":[-0.3348585642,0.2975341449],"nodes":["2488222","1751762","1724875"]},"61845763cd9c8865870b17f6efa034e8fa862978":{"id":"61845763cd9c8865870b17f6efa034e8fa862978","date":"2003-01-01T12:12:00Z","text":"","category_a":"ICDE","category_b":"DB","keywords":["Regular expression","XML"],"vec":[-0.1925470263,-0.0221078554],"nodes":["1736846","1721062","1704011"]},"bfe947a04e9716fb6a581d9609f448aa5daa50f1":{"id":"bfe947a04e9716fb6a581d9609f448aa5daa50f1","date":"2010-01-01T12:12:00Z","text":"We show that Viterbi (or \u201chard\u201d) EM is well-suited to unsupervised grammar induction. It is more accurate than standard inside-outside re-estimation (classic EM), significantly faster, and simpler. Our experiments with Klein and Manning\u2019s Dependency Model with Valence (DMV) attain state-of-the-art performance \u2014 44.8% accuracy on Section 23 (all sentences) of the Wall Street Journal corpus \u2014 without clever initialization; with a good initializer, Viterbi training improves to 47.9%. This generalizes to the Brown corpus, our held-out set, where accuracy reaches 50.8% \u2014 a 7.5% gain over previous best results. We find that classic EM learns better from short sentences but cannot cope with longer ones, where Viterbi thrives. However, we explain that both algorithms optimize the wrong objectives and prove that there are fundamental disconnects between the likelihoods of sentences, best parses, and true parses, beyond the wellestablished discrepancies between likelihood, accuracy and extrinsic performance.","category_a":"CoNLL","category_b":"Others","keywords":["Algorithm","Brown Corpus","Experiment","Grammar induction","Initialization (programming)","Parsing","The Wall Street Journal","Viterbi algorithm"],"vec":[-0.5221377587,-0.4155142788],"nodes":["2556884","1767307","1746807","1812612"]},"0d60fce590359780f62017e472fd4523a5de3f5f":{"id":"0d60fce590359780f62017e472fd4523a5de3f5f","date":"2010-01-01T12:12:00Z","text":"Recent work on anonymizing online social networks (OSNs) has looked at privacy preserving techniques for publishing a single instance of the network. However, OSNs evolve and a single instance is inadequate for analyzing their evolution or performing longitudinal data analysis. We study the problem of repeatedly publishing OSN data as the network evolves while preserving privacy of users. Publishing multiple instances independently has privacy risks, since stitching the information together may allow an adversary to identify users. We provide methods to anonymize a dynamic network when new nodes and edges are added to the published network. These methods use link prediction algorithms to model the evolution. Using this predicted graph to perform group-based anonymization, the loss in privacy caused by new edges can be eliminated almost entirely. We propose metrics for privacy loss, and evaluate them for publishing multiple OSN instances.","category_a":"WOSN","category_b":"Others","keywords":["Adversary (cryptography)","Algorithm","Anonymous web browsing","Data anonymization","Image stitching","Privacy","Single-instance storage","Social Networks","Social network"],"vec":[1.2618743716,0.6907240146],"nodes":["1738297","1709589","1704011"]},"710822be03043849bc016acc987695a3b8b34ecf":{"id":"710822be03043849bc016acc987695a3b8b34ecf","date":"2014-01-01T12:12:00Z","text":"Distant supervision is a successful paradigm that gathers training data for information extraction systems by automatically aligning vast databases of facts with text. Previous work has demonstrated its usefulness for the extraction of binary relations such as a person\u2019s employer or a film\u2019s director. Here, we extend the distant supervision approach to template-based event extraction, focusing on the extraction of passenger counts, aircraft types, and other facts concerning airplane crash events. We present a new publicly available dataset and event extraction task in the plane crash domain based on Wikipedia infoboxes and newswire text. Using this dataset, we conduct a preliminary evaluation of four distantly supervised extraction models which assign named entity mentions in text to entries in the event template. Our results indicate that joint inference over sequences of candidate entity mentions is beneficial. Furthermore, we demonstrate that the SEARN algorithm outperforms a linear-chain CRF and strong baselines with local inference.","category_a":"LREC","category_b":"Others","keywords":["Algorithm","Conditional random field","Crostata","Database","Information extraction","Named entity","Programming paradigm","Test set","Utility","Wikipedia"],"vec":[-0.4849172378,-0.3034189129],"nodes":["1772645","2528953","1760868","1812612","1746807"]},"72dbacd0dbd157d881d284e922e21fcc5a2a4eeb":{"id":"72dbacd0dbd157d881d284e922e21fcc5a2a4eeb","date":"2011-01-01T12:12:00Z","text":"A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Specifically, we will apply several offthe-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR, NORB, and STL datasets using only singlelayer networks. We then present a detailed analysis of the effect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (\u201cstride\u201d) between extracted features, and the effect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance\u2014so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyperparameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively). Appearing in Proceedings of the 14 International Conference on Artificial Intelligence and Statistics (AISTATS) 2011, Fort Lauderdale, FL, USA. Volume 15 of JMLR: W&CP 15. Copyright 2011 by the authors.","category_a":"AISTATS","category_b":"ML","keywords":["Algorithm","Artificial intelligence","Benchmark (computing)","Cluster analysis","Encoder","Feature extraction","Feature learning","Gaussian (software)","Journal of Machine Learning Research","K-means clustering","Machine learning","STL (file format)","Sparse matrix","Unsupervised learning","Whitening transformation"],"vec":[-0.4260332885,0.3325470616],"nodes":["5574038","1701538","1697141"]},"207def18c67fa8024741b7ae3cdc655b57f2053f":{"id":"207def18c67fa8024741b7ae3cdc655b57f2053f","date":"2005-01-01T12:12:00Z","text":"\u2022 C-Store: A Column-oriented DBMS* Mike Stonebraker, Daniel J. Abadi, Adam Batkin, Xuedong Chen, Mitch Cherniack, Miguel Ferreira, Edmond Lau, Amerson Lin, Sam Madden, Elizabeth O\u2019Neil, Pat O\u2019Neil, Alex Rasin, Nga Tran, Stan Zdonik. Appears in Proceedings of the ACM Conference on Very Large Databases(VLDB), 2005 \u2022 Database Cracking+ Stratos Idreos, Martin L. Kersten, and Stefan Manegold. Appears in the 3rd Biennial Conference on Innovative Data Systems Research (CIDR), January 7-10, 2007","category_a":"VLDB","category_b":"DB","keywords":["C-Store","CIDR","Column-oriented DBMS","Database","Local Interconnect Network","Stan","Stan Franklin"],"vec":[0.1127529143,-0.2097248926],"nodes":["1695715","2254232","2640233","1940326","31987589","1993314","40626880","38375600","2033016","2582830","34661814","6850164","38704448","2031287"]},"5578cabaef7b5dfc88443626e74d2e04951818f7":{"id":"5578cabaef7b5dfc88443626e74d2e04951818f7","date":"2005-01-01T12:12:00Z","text":"We present a replication-based approach to fault-tolerant distributed stream processing in the face of node failures, network failures, and network partitions. Our approach aims to reduce the degree of inconsistency in the system while guaranteeing that available inputs capable of being processed are processed within a specified time threshold. This threshold allows a user to trade availability for consistency: a larger time threshold decreases availability but limits inconsistency, while a smaller threshold increases availability but produces more inconsistent results based on partial data. In addition, when failures heal, our scheme corrects previously produced results, ensuring eventual consistency.Our scheme uses a data-serializing operator to ensure that all replicas process data in the same order, and thus remain consistent in the absence of failures. To regain consistency after a failure heals, we experimentally compare approaches based on checkpoint\/redo and undo\/redo techniques and illustrate the performance trade-offs between these schemes.","category_a":"SIGMOD","category_b":"DB","keywords":["Application checkpointing","Eventual consistency","Failure rate","Fault tolerance","Serialization","Stream processing","Undo"],"vec":[0.1442070606,-0.0854734531],"nodes":["1718134","1712771","2033016","1695715"]},"47ec99aed698b07b24fdec85e4dbf584c4d2e707":{"id":"47ec99aed698b07b24fdec85e4dbf584c4d2e707","date":"2009-01-01T12:12:00Z","text":"Data anonymization techniques have been the subject of intense investigation in recent years, for many kinds of structured data, including tabular, graph and item set data. They enable publication of detailed information, which permits ad hoc queries and analyses, while guaranteeing the privacy of sensitive information in the data against a variety of attacks. In this tutorial, we aim to present a <i>unified<\/i> framework of data anonymization techniques, viewed through the lens of uncertainty. Essentially, anonymized data describes a set of possible worlds, one of which corresponds to the original data. We show that anonymization approaches such as suppression, generalization, perturbation and permutation generate different working models of uncertain data, some of which have been well studied, while others open new directions for research. We demonstrate that the privacy guarantees offered by methods such as k-anonymization and l-diversity can be naturally understood in terms of similarities and differences in the sets of possible worlds that correspond to the anonymized data. We describe how the body of work in query evaluation over uncertain databases can be used for answering ad hoc queries over anonymized data in a principled manner. A key benefit of the unified approach is the identification of a rich set of new problems for both the Data Anonymization and the Uncertain Data communities.","category_a":"ICDE","category_b":"DB","keywords":["Data anonymization","Data model","Database","Hoc (programming language)","Information sensitivity","Perturbation theory (quantum mechanics)","Possible world","Table (information)","Uncertain data","Zero suppression"],"vec":[1.1721894544,0.5969477273],"nodes":["1709589","1704011"]},"400f6f4304b1c12efb22acf7e80a1784015cb23a":{"id":"400f6f4304b1c12efb22acf7e80a1784015cb23a","date":"2010-01-01T12:12:00Z","text":"Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.","category_a":"Journal of Machine Learning Research","category_b":"Others","keywords":["Algorithm","Autoencoder","Bayesian network","Deep learning","Encoder","Experiment","Machine learning","Matrix regularization","Maxima and minima","Simulation","Stack (abstract data type)","Supervised learning","Test set","Unsupervised learning"],"vec":[-0.3917193963,0.4934798454],"nodes":["1761978","1751762","1760871","1798462","1724875","1751569"]},"315cd76e4a34b8fe27e20345abcd4fc27c7ee1ab":{"id":"315cd76e4a34b8fe27e20345abcd4fc27c7ee1ab","date":"2018-01-01T12:12:00Z","text":"In order for an enterprise to gain insight into its internal business and the changing outside environment, it is essential to provide the relevant data for in-depth analysis. Enterprise data is usually scattered across departments and geographic regions, and is often inconsistent. Data scientists spend the majority of their time finding, preparing, integrating, and cleaning relevant data sets. Data Civilizer is an end-toend data preparation system. In this paper, we present the complete system, focusing on our new workflow engine, a superior system for entity matching and consolidation, and new cleaning tools. Our workflow engine allows data scientists to author, execute and retrofit data preparation pipelines of different data discovery and cleaning services. Our end-to-end demo scenario is based on data from the MIT data warehouse and e-commerce data sets.","category_a":"","category_b":"Others","keywords":["Data science","E-commerce","End-to-end encryption","Pipeline (computing)","Semiconductor consolidation","Sputter cleaning","Tandem Mass Spectrometry Scoring Engine","Visual Accommodation","Workflow engine"],"vec":[0.6510805907,-0.5175717709],"nodes":["1801187","38737063","34568734","2523205","2893500","2034349","40394220","1743316","2033016","2168047","1695715","40384618"]},"00d3fadfadc977ba4b6511bec1b2a3026d877099":{"id":"00d3fadfadc977ba4b6511bec1b2a3026d877099","date":"2008-01-01T12:12:00Z","text":"We introduce the problem of zero-data learning, where a model must generalize to classes or tasks for which no training data are available and only a description of the classes or tasks are provided. Zero-data learning is useful for problems where the set of classes to distinguish or tasks to solve is very large and is not entirely covered by the training data. The main contributions of this work lie in the presentation of a general formalization of zero-data learning, in an experimental analysis of its properties and in empirical evidence showing that generalization is possible and significant in this context. The experimental work of this paper addresses two classification problems of character recognition and a multitask ranking problem in the context of drug discovery. Finally, we conclude by discussing how this new framework could lead to a novel perspective on how to extend machine learning towards AI, where an agent can be given a specification for a learning problem before attempting to solve it (with very few or even zero examples).","category_a":"AAAI","category_b":"AI","keywords":["Computer multitasking","Machine learning","Optical character recognition","Test set","Zero"],"vec":[-0.0296805488,-0.1387380407],"nodes":["1777528","1761978","1751762"]},"80fe1abae2594a2cb5466d3646abbc57fb13d144":{"id":"80fe1abae2594a2cb5466d3646abbc57fb13d144","date":"2008-01-01T12:12:00Z","text":"Data items that arrive online as streams typically have attributes which take values from one or more hierarchies (time and geographic location, source and destination IP addresses, etc.). Providing an aggregate view of such data is important for summarization, visualization, and analysis. We develop an aggregate view based on certain organized sets of large-valued regions (&#8220;heavy hitters&#8221;) corresponding to hierarchically discounted frequency counts. We formally define the notion of <i>hierarchical heavy hitters<\/i> (HHHs). We first consider computing (approximate) HHHs over a data stream drawn from a single hierarchical attribute. We formalize the problem and give deterministic algorithms to find them in a single pass over the input.\n In order to analyze a wider range of realistic data streams (e.g., from IP traffic-monitoring applications), we generalize this problem to multiple dimensions. Here, the semantics of HHHs are more complex, since a &#8220;child&#8221; node can have multiple &#8220;parent&#8221; nodes. We present online algorithms that find approximate HHHs in one pass, with provable accuracy guarantees. The product of hierarchical dimensions forms a mathematical lattice structure. Our algorithms exploit this structure, and so are able to track approximate HHHs using only a small, fixed number of statistics per stored item, regardless of the number of dimensions.\n We show experimentally, using real data, that our proposed algorithms yields outputs which are very similar (virtually identical, in many cases) to offline computations of the exact solutions, whereas straightforward heavy-hitters-based approaches give significantly inferior answer quality. Furthermore, the proposed algorithms result in an order of magnitude savings in data structure size while performing competitively.","category_a":"KDD","category_b":"AI","keywords":["Algorithm","Approximation algorithm","Computation","Crystal structure","Data structure","Geographic coordinate system","Online algorithm","Provable prime","Stream (computing)","Value (ethics)"],"vec":[1.0862012775,0.414497631],"nodes":["1709589","2096611","1711192","1704011"]},"38211dc39e41273c0007889202c69f841e02248a":{"id":"38211dc39e41273c0007889202c69f841e02248a","date":"2009-01-01T12:12:00Z","text":"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called &#x201C;ImageNet&#x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.","category_a":"CVPR","category_b":"ComVis","keywords":["Algorithm","Amazon Mechanical Turk","B-tree","Cluster analysis","Computer vision","ImageNet","Internet backbone","Outline of object recognition","Population","Synonym ring","The Turk","Utility","WordNet"],"vec":[-0.2760398799,-0.3495856052],"nodes":["8342699","40108904","2166511","33642044","2168945","3216322"]},"0a49b4de21363d86599d4a058aaf4f5aed019495":{"id":"0a49b4de21363d86599d4a058aaf4f5aed019495","date":"2016-01-01T12:12:00Z","text":"The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder\u2013 decoder with a subword-level encoder and a character-level decoder on four language pairs\u2013En-Cs, En-De, En-Ru and En-Fi\u2013 using the parallel corpora from WMT\u201915. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.","category_a":"ACL","category_b":"ComLing","keywords":["Encoder","Experiment","Machine translation","Neural machine translation","Parallel text","Substring"],"vec":[-1.0558082633,0.1648860981],"nodes":["8270717","1979489","1751762"]},"208edd765ff1fa2320dc87d2746b5b7460303bd8":{"id":"208edd765ff1fa2320dc87d2746b5b7460303bd8","date":"2003-01-01T12:12:00Z","text":"We introduce and study a new class of queries that we refer to as OPAC (optimization under parametric aggregation constraints) queries. Such queries aim to identify sets of database tuples that constitute solutions of a large class of optimization problems involving the database tuples. The constraints and the objective function are specified in terms of aggregate functions of relational attributes, and the parameter values identify the constants used in the aggregation constraints. We develop algorithms that preprocess relations and construct indices to efficiently provide answers to OPAC queries. The answers returned by our indices are approximate, not exact, and provide guarantees for their accuracy. Moreover, the indices can be tuned easily to meet desired accuracy levels, providing a graceful tradeoff between answer accuracy and index space. We present the results of a thorough experimental evaluation analyzing the impact of several parameters on the accuracy and performance of our techniques. Our results indicate that our methodology is effective and can be deployed easily, utilizing index structures such as R-trees.","category_a":"VLDB","category_b":"DB","keywords":["Aggregate function","Algorithm","Approximation","Approximation algorithm","Loss function","Mathematical optimization","Online public access catalog","Optimization problem","Preprocessor","Program optimization","R-tree"],"vec":[1.1128119274,0.1035207762],"nodes":["1679363","1736832","1721062","1704011","1699100"]},"4d8d303fd622cf3bd0899bfe532fbee41202e718":{"id":"4d8d303fd622cf3bd0899bfe532fbee41202e718","date":"2016-01-01T12:12:00Z","text":"","category_a":"","category_b":"Others","keywords":[],"vec":[-0.1917037281,0.0510806894],"nodes":["2681887","1728478","1728478"]},"63cbac5a39cd926a806f60116b845f9bd70f5544":{"id":"63cbac5a39cd926a806f60116b845f9bd70f5544","date":"2012-01-01T12:12:00Z","text":"Here we propose a novel model family with the objective of learning to disentangle the factors of variation in data. Our approach is based on the spike-and-slab restricted Boltzmann machine which we generalize to include higher-order interactions among multiple latent variables. Seen from a generative perspective, the multiplicative interactions emulates the entangling of factors of variation. Inference in the model can be seen as disentangling these generative factors. Unlike previous attempts at disentangling latent factors, the proposed model is trained using no supervised information regarding the latent factors. We apply our model to the task of facial expression classification.","category_a":"ArXiv","category_b":"Journal","keywords":["Boltzmann machine","Emulator","Interaction","Latent variable","Restricted Boltzmann machine","Slab allocation","Supervised learning","The Spike (1997)"],"vec":[-0.2122429291,0.2622598744],"nodes":["2755582","1760871","1751762"]},"57e979025374da67fae37fbb81bbadecee68cc08":{"id":"57e979025374da67fae37fbb81bbadecee68cc08","date":"2017-01-01T12:12:00Z","text":"Finding relevant data for a specific task from the numerous data sources available in any organization is a daunting task. This is not only because of the number of possible data sources where the data of interest resides, but also due to the data being scattered all over the enterprise and being typically dirty and inconsistent. In practice, data scientists are routinely reporting that the majority (more than 80%) of their effort is spent finding, cleaning, integrating, and accessing data of interest to a task at hand. We propose to demonstrate DATA CIVILIZER to ease the pain faced in analyzing data \"in the wild\". DATA CIVILIZER is an end-to-end big data management system with components for data discovery, data integration and stitching, data cleaning, and querying data from a large variety of storage engines, running in large enterprises.","category_a":"SIGMOD","category_b":"DB","keywords":["Big data","Cache (computing)","Data hub","Data science","Database engine","End-to-end encryption","Image stitching","Sputter cleaning"],"vec":[0.3632948452,-0.3829675465],"nodes":["34568734","38737063","1801187","2523205","2893500","2034349","1740095","1743316","2033016","2168047","1695715","8669763"]},"0bed90488e8950e5bea85ffca8b48fab3b67c1a1":{"id":"0bed90488e8950e5bea85ffca8b48fab3b67c1a1","date":"2005-01-01T12:12:00Z","text":"To escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x. With this objective, we present a non-local non-parametric density estimator. It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold. It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models.","category_a":"NIPS","category_b":"ML","keywords":["Curse of dimensionality","Kernel density estimation","Microsoft Windows","Mixture model","Test set"],"vec":[-0.4637467844,0.5770613318],"nodes":["1751762","1777528","1724875"]},"3c43d0c9a1d7864e7d819179940d857f8633d4ce":{"id":"3c43d0c9a1d7864e7d819179940d857f8633d4ce","date":"2004-01-01T12:12:00Z","text":"XML is the undisputed standard for data representation and exchange. As companies transact business over the Internet, letting authorized customers directly access, and even modify, XML data offers many advantages in terms of cost, accuracy, and timeliness. Given the complex business relationships between companies, and the sensitive nature of information, access must be provided selectively, using sophisticated access control specifications. Using the specification directly to determine if a user has access to an XML data item can be extremely inefficient. The alternative of fully materializing, for each data item, the users authorized to access it can be space-inefficient. In this article, we introduce a compressed accessibility map (CAM) as a space- and time-efficient solution to the access control problem for XML data. A CAM compactly identifies the XML data items to which a user has access, by exploiting structural locality of accessibility in tree-structured data. We present a CAM lookup algorithm for determining if a user has access to a data item that takes time proportional to the product of the depth of the item in the XML data and logarithm of the CAM size. We develop an algorithm for building an optimal size CAM that takes time linear in the size of the XML data set. While optimality cannot be preserved incrementally under data item updates, we provide an algorithm for incrementally maintaining near-optimality. Finally, we experimentally demonstrate the effectiveness of the CAM for multiple users on a variety of real and synthetic data sets.","category_a":"ACM Trans. Database Syst.","category_b":"Others","keywords":["Access control","Accessibility","Algorithm","Authentication","Authorization","Data (computing)","Data item","Data model","Foreach loop","Internet","Locality of reference","Lookup table","Multi-user","Synthetic data","XML"],"vec":[0.9411528699,0.0534734979],"nodes":["1689202","1704011","1708593","1735239"]},"1828eaa8750ee188111b92deae5c7f67323e723f":{"id":"1828eaa8750ee188111b92deae5c7f67323e723f","date":"2009-01-01T12:12:00Z","text":"We consider the problem of robotic object detection of such objects as mugs, cups, and staplers in indoor environments. While object detection has made significant progress in recent years, many current approaches involve extremely complex algorithms, and are prohibitively slow when applied to large scale robotic settings. In this paper, we describe an object detection system that is designed to scale gracefully to large data sets and leverages upward trends in computational power (as exemplified by Graphics Processing Unit (GPU) technology) and memory. We show that our GPU-based detector is up to 90 times faster than a well-optimized software version and can be easily trained on millions of examples. Using inexpensive off-the-shelf hardware, it can recognize multiple object types reliably in just a few seconds per frame.","category_a":"Intelligent Robots and Systems","category_b":"Others","keywords":["Algorithm","CUPS","Graphics","Graphics processing unit","Object detection","Object type (object-oriented programming)","Robot","Software versioning"],"vec":[-0.3595768198,-0.1952937006],"nodes":["5574038","3128990","2827616","1701538"]},"9315cf87a57a346c416a98984b8bf82d3b85e679":{"id":"9315cf87a57a346c416a98984b8bf82d3b85e679","date":"2016-01-01T12:12:00Z","text":"A GROUP OF database researchers meets periodically to discuss the state of the field and its key directions going forward. Past meetings were held in 1989,6 1990,11 1995,12 1996,1","category_a":"","category_b":"Others","keywords":["meeting"],"vec":[0.2774407054,-0.2293373959],"nodes":["2254232","1680081","1728318","1718134","1737944","1703347","1728620","1796515","3030274","1712149","1710965","2046257","1770962","1695576","1684197","1691108","2033016","1686199","1702212","5151034","1709145","1733290","2072569","1693070","1803218","9246931","1695715","1681578","1737896"]},"f9c431f58565f874f76a024add2aa80717ec5cf5":{"id":"f9c431f58565f874f76a024add2aa80717ec5cf5","date":"2012-01-01T12:12:00Z","text":"We propose a semi-supervised approach to solve the task of emotion recognition in 2D face images using recent ideas in deep learning for handling the factors of variation present in data. An emotion classification algorithm should be both robust to (1) remaining variations due to the pose of the face in the image after centering and alignment, (2) the identity or morphology of the face. In order to achieve this invariance, we propose to learn a hierarchy of features in which we gradually filter the factors of variation arising from both (1) and (2). We address (1) by using a multi-scale contractive convolutional network (CCNET) in order to obtain invariance to translations of the facial traits in the image. Using the feature representation produced by the CCNET, we train a Contractive Discriminative Analysis (CDA) feature extractor, a novel variant of the Contractive Auto-Encoder (CAE), designed to learn a representation separating out the emotion-related factors from the others (which mostly capture the subject identity, and what is left of pose after the CCNET). This system beats the state-of-the-art on a recently proposed dataset for facial expression recognition, the Toronto Face Database, moving the state-of-art accuracy from 82.4% to 85.0%, while the CCNET and CDA improve accuracy of a standard CAE by 8%.","category_a":"ECCV","category_b":"ComVis","keywords":[".cda file","Algorithm","Deep learning","Emotion recognition","Encoder","Extractor (mathematics)","Facial recognition system","Mathematical morphology","X\/Open"],"vec":[-0.4073176293,0.3549785808],"nodes":["2425018","1751762","1760871","1724875","1778734"]},"182015c5edff1956cbafbcb3e7bbe294aa54f9fc":{"id":"182015c5edff1956cbafbcb3e7bbe294aa54f9fc","date":"2011-01-01T12:12:00Z","text":"Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer. Unfortunately, for such large architectures the number of parameters can grow quadratically in the width of the network, thus necessitating hand-coded \u201clocal receptive fields\u201d that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality). In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods. Specifically, we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric. This approach allows us to harness the advantages of local receptive fields (such as improved scalability, and reduced data requirements) when we do not know how to specify such receptive fields by hand or where our unsupervised training algorithm has no obvious generalization to a topographic setting. We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered networks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0% and 60.1% accuracy, respectively.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Deep learning","Feature learning","High- and low-level","Locality of reference","Principle of locality","Requirement","STL (file format)","Scalability","Topography","Unsupervised learning"],"vec":[-0.4250745012,0.1971373738],"nodes":["5574038","1701538"]},"046a1302079f56b94c81457bf7fd21c3417a9f72":{"id":"046a1302079f56b94c81457bf7fd21c3417a9f72","date":"2015-01-01T12:12:00Z","text":"Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users\u2019 interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.","category_a":"Nature","category_b":"Others","keywords":["Algorithm","Architecture as Topic","Artificial Intelligence","Artificial intelligence","Artificial neural network","Computation","Computation (action)","Computer vision","Content-control software","Deep learning","E-commerce","Engineering","Extractor (mathematics)","Extractors","Feature vector","Gene Expression","Languages","Machine learning","Mental Orientation","Metrorrhagia","Mutation","Natural language","Natural language understanding","Neural Network Simulation","Nonlinear system","Particle Accelerators","Pattern recognition","Physical object","Pixel","Power (Psychology)","Question answering","Recommender system","Relevance","Sentiment analysis","Sequence motif","Smartphone","Smartphone","Social network","Speech Disorders","Speech recognition","algorithm","anatomical layer","cell transformation"],"vec":[-0.3221875749,-0.138798879],"nodes":["34740554","1751762","1760871"]},"6e7dadd63455c194e3472bb181aaf509f89b9166":{"id":"6e7dadd63455c194e3472bb181aaf509f89b9166","date":"2008-01-01T12:12:00Z","text":"Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Artificial neural network","Discriminative model","Feature extraction","Feedforward neural network","Initialization (programming)","Linear classifier","Machine learning","Nonlinear system","Program animation","Restricted Boltzmann machine"],"vec":[-0.3068261334,0.4742922153],"nodes":["1777528","1751762"]},"032e9974cedb31f5c6e354626760e54e5ebf1e3c":{"id":"032e9974cedb31f5c6e354626760e54e5ebf1e3c","date":"2017-01-01T12:12:00Z","text":"This work is the first comprehensive analysis of the properties of word embeddings learned by neural machine translation (NMT) models trained on bilingual texts. We show the word representations of NMT models outperform those learned from monolingual text by established algorithms such as Skipgram and CBOW on tasks that require knowledge of semantic similarity and\/or lexical\u2013syntactic role. These effects hold when translating from English to French and English to German, and we argue that the desirable properties of NMT word embeddings should emerge largely independently of the source and target languages. Further, we apply a recently-proposed heuristic method for training NMT models with very large vocabularies, and show that this vocabulary expansion method results in minimal degradation of embedding quality. This allows us to make a large vocabulary of NMT embeddings available for future research and applications. Overall, our analyses indicate that NMT embeddings should be used in applications that require word concepts to be organised according to similarity and\/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness.","category_a":"Machine Translation","category_b":"Others","keywords":["Algorithm","Heuristic","Lexical function","Machine translation","N-gram","Neural machine translation","Semantic similarity","Vocabulary","Word embedding"],"vec":[-1.1643229291,0.1377963208],"nodes":["38909889","1979489","34564969","1751762"]},"79ed6d5157b73449dc663516fc2c8dfcb917b99e":{"id":"79ed6d5157b73449dc663516fc2c8dfcb917b99e","date":"2017-01-01T12:12:00Z","text":"Concern about how to aggregate sensitive user data without compromising individual privacy is a major barrier to greater availability of data. The model of differential privacy has emerged as an accepted model to release sensitive information while giving a statistical guarantee for privacy. Many different algorithms are possible to address different target functions. We focus on the core problem of count queries, and seek to design mechanisms to release data associated with a group of n individuals. Prior work has focused on designing mechanisms by raw optimization of a loss function, without regard to the consequences on the results. This can leads to mechanisms with undesirable properties, such as never reporting some outputs (gaps), and overreporting others (spikes). We tame these pathological behaviors by introducing a set of desirable properties that mechanisms can obey. Any combination of these can be satisfied by solving a linear program (LP) which minimizes a cost function, with constraints enforcing the properties. We focus on a particular cost function, and provide explicit constructions that are optimal for certain combinations of properties, and show a closed form for their cost. In the end, there are only a handful of distinct optimal mechanisms to choose between: one is the well-known (truncated) geometric mechanism; the second a novel mechanism that we introduce here, and the remainder are found as the solution to particular LPs. These all avoid the bad behaviors we identify. We demonstrate in a set of experiments on real and synthetic data which is preferable in practice, for different combinations of data distributions, constraints, and privacy parameters.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Count data","Differential privacy","Experiment","Information sensitivity","Linear programming","Loss function","Mathematical optimization","Personally identifiable information","Privacy","Program optimization","Synthetic data","Tame","Whole Earth 'Lectronic Link"],"vec":[0.9488801499,0.3477553154],"nodes":["1709589","40415081","1704011"]},"b2bf34c0c0007145a389e014b7ddaa3daa76f332":{"id":"b2bf34c0c0007145a389e014b7ddaa3daa76f332","date":"2014-01-01T12:12:00Z","text":"In this paper, we present Vertexica, a graph analytics tools on top of a relational database, which is user friendly and yet highly e\u009bcient. Instead of constraining programmers to SQL, Vertexica offers a popular vertex-centric query interface, which is more natural for analysts to express many graph queries. e programmers simply provide their vertex-compute functions and Vertexica takes care of e\u009bciently executing them in the standard SQL engine. e advantage of using Vertexica is its ability to leverage the relational features and enable much more sophisticated graph analysis. ese include expressing graph algorithms which are di\u009bcult in vertexcentric but straightforward in SQL and the ability to compose endto-end data processing pipelines, including preand postprocessing of graphs as well as combining multiple algorithms for deeper insights. Vertexica has a graphical user interface and we outline several demonstration scenarios including, interactive graph analysis, complex graph analysis, and continuous and time series analysis.","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Care-of address","Graphical user interface","Pipeline (computing)","Programmer","Relational database","SQL","Time series","Usability","User interface"],"vec":[0.619067299,-0.4014039185],"nodes":["2153832","1960281","39708150","2033016","2313625","1695715"]},"40709c0e27d0fe6f9265b6c29847e63392f4001d":{"id":"40709c0e27d0fe6f9265b6c29847e63392f4001d","date":"2005-01-01T12:12:00Z","text":"Monitoring aggregates on IP traffic data streams is a compelling application for data stream management systems. The need for exploratory IP traffic data analysis naturally leads to posing related aggregation queries on data streams, that differ only in the choice of grouping attributes. In this paper, we address this problem of efficiently computing multiple aggregations over high speed data streams, based on a two-level LFTA\/HFTA DSMS architecture, inspired by Gigascope.Our first contribution is the insight that in such a scenario, additionally computing and maintaining fine-granularity aggregation queries (phantoms) at the LFTA has the benefit of supporting shared computation. Our second contribution is an investigation into the problem of identifying beneficial LFTA configurations of phantoms and user-queries. We formulate this problem as a cost optimization problem, which consists of two sub-optimization problems: how to choose phantoms and how to allocate space for them in the LFTA. We formally show the hardness of determining the optimal configuration, and propose cost greedy heuristics for these independent sub-problems based on detailed analyses. Our final contribution is a thorough experimental study, based on real IP traffic data, as well as synthetic data, to demonstrate the effectiveness of our techniques for identifying beneficial configurations.","category_a":"SIGMOD","category_b":"DB","keywords":["Computation","Experiment","Exploratory testing","Greedy algorithm","Heuristic","Optimization problem","Program optimization","Synthetic data","XSLT\/Muenchian grouping"],"vec":[0.5443152102,0.1318724477],"nodes":["40482866","1721062","1693070","1704011"]},"2febe441afece9e3f427fa4d541887fb89d0088f":{"id":"2febe441afece9e3f427fa4d541887fb89d0088f","date":"2004-01-01T12:12:00Z","text":"Many NLP tasks rely on accurately estimating word dependency probabilities P(&#969;<inf>1<\/inf>|&#969;<inf>2<\/inf>), where the words <i>w<\/i><inf>1<\/inf> and <i>w<\/i><inf>2<\/inf> have a particular relationship (such as verb-object). Because of the sparseness of counts of such dependencies, smoothing and the ability to use multiple sources of knowledge are important challenges. For example, if the probability P(<i>N<\/i>|<i>V<\/i>) of noun <i>N<\/i> being the subject of verb <i>V<\/i> is high, and <i>V<\/i> takes similar objects to <i>V'<\/i>, and <i>V'<\/i> is synonymous to <i>V\"<\/i>, then we want to conclude that P(<i>N<\/i>|<i>V\"<\/i>) should also be reasonably high---even when those words did not cooccur in the training data.To capture these higher order relationships, we propose a Markov chain model, whose stationary distribution is used to give word probability estimates. Unlike the manually defined random walks used in some link analysis algorithms, we show how to automatically learn a rich set of parameters for the Markov chain's transition probabilities. We apply this model to the task of prepositional phrase attachment, obtaining an accuracy of 87.54%.","category_a":"ICML","category_b":"ML","keywords":["Algorithm","Link analysis","Markov chain","Markov decision process","Natural language processing","Neural coding","Smoothing","Stationary process","Test set"],"vec":[-0.36836351,-0.2355289588],"nodes":["3259253","1812612","1701538"]},"205b9f4891a2ead886604f161a44b3aed483609a":{"id":"205b9f4891a2ead886604f161a44b3aed483609a","date":"2002-01-01T12:12:00Z","text":"This paper separates conditional parameter estimation, which consistently raises test set accuracy on statistical NLP tasks, from conditional model structures, such as the conditional Markov model used for maximum-entropy tagging, which tend to lower accuracy. Error analysis on the POS tagging task shows that the actual tagging errors made by the conditionally structured model derive principally not from label bias, as has been claimed, but from other ways in which the independence assumptions of the conditional model structure are unsuited to linguistic sequences. The paper presents new word-sense disambiguation and POS tagging experiments, and integrates apparently conflicting reports from other recent work. Conditional Structure versus Conditional Estimation in NLP Models","category_a":"EMNLP","category_b":"ComLing","keywords":["Conditional (computer programming)","Discriminative model","Error analysis for the Global Positioning System","Estimation theory","Experiment","Markov chain","Markov model","Maximum-entropy Markov model","Natural language processing","Part-of-speech tagging","Point of sale","Protologism","Test set","Word sense","Word-sense disambiguation"],"vec":[-0.3621095223,0.1606678937],"nodes":["38666915","1812612"]},"f21fcb8a04f764eb482479823a4933b09534a9b2":{"id":"f21fcb8a04f764eb482479823a4933b09534a9b2","date":"2003-01-01T12:12:00Z","text":"The usual procedure of optimizing hidden Markov Models for data likelihood has undesirable consequences in information extraction: it focuses attention on the data rather than on the labeling task. Often, joint likelihood is poorly correlated with extraction F1. We demonstrate that optimizing the conditional likelihood of the target labels addresses these limitations and is more indicative of task performance. Comparing joint and conditional likelihood also helps to explain the empirical finding that, for IE, HMMs with fixed structures tend to outperform those with more flexible structures: fixed structures constrain EM to better optimize conditional likelihood.","category_a":"","category_b":"Others","keywords":["Electron Microscopy","Hidden Markov model","Information extraction","Labels (device)"],"vec":[-0.3449926835,0.0339677345],"nodes":["1855302","2623915","38666915","1812612"]},"1680eaa31f30911e7460ff694b51794391bc7514":{"id":"1680eaa31f30911e7460ff694b51794391bc7514","date":"2004-01-01T12:12:00Z","text":"We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.","category_a":"ACL","category_b":"ComLing","keywords":["Generative model","Parsing","Unsupervised learning"],"vec":[-0.3863393959,-0.5181921798],"nodes":["38666915","1812612"]},"0825788b9b5a18e3dfea5b0af123b5e939a4f564":{"id":"0825788b9b5a18e3dfea5b0af123b5e939a4f564","date":"2014-01-01T12:12:00Z","text":"Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.","category_a":"EMNLP","category_b":"ComLing","keywords":["Microsoft Windows","Named-entity recognition","Sparse matrix","Word embedding"],"vec":[-0.5198235847,0.3286573653],"nodes":["40113280","2166511","1812612"]},"b9392e6965137b2f81c81ff17addf3096faa4ffa":{"id":"b9392e6965137b2f81c81ff17addf3096faa4ffa","date":"2001-01-01T12:12:00Z","text":"While O(n3) methods for parsing probabilistic context-free grammars (PCFGs) are well known, a tabular parsing framework for arbitrary PCFGs which allows for botton-up, topdown, and other parsing strategies, has not yet been provided. This paper presents such an algorithm, and shows its correctness and advantages over prior work. The paper finishes by bringing out the connections between the algorithm and work on hypergraphs, which permits us to extend the presented Viterbi (best parse) algorithm to an inside (total probability) algorithm.","category_a":"","category_b":"Others","keywords":["Algorithm","Chart parser","Context-free grammar","Context-free language","Correctness (computer science)","License","Parser","Parsing","Stochastic context-free grammar","Table (information)","algorithm"],"vec":[0.1011613254,-1.1443630131],"nodes":["38666915","1812612"]},"12244deb997152492d96c6246ec21b2b9804800d":{"id":"12244deb997152492d96c6246ec21b2b9804800d","date":"2011-01-01T12:12:00Z","text":"Reading text from photographs is a challenging problem that has received a significant amount of attention. Two key components of most systems are (i) text detection from images and (ii) character recognition, and many recent methods have been proposed to design better feature representations and models for both. In this paper, we apply methods recently developed in machine learning -- specifically, large-scale algorithms for learning the features automatically from unlabeled data -- and show that they allow us to construct highly effective classifiers for both detection and recognition to be used in a high accuracy end-to-end system.","category_a":"Document Analysis and Recognition","category_b":"Others","keywords":["Algorithm","End system","End-to-end principle","Feature learning","Machine learning","Optical character recognition","Unsupervised learning"],"vec":[-0.5056729606,-0.3590456105],"nodes":["5574038","37742741","1714272","2638806","39086009","1685072","25629078","1701538"]},"94b0e8e97c19ad0977d26e3e355d3ae09ad49365":{"id":"94b0e8e97c19ad0977d26e3e355d3ae09ad49365","date":"2011-01-01T12:12:00Z","text":"Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic models that have recently been applied to a wide range of problems, including collaborative filtering, classification, and modeling motion capture data. While much progress has been made in training non-conditional RBMs, these algorithms are not applicable to conditional models and there has been almost no work on training and generating predictions from conditional RBMs for structured output problems. We first argue that standard Contrastive Divergence-based learning may not be suitable for training CRBMs. We then identify two distinct types of structured output prediction problems and propose an improved learning algorithm for each. The first problem type is one where the output space has arbitrary structure but the set of likely output configurations is relatively small, such as in multi-label classification. The second problem is one where the output space is arbitrarily structured but where the output space variability is much greater, such as in image denoising or pixel labeling. We show that the new learning algorithms can work much better than Contrastive Divergence on both types of problems.","category_a":"UAI","category_b":"Others","keywords":["Algorithm","Collaborative filtering","Kullback\u2013Leibler divergence","Machine learning","Motion capture","Multi-label classification","Noise reduction","Pixel","Restricted Boltzmann machine","Spatial variability"],"vec":[-0.2467493823,0.600430374],"nodes":["3255983","1777528","1695689"]},"35c14fd2453b0dc02a02dcf538fbd775e4993e81":{"id":"35c14fd2453b0dc02a02dcf538fbd775e4993e81","date":"2013-01-01T12:12:00Z","text":"There is currently a tug-of-war going on surrounding data releases. On one side, there are many strong reasons pulling to release data to other parties: business factors, freedom of information rules, and scientific sharing agreements. On the other side, concerns about individual privacy pull back, and seek to limit releases. Privacy technologies such as differential privacy have been proposed to resolve this deadlock, and there has been much study of how to perform private data release of data in various forms. The focus of such works has been largely on the <i>data owner<\/i>: what process should they apply to ensure that the released data preserves privacy whilst still capturing the input data distribution accurately. Almost no attention has been paid to the needs of the <i>data user<\/i>, who wants to make use of the released data within their existing suite of tools and data. The difficulty of making use of data releases is a major stumbling block for the widespread adoption of data privacy technologies.\n In this paper, instead of proposing new privacy mechanisms for data publishing, we consider the whole data release process, from the data owner to the data user. We lay out a set of principles for privacy tool design that highlights the requirements for <i>interoperability<\/i>, <i>extensibility<\/i> and <i>scalability<\/i>. We put these into practice with UMicS, an end-to-end prototype system to control the release and use of private data. An overarching tenet is that it should be possible to integrate the released data into the data user's systems with the minimum of change and cost. We describe how to instantiate UMicS in a variety of usage scenarios. We show how using data modeling techniques from machine learning can improve the utility, in particular when combined with background knowledge that the data user may possess. We implement UMicS, and evaluate it over a selection of data sets and release cases. We see that UMicS allows for very effective use of released data, while upholding our privacy principles.","category_a":"CIKM","category_b":"Others","keywords":["Data anonymization","Data modeling","Deadlock","Differential privacy","End-to-end encryption","Information Rules","Information privacy","Machine learning","Microdata Corporation","Privacy","Requirement","StumbleUpon"],"vec":[1.1628153686,0.6126485603],"nodes":["1709589","39659954","1907373","1689202","1738196","1704011"]},"52d6d89cccb79fc5f8fa3c0e4e9c4021645b70f0":{"id":"52d6d89cccb79fc5f8fa3c0e4e9c4021645b70f0","date":"2018-01-01T12:12:00Z","text":"In many environments only a tiny subset of all states yield high reward. In these cases, few of the interactions with the environment provide a relevant learning signal. Hence, we may want to preferentially train on those high-reward states and the probable trajectories leading to them. To this end, we advocate for the use of a backtracking model that predicts the preceding states that terminate at a given high-reward state. We can train a model which, starting from a high value state (or one that is estimated to have high value), predicts and sample for which the (state, action)tuples may have led to that high value state. These traces of (state, action) pairs, which we refer to as Recall Traces, sampled from this backtracking model starting from a high value state, are informative as they terminate in good states, and hence we can use these traces to improve a policy. We provide a variational interpretation for this idea and a practical algorithm in which the backtracking model samples from an approximate posterior distribution over trajectories which lead to large rewards. Our method improves the sample efficiency of both onand off-policy RL algorithms across several environments and tasks.","category_a":"","category_b":"Others","keywords":["Algorithm","Approximation algorithm","Backtracking","Calculus of variations","Information","Interaction","RL (complexity)","RL circuit","Reinforcement learning","Rewards","Subgroup","Termination analysis","Trace theory","algorithm"],"vec":[-0.1445866365,0.2439551984],"nodes":["1996705","39728786","26958176","2542999","1736651","1777528","1751762"]},"45d814078179b7ac69086d8c7f40576244258549":{"id":"45d814078179b7ac69086d8c7f40576244258549","date":"2015-01-01T12:12:00Z","text":"We present an approach to speech recognition that uses only a neural network to map acoustic input to characters, a character-level language model, and a beam search decoding procedure. This approach eliminates much of the complex infrastructure of modern speech recognition systems, making it possible to directly train a speech recognizer using errors generated by spoken language understanding tasks. The system naturally handles out of vocabulary words and spoken word fragments. We demonstrate our approach using the challenging Switchboard telephone conversation transcription task, achieving a word error rate competitive with existing baseline systems. To our knowledge, this is the first entirely neural-network-based system to achieve strong speech transcription results on a conversational speech task. We analyze qualitative differences between transcriptions produced by our lexicon-free approach and transcriptions produced by a standard speech recognition system. Finally, we evaluate the impact of large context neural network character language models as compared to standard n-gram models within our framework.","category_a":"HLT-NAACL","category_b":"ComLing","keywords":["Artificial neural network","Baseline (configuration management)","Beam search","Finite-state machine","Language model","Lexicon","Medical transcription","N-gram","Natural language understanding","Speech recognition","Speech synthesis","Telephone exchange","Transcription (software)","Vocabulary","Word error rate"],"vec":[-0.4894929257,-0.1711326039],"nodes":["34961461","40548699","1746807","1701538"]},"4def24ede97126c946c58e8af355a87a6775e268":{"id":"4def24ede97126c946c58e8af355a87a6775e268","date":"2005-01-01T12:12:00Z","text":"The applicability of current information extraction techniques is severely limited by the need for supervised training data. We demonstrate that for certain field structured extraction tasks, small amounts of prior knowledge can be used to effectively learn models in a primarily unsupervised fashion. Many text information sources exhibit a latent field structure: such documents can be viewed as dense sequences of semantically coherent fields. Examples include classified advertisements and bibliographic citations, which we investigate here. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, we show that one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, unsupervised methods can attain accuracies comparable to simple supervised methods trained on the same data, using a combination of structural model constraints and targeted initializations.","category_a":"","category_b":"Others","keywords":["Advertisements","Classification","Coherence (physics)","Databases, Bibliographic","Document","Generative model","Hidden Markov model","Information extraction","Solutions","Structured text","Supervised learning","Test set","Unsupervised learning","citation"],"vec":[-0.7827169798,-0.6276441022],"nodes":["3050250","38666915","1812612"]},"9b1725d100e06bfd43ed5e996288e8c55c7d510b":{"id":"9b1725d100e06bfd43ed5e996288e8c55c7d510b","date":"2007-01-01T12:12:00Z","text":"Approximate queries on string data are important due to the prevalence of such data in databases and various conventions and errors in string data. We present the VSol estimator, a novel technique for estimating the selectivity of approximate string queries. The VSol estimator is based on <i>inverse strings<\/i> and makes the performance of the selectivity estimator independent of the number of strings. To get inverse strings we decompose all database strings into overlapping substrings of length q (q-grams) and then associate each q-gram with its inverse string: the IDs of all strings that contain the q-gram. We use signatures to compress inverse strings, and clustering to group similar signatures.\n We study our technique analytically and experimentally. The space complexity of our estimator only depends on the number of neighborhoods in the database and the desired estimation error. The time to estimate the selectivity is <i>independent<\/i> of the number of database strings and <i>linear<\/i> with respect to the length of query string. We give a detailed empirical performance evaluation of our solution for synthetic and real-world datasets. We show that VSol is effective for large skewed databases of short strings.","category_a":"ACM Trans. Database Syst.","category_b":"Others","keywords":["Approximation algorithm","Cluster analysis","DSPACE","Database","Grams","N-gram","Performance Evaluation","Query string","Selectivity (electronic)","Synthetic data","Type signature"],"vec":[0.8886186957,0.2275317786],"nodes":["2589589","1694144","1721062","1704011"]},"4e88de2930a4435f737c3996287a90ff87b95c59":{"id":"4e88de2930a4435f737c3996287a90ff87b95c59","date":"2015-01-01T12:12:00Z","text":"A Long Short-Term Memory (LSTM) network is a type of recurrent neural network architecture which has recently obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. TreeLSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).","category_a":"ACL","category_b":"ComLing","keywords":["Artificial neural network","Long short-term memory","Natural language","Network architecture","Network topology","Recurrent neural network","SemEval","Semantic similarity","Treebank"],"vec":[-0.4897826916,-0.3759625179],"nodes":["8421815","2166511","1812612"]},"2b329183e93cb8c1c20c911c765d9a94f34b5ed5":{"id":"2b329183e93cb8c1c20c911c765d9a94f34b5ed5","date":"2014-01-01T12:12:00Z","text":"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.","category_a":"ArXiv","category_b":"Journal","keywords":["Approximation algorithm","Backpropagation","Discriminative model","Experiment","Generative adversarial networks","Generative model","Markov chain","Markov decision process","Minimax","Multilayer perceptron","Perceptron","Test set"],"vec":[-0.2418676467,0.8695069501],"nodes":["34740554","2409581","1778734","30139116","1923596","1955694","1760871","1751762"]},"9fe93ec9c2f8ca5ddbc46c050ba4216c09154980":{"id":"9fe93ec9c2f8ca5ddbc46c050ba4216c09154980","date":"2015-01-01T12:12:00Z","text":"Recently datasets that contain sentence descriptions of images have enabled models that can automatically generate image captions. However, collecting these datasets are still very expensive. Here, we present SentenceRacer, an online game that gathers and verifies descriptions of images at no cost. Similar to the game hangman, players compete to uncover words in a sentence that ultimately describes an image. SentenceRacer both generates and verifies that the sentences are accurate descriptions. We show that SentenceRacer generates annotations of higher quality than those generated on Amazon Mechanical Turk (AMT).","category_a":"ArXiv","category_b":"Journal","keywords":["Amazon Mechanical Turk","Human-based computation game","The Turk"],"vec":[-0.3315320211,-0.3444416169],"nodes":["35163655","2271195","2580593","35609041","3216322"]},"54c22be7d4bdcb761c42f2670242c8689e29206f":{"id":"54c22be7d4bdcb761c42f2670242c8689e29206f","date":"2008-01-01T12:12:00Z","text":"Recent theoretical studies indicate that deep architectures [4, 2] are needed to efficiently model complex distributions and to achieve better generalization performance on challenging recognition tasks. The belief that additional levels of functional composition will yield increased representational and modeling power is not new [13, 9, 16]. In practice, learning in deep architectures had proven to be difficult. However, this situation recently changed with the successful approaches of [7, 10, 1, 14, 12] for training Deep Belief Networks and stacked autoencoders (deep neural networks). What these works have in common is several principles for effectively learning deep architectures. In our work we attempt to generalize these learning principles to deep architectures in which each level is represented by an ensemble of decision trees. \u201cDeep woods\u201d are what we call the resulting stacked ensembles of trees. We introduce two new algorithms for learning of deep woods: one is inspired by the deterministic autoencoders which have been used to build deep neural networks, and one is a generalization of Restricted Boltzmann Machines, which have been used to build Deep Belief Networks. Introduction Theoretical results suggest that, in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. for vision, language, and other AI-level tasks), one needs deep architectures [7, 4, 2]. Deep architectures are composed of multiple levels of non-linear operations, such as neural nets with many hidden layers or complicated propositional formulae re-using many sub-formulae. The depth of an architecture is the depth of the circuit that represents this function [2, 4]. Searching the parameter space of deep architectures is a difficult optimization task, but recently proposed learning algorithms\u2014such as those for Deep Belief Networks [7]\u2014can successfully train deep architectures. The resulting deep machines can beat the shallow state-of-the-art in certain areas. A principle that has been found to help optimizing deep networks is based on the use of unsupervised learning to initialize each layer in the network. Each layer learns a representation of its input that serves as input for the next layer, and training progresses by greedily [7]. The principle of training a deep architecture by greedy layer-wise unsupervised training has been shown to be successful for deep connectionist architectures [7, 10, 1, 14, 12]. Does this deep training principle apply to deep architectures comprising other kinds of primitive units? We attempt to exploit this principle to develop new deep architectures based on deterministic or stochastic decision trees. Decision trees are local estimators in the sense of relying on a partition of the input space and using separate parameters for each region [3]. For this reason, single decision trees need at least as many training examples as there are variations of interest in the target function, and they cannot generalize to new variations not covered in the training set. On the other hand, ensembles of trees (like boosted trees [6], and forests [11, 5]) are more powerful than a single tree. They add a level to the architecture, which allows the model to discriminate among a number of regions exponential in the number of parameters [3]. By analogy, clustering forms a single partition and generally involves a loss of information about the input, whereas a multi-clustering provides a set of separate partitions of the input space. If we identify for an input example in which region of each partition it belongs, we have a description of the input pattern which might be very rich, possibly not losing any information. The tuple of symbols specifying to which region of each partition the input belongs can be seen as a transformation of the input into a distributed representation [8], where the statistical structure of the data and the factors of variation in it could be disentangled. This corresponds to the kind of partition of the input space that an ensemble of trees can represent: the distributed representation output at each level transforms the input into a binary code representing the leaves (and perhaps internal nodes) in the ensemble into which the input falls, with one bit per node. Motivated by the above considerations, we propose two approaches to train a deep architecture where each level is an ensemble of trees. The first approach is analogous to autoencoders, which compute a deterministic reconstruction and minimize a reconstruction error, while the second approach is analogous to Restricted Boltzmann Machines (RBMs), which build a generative model of the input through a discrete hidden variable. In both cases, a global training criterion is defined which pushes the different trees in the ensemble to take on complementary roles in explaining the input. Consider a binary split ck(x) for decision node k of the ensemble. ck can be a deterministic or stochastic function of the input pattern x. A node or leaf j, or equivalently a region associated with one of the decision trees, is defined by a conjunction of splits, i.e. a product of binary decisions of the form ck(x) or 1\u2212 ck(x). We denote hj(x) the binary (deterministic or stochastic) variable which indicates the presence of x in the corresponding region (or node of a tree) j. Note that whereas the ck\u2019s can take arbitrary patterns of binary values, the tree structure imposes some constraints on the values of the hj\u2019s, such that hj can be 1 only if its parent (if any) has the appropriate binary value (depending on whether hj is child 0 or child 1 of its parent). Let code(j)k be a binary value that specifies for node j whether node k (which should be an ancestor of j in j\u2019s tree) should be 1 or 0 for node j to be allowed to be non-zero, and let ancestors(j) be the set of ancestors of j (i.e. nodes whose value constrains the value of hj). AutoEncoding Woods In autoencoding forests, we associate with each leaf a reconstruction vector rj , and we optimize both the reconstruction patterns and the node decision conditions so as to minimize a reconstruction error. The reconstruction for the whole forest is obtained by summing the reconstructions from each tree:","category_a":"","category_b":"Others","keywords":["Algorithm","Anatomic Node","Arabic numeral 0","Architecture as Topic","Artificial neural network","Autoencoder","Bayesian network","Binary code","Binary number","Citrus aurantium","Cluster analysis","Connectionism","Decision","Decision Trees","Decision tree","Decision tree learning","Deep learning","Eisenstein's criterion","Error detection and correction","Forests","Generalization (Psychology)","Generative model","Gradient boosting","Greedy algorithm","High- and low-level","Influence diagram","Inspiration function","Local hidden variable theory","Machine learning","Neural Network Simulation","Nonlinear system","Program optimization","Reconstructive Surgical Procedures","Representation (action)","Restricted Boltzmann machine","Time complexity","Tree structure","Trees (plant)","Unit","Unsupervised learning","Wood material","algorithm","anatomical layer","exponential","statistical cluster"],"vec":[-0.3706542761,0.4504900992],"nodes":["36872644","1751762","34999784","1777528"]},"3f43314011db0e92e629d6ee6cf6a918e1e258bb":{"id":"3f43314011db0e92e629d6ee6cf6a918e1e258bb","date":"2016-01-01T12:12:00Z","text":"We describe the neural machine translation system of New York University (NYU) and University of Montreal (MILA) for the translation tasks of WMT\u201916. The main goal of NYU-MILA submission to WMT\u201916 is to evaluate a new character-level decoding approach in neural machine translation on various language pairs. The proposed neural machine translation system is an attention-based encoder\u2013decoder with a subword-level encoder and a character-level decoder. The decoder of the neural machine translation system does not require explicit segmentation, when characters are used as tokens. The character-level decoding approach provides benefits especially when translating a source language into other morphologically rich languages.","category_a":"WMT","category_b":"Others","keywords":["Encoder","Machine translation","Neural machine translation","Substring"],"vec":[-0.9788347849,0.0295108718],"nodes":["8270717","1979489","1751762"]},"65d994fb778a8d9e0f632659fb33a082949a50d3":{"id":"65d994fb778a8d9e0f632659fb33a082949a50d3","date":"2009-01-01T12:12:00Z","text":"Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work.","category_a":"","category_b":"Others","keywords":["Architecture as Topic","Bayesian network","High-level programming language","Noise reduction","Solutions"],"vec":[-0.3529986319,0.2875411525],"nodes":["1761978","1751762","40632605","1724875"]},"0f41cd1792db9ff879fdfffc746cf5a01adf207f":{"id":"0f41cd1792db9ff879fdfffc746cf5a01adf207f","date":"2012-01-01T12:12:00Z","text":"In this paper, we describe a versioned database storage manager we are developing for the SciDB scientific database. The system is designed to efficiently store and retrieve array-oriented data, exposing a \"no-overwrite\" storage model in which each update creates a new \"version\" of an array. This makes it possible to perform comparisons of versions produced at different times or by different algorithms, and to create complex chains and trees of versions. We present algorithms to efficiently encode these versions, minimizing storage space while still providing efficient access to the data. Additionally, we present an optimal algorithm that, given a long sequence of versions, determines which versions to encode in terms of each other (using delta compression) to minimize total storage space or query execution cost. We compare the performance of these algorithms on real world data sets from the National Oceanic and Atmospheric Administration (NOAA), Open Street Maps, and several other sources. We show that our algorithms provide better performance than existing version control systems not optimized for array data, both in terms of storage size and access time, and that our delta-compression algorithms are able to substantially reduce the total storage space when versions exist with a high degree of similarity.","category_a":"Data Engineering","category_b":"DB","keywords":["Access time","Algorithm","Array programming","Control system","Data compression","Database","Database storage structures","Delta encoding","Map","OpenStreetMap","SciDB","Software versioning","Storage model","Version control"],"vec":[0.7388018754,-0.2245546421],"nodes":["3270589","1680925","2033016","1695715"]},"27211ed68a7a00f1df0121fa1890a1b2acdd1a88":{"id":"27211ed68a7a00f1df0121fa1890a1b2acdd1a88","date":"2000-01-01T12:12:00Z","text":"A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.","category_a":"NIPS","category_b":"ML","keywords":["Artificial neural network","Concatenation","Corpus linguistics","Curse of dimensionality","Experiment","Grams","Language model","N-gram","Text corpus"],"vec":[-0.6760075323,-0.3186924857],"nodes":["1751762","34388423","1724875"]},"6edc18adf0b57400726b8fc76b3428173d118826":{"id":"6edc18adf0b57400726b8fc76b3428173d118826","date":"2011-01-01T12:12:00Z","text":"Microblogs such as Twitter are a tremendous repository of user-generated content. Increasingly, we see tweets used as data sources for novel applications such as disaster mapping, brand sentiment analysis, and real-time visualizations. In each scenario, the workflow for processing tweets is ad-hoc, and a lot of unnecessary work goes into repeating common data processing patterns. We introduce TweeQL, a stream query processing language that presents a SQL-like query interface for unstructured tweets to generate structured data for downstream applications. We have built several tools on top of TweeQL, most notably TwitInfo, an event timeline generation and exploration interface that summarizes events as they are discussed on Twitter. Our demonstration will allow the audience to interact with both TweeQL and TwitInfo to convey the value of data embedded in tweets.","category_a":"SIGMOD","category_b":"DB","keywords":["Data model","Database","Downstream (software development)","Hoc (programming language)","SQL","Sentiment analysis","User-generated content"],"vec":[0.4666519157,-0.6656864213],"nodes":["40030541","35609041","2259673","1743286","2033016","34205614"]},"0c7650ed95c7d5fc90d769d5e34fd519313e8383":{"id":"0c7650ed95c7d5fc90d769d5e34fd519313e8383","date":"2005-01-01T12:12:00Z","text":"Skew is prevalent in many data sources such as IP traffic streams. To continually summarize the distribution of such data, a high-biased set of quantiles (e.g., 50th, 90th and 99th percentiles) with finer error guarantees at higher ranks (e.g., errors of 5, 1 and 0.1 percent, respectively) is more useful than uniformly distributed quantiles (e.g., 25th, 50th and 75th percentiles) with uniform error guarantees. In this paper, we address the following two problems. First, can we compute quantiles with finer error guarantees for the higher ranks of the data distribution effectively using less space and computation time than computing all quantiles uniformly at the finest error? Second, if specific quantiles and their error bounds are requested a priori, can the necessary space usage and computation time be reduced? We answer both questions in the affirmative by formalizing them as the \"high-biased\" and the \"targeted\" quantiles problems, respectively, and presenting algorithms with provable guarantees, that perform significantly better than previously known solutions for these problems. We implemented our algorithms in the Gigascope data stream management system, and evaluated alternate approaches for maintaining the relevant summary structures. Our experimental results on real and synthetic IP data streams complement our theoretical analyses, and highlight the importance of lightweight, non-blocking implementations when maintaining summary structures over highspeed data streams.","category_a":"ICDE","category_b":"DB","keywords":["Algorithm","Computation","Non-blocking algorithm","Provable prime","Space\u2013time tradeoff","Synthetic data","Time complexity"],"vec":[0.7547367791,0.2728386282],"nodes":["1709589","2096611","1711192","1704011"]},"232d3305b2c4c68a57b357c2cdea7c754891ebcb":{"id":"232d3305b2c4c68a57b357c2cdea7c754891ebcb","date":"2009-01-01T12:12:00Z","text":"When merging data from various sources, it is often the case that small variations in data format and interpretation cause traditional functional dependencies (FDs) to be violated, without there being an intrinsic violation of semantics. Examples include differing address formats, or different reported latitude\/longitudes for a given address. In this paper, we define metric functional dependencies, which strictly generalize traditional FDs by allowing small differences (controlled by a metric) in values of the consequent attribute of an FD. We present efficient algorithms for the verification problem: determining whether a given metric FD holds for a given relation. We experimentally demonstrate the validity and efficiency of our approach on various data sets that lie in multidimensional spaces.","category_a":"Data Engineering","category_b":"DB","keywords":["Algorithm","Functional dependency"],"vec":[0.534173757,0.0293762206],"nodes":["1721062","3157944","1704011","1747652"]},"0406f0696982b26bdf2a456123439c8ddcf8afb1":{"id":"0406f0696982b26bdf2a456123439c8ddcf8afb1","date":"2015-01-01T12:12:00Z","text":"Often, the performance on a supervised machine learning task is evaluated with a task loss function that cannot be optimized directly. Examples of such loss functions include the classification error, the edit distance and the BLEU score. A common workaround for this problem is to instead optimize a surrogate loss function, such as for instance cross-entropy or hinge loss. In order for this remedy to be effective, it is important to ensure that minimization of the surrogate loss results in minimization of the task loss, a condition that we call consistency with the task loss. In this work, we propose another method for deriving differentiable surrogate losses that provably meet this requirement. We focus on the broad class of models that define a score for every input-output pair. Our idea is that this score can be interpreted as an estimate of the task loss, and that the estimation error may be used as a consistent surrogate loss. A distinct feature of such an approach is that it defines the desirable value of the score for every input-output pair. We use this property to design specialized surrogate losses for Encoder-Decoder models often used for sequence prediction tasks. In our experiment, we benchmark on the task of speech recognition. Using a new surrogate loss instead of cross-entropy to train an Encoder-Decoder speech recognizer brings a significant 9% relative improvement in terms of Character Error Rate (CER) in the case when no extra corpora are used for language modeling.","category_a":"ArXiv","category_b":"Journal","keywords":["BLEU","Benchmark (computing)","Cross entropy","Edit distance","Encoder","Finite-state machine","Hinge loss","Language model","Loss function","Machine learning","Speech recognition","Supervised learning","Text corpus","Workaround"],"vec":[-0.6799310279,-0.1517801857],"nodes":["3335364","1862138","2616163","3195305","2292403","1760871","1751762"]},"3e4883a0ab6c5830785b83b5af74fcd63b1c556e":{"id":"3e4883a0ab6c5830785b83b5af74fcd63b1c556e","date":"2017-01-01T12:12:00Z","text":"Directed latent variable models that formulate the joint distribution as p(x, z) = p(z)p(x | z) have the advantage of fast and exact sampling. However, these models have the weakness of needing to specify p(z), often with a simple fixed prior that limits the expressiveness of the model. Undirected latent variable models discard the requirement that p(z) be specified with a prior, yet sampling from them generally requires an iterative procedure such as blocked Gibbs-sampling that may require many steps to draw samples from the joint distribution p(x, z). We propose a novel approach to learning the joint distribution between the data and a latent code which uses an adversarially learned iterative procedure to gradually refine the joint distribution, p(x, z), to better match with the data distribution on each step. GibbsNet is the best of both worlds both in theory and in practice. Achieving the speed and simplicity of a directed latent variable model, it is guaranteed (assuming the adversarial game reaches the virtual training criteria global minimum) to produce samples from p(x, z) with only a few sampling iterations. Achieving the expressiveness and flexibility of an undirected latent variable model, GibbsNet does away with the need for an explicit p(z) and has the ability to do attribute prediction, class-conditional generation, and joint image-attribute modeling in a single model which is not trained for any of these specific tasks. We show empirically that GibbsNet is able to learn a more complex p(z) and show that this leads to improved inpainting and iterative refinement of p(x, z) for dozens of steps and stable generation without collapse for thousands of steps, despite being trained on only a few steps.","category_a":"NIPS","category_b":"ML","keywords":["Gibbs sampling","Graph (discrete mathematics)","Inpainting","Iteration","Iterative method","Iterative refinement","Latent variable","Latent variable model","Maxima and minima","Sampling (signal processing)"],"vec":[-0.233030968,0.6567577692],"nodes":["2059369","40482726","2825246","40061310","1760871","1751762"]},"771a8edc3a434af7e1af3b0c9424c7e199e4c646":{"id":"771a8edc3a434af7e1af3b0c9424c7e199e4c646","date":"2018-01-01T12:12:00Z","text":"Concern about how to aggregate sensitive user data without compromising individual privacy is a major barrier to greater availability of data. Differential privacy has emerged as an accepted model to release sensitive information while giving a statistical guarantee for privacy. Many different algorithms are possible to address different target functions. We focus on the core problem of count queries, and seek to design mechanisms to release data associated with a group of n individuals. Prior work has focused on designing mechanisms by raw optimization of a loss function, without regard to the consequences on the results. This can leads to mechanisms with undesirable properties, such as never reporting some outputs (gaps), and overreporting others (spikes). We tame these pathological behaviors by introducing a set of desirable properties that mechanisms can obey. Any combination of these can be satisfied by solving a linear program (LP) which minimizes a cost function, with constraints enforcing the properties. We focus on a particular cost function, and provide explicit constructions that are optimal for certain combinations of properties, and show a closed form for their cost. In the end, there are only a handful of distinct optimal mechanisms to choose between: one is the well-known (truncated) geometric mechanism; the second a novel mechanism that we introduce here, and the remainder are found as the solution to particular LPs. These all avoid the bad behaviors we identify. We demonstrate in a set of experiments on real and synthetic data which is preferable in practice, for different combinations of data distributions, constraints, and privacy parameters.","category_a":"","category_b":"Others","keywords":["Algorithm","Count data","Differential privacy","Experiment","Information sensitivity","Linear programming","Loss function","Mathematical optimization","Personally identifiable information","Privacy","Program optimization","Programming, Linear","Synthetic data","Tame","Tosylarginine Methyl Ester","Whole Earth 'Lectronic Link","algorithm"],"vec":[0.9487591938,0.34754176],"nodes":["1709589","40415081","1704011"]},"0a22ce29e0fd1b0303ede3f5a94e1fe70764e455":{"id":"0a22ce29e0fd1b0303ede3f5a94e1fe70764e455","date":"2015-01-01T12:12:00Z","text":"This paper reviews video content analysis from the various situations into matter version. The totally different researchers are applied different technique to unravel the approaches. It is a tendency to tend to jointly obtaining down addressing the required down siting extracting the frames from video, comparison the frames; pattern matching and generating the corresponding text description is address here. Hence additionally created a discussion, observation and comparison of quick work applied during this work. It is a tendency to mix the output of progressive object and activity detectors with &quot;real-world&quot; data to pick the foremost probable subject-verb-object triplet for describing a video. It is a tendency to show that this data, mechanically well-mined from web-scale text corpora, hence projected choice rule by","category_a":"","category_b":"Others","keywords":["Creation","Detectors","Digital video","Foremost","Frame (physical object)","MinEd","Pattern matching","Probability","Text corpus","Triplet state","Video content analysis","X86"],"vec":[-0.2372947414,-0.2021953749],"nodes":[0,"6672188","4746195","8428273","1812612","39230960","3614955","1685978","32868266","38716310",0,0,"8342699","40441139","2166511","33642044","38262926","1763086","39668247",0,"10771328","3056091","1681236","2560099","33652486","1782755","1685089","2983898","1701355","1737809","1693428"]},"73463ec3391df70d4c38de6a1e963830a85efbe6":{"id":"73463ec3391df70d4c38de6a1e963830a85efbe6","date":"2013-01-01T12:12:00Z","text":"We introduce a new method for training deep Boltzmann machines jointly. Prior methods of training DBMs require an initial learning pass that trains the model greedily, one layer at a time, or do not perform well on classification tasks. In our approach, we train all layers of the DBM simultaneously, using a novel training procedure called multi-prediction training. The resulting model can either be interpreted as a single generative model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent networks that share parameters and may be approximately averaged together using a novel technique we call the multi-inference trick. We show that our approach performs competitively for classification and outperforms previous methods in terms of accuracy of approximate inference and classification with missing inputs. 1 Deep Boltzmann machines A deep Boltzmann machine (Salakhutdinov and Hinton, 2009) is a probabilistic model consisting of many layers of random variables, most of which are latent. Typically, a DBM contains a set of D input features v that are called the visible units because they are always observed during both training and evaluation. The DBM is usually applied to classification problems and thus often represents the class label with a one-of-k code in the form of a discrete-valued label unit y. y is observed (on examples for which it is available) during training. The DBM also contains several latent variables that are never observed. These hidden units are usually organized into L layers h of size Ni, i = 1, . . . , L, with each unit in a layer conditionally independent of the other units in the layer given the neighboring layers. These conditional independence properties allow fast Gibbs sampling because an entire layer of units can be sampled at a time. Likewise, mean field inference with fixed point equations is fast because each fixed point equation gives a solution to roughly half of the variational parameters. Inference proceeds by alternating between updating all of the even numbered layers and updating all of the odd numbered layers. A DBM defines a probability distribution by exponentiating and normalizing an energy function P (v, h, y) = 1 Z exp (\u2212E(v, h, y))","category_a":"ArXiv","category_b":"Journal","keywords":["Approximation algorithm","Boltzmann machine","Calculus of variations","Dbm","Decibel","EXPTIME","Fixed point (mathematics)","Generative model","Gibbs sampling","Greedy algorithm","Latent variable","Mathematical optimization","Sampling (signal processing)","Statistical model","Variational method (quantum mechanics)"],"vec":[-0.4685831125,0.7553357471],"nodes":["34740554","1760871","1751762"]},"069340a9fb06268b19e12a59de87547c9750fc79":{"id":"069340a9fb06268b19e12a59de87547c9750fc79","date":"2015-01-01T12:12:00Z","text":"","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network"],"vec":[-0.1960495245,0.0004354105],"nodes":["35224828","2041695","1751762","1760871","1723772"]},"e27d81521dc4e8b6ea93947c05ffccf06784f569":{"id":"e27d81521dc4e8b6ea93947c05ffccf06784f569","date":"2013-01-01T12:12:00Z","text":"In this paper, we present an audio chord recognition system based on a recurrent neural network. The audio features are obtained from a deep neural network optimized with a combination of chromagram targets and chord information, and aggregated over different time scales. Contrarily to other existing approaches, our system incorporates acoustic and musicological models under a single training objective. We devise an efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account. The resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major\/minor prediction task.","category_a":"ISMIR","category_b":"Others","keywords":["Algorithm","Artificial neural network","Chroma feature","Deep learning","Recurrent neural network"],"vec":[-0.2797537218,-0.1122832041],"nodes":["2488222","1751762","1724875"]},"6bf0414dae4f10c7e54fb9e5e8af5d0d0cab290b":{"id":"6bf0414dae4f10c7e54fb9e5e8af5d0d0cab290b","date":"2012-01-01T12:12:00Z","text":"The contractive auto-encoder learns a representation of the input data that captures the local manifold structure around each data point, through the leading singular vectors of the Jacobian of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a high-density region of the input space. This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto-encoder. The associated stochastic process defines a distribution from which one can sample, and which experimentally appears to converge quickly and mix well between modes, compared to Restricted Boltzmann Machines and Deep Belief Networks. The intuitions behind this procedure can also be used to train the second layer of contraction that pools lower-level features and learns to be invariant to the local directions of variation discovered in the first layer. We show that this can help learn and represent invariances present in the data and improve classification error.","category_a":"ICML","category_b":"ML","keywords":["Autoencoder","Bayesian network","Contraction mapping","Data point","Encoder","Experiment","Jacobian matrix and determinant","Singular value decomposition","Stochastic process"],"vec":[-0.5018700368,0.6536243411],"nodes":["2425018","2921469","1724875","1751762"]},"2c27683f6ecf8f2e94ff3254fa2630f90e3aacf9":{"id":"2c27683f6ecf8f2e94ff3254fa2630f90e3aacf9","date":"2014-01-01T12:12:00Z","text":"In this paper we propose and investigate a novel nonlinear unit, called an Lp unit, for deep neural networks. The proposed Lp unit receives signal from several projections of a subset of units in the layer below and computes the normalized Lp norm. We notice two interesting interpretations of the Lp unit. First, the proposed unit can be understood as a generalization of a number of conventional pooling operators such as average, root-mean-square and max pooling widely used in, for instance, convolutional neural networks (CNN), HMAX models and neocognitrons. Furthermore, the Lp unit is, to a certain degree, similar to the recently proposed maxout unit (Goodfellow et al., 2013b) which achieved the stateof-the-art object recognition results on a number of benchmark datasets. Secondly, we provide a geometrical interpretation of the activation function based on which we argue that the Lp unit is more efficient at representing complex, nonlinear separating boundaries. Each Lp unit defines a superelliptic boundary, with its exact shape defined by the order p. We claim that this makes it possible to model arbitrarily shaped, curved boundaries more efficiently by combining a few Lp units of different orders. This insight justifies the need of using or learning different orders for each unit in the model. We empirically evaluate the proposed Lp units on a number of datasets and show that multilayer perceptrons (MLP) consisting of the Lp units achieves the state-of-theart results on a number of benchmark datasets. Furthermore, we evaluate the proposed Lp unit on the recently proposed deep recurrent neural networks (RNN).","category_a":"KDD","category_b":"AI","keywords":["Activation function","Artificial neural network","Benchmark (computing)","Convolutional neural network","Deep learning","Feedforward neural network","Memory-level parallelism","Multilayer perceptron","Neural Networks","Nonlinear system","Outline of object recognition","Perceptron","Recurrent neural network"],"vec":[-0.404800036,0.2921673238],"nodes":["1854385","1979489","1996134","1751762"]},"60fdfa95dee2dc3b125f1552aa5c080542707e8f":{"id":"60fdfa95dee2dc3b125f1552aa5c080542707e8f","date":"2008-01-01T12:12:00Z","text":"Recently, there has been considerable interest in employing unsupervised learning methods as feature extractors for supervised learning tasks such as classification. The literature shows that methods based on this approach have proved to be competitive with established state-of-the-art machine learning strategies. One important recent advance was the discovery by (Hinton, Osindero, & Teh, 2006) of the role that unsupervised learning methods can play as an initialization for subsequent training for a supervised task. Unsupervised learning methods that extract sparse features of the data have received particular attention and has been shown by Raina, Battle, Lee, Packer, and Ng (2007) to significantly improve classification performance. While there are some clear advantages of not including label information in learning the feature set or initialization \u2014 for instance, Raina et al. (2007) use a sparse coding scheme trained on a large quantity of \u201crelated\u201d unlabeled data to augment the feature set training \u2014 the use of unsupervised feature extraction gives no safeguard against highly discriminative features being cast aside. In relatively uncomplicated tasks, such as classifying hand-written digits using the MNIST dataset (Lecun, Bottou, Bengio, & Haffner, 1998), the lack of supervisory information in determining the representation is not likely to present a problem as most of the salient features of the image are useful in the classification task. However in more complex tasks where a large number of salient features of the data can have nothing to do with the target task, unsupervised learning methods are not likely to efficiently generate discriminative features. On the other hand, the lesson of the utility of extracting features of the unsupervised input data pattern, demonstrated in Hinton et al. (2006) and in Raina et al. (2007) should not be ignored. In this work, we focus on the problem of learning a sparse representation of data that stakes out a compromise position: explicitly taking into account information such as the labels in a classification task, while simultaneously attempting to capture descriptive features of the input data. Our method is based on a novel probabilistic interpretation of the canonical ridge analysis (Vinod, 1976), a regularized version of canonical correlation analysis.","category_a":"","category_b":"Others","keywords":["Classification","Description","Digit structure","Executable compression","Extractors","Feature extraction","Initialization (programming)","MNIST database","Machine learning","Neural coding","Sparse","Sparse approximation","Sparse matrix","Statistical classification","Supervised learning","Teh","Unsupervised learning"],"vec":[-0.7978832749,-0.5978633474],"nodes":["40632605","1761978","1724875","1751762"]},"0aa8c4caef4ad23da2e1def1f22b74badd5be4e2":{"id":"0aa8c4caef4ad23da2e1def1f22b74badd5be4e2","date":"2012-01-01T12:12:00Z","text":"Recent years have witnessed an unprecedented proliferation of social media. People around the globe author, everyday, millions of blog posts, social network status updates, etc. This rich stream of information can be used to identify, on an ongoing basis, emerging stories, and events that capture popular attention. Stories can be identified via groups of tightly coupled real-world entities, namely the people, locations, products, etc, that are involved in the story. The sheer scale and rapid evolution of the data involved necessitate highly efficient techniques for identifying important stories at every point of time. The main challenge in real-time story identification is the maintenance of dense subgraphs (corresponding to groups of tightly coupled entities) under streaming edge weight updates (resulting from a stream of user-generated content). This is the first work to study the efficient maintenance of dense subgraphs under such streaming edge weight updates. For a wide range of definitions of density, we derive theoretical results regarding the magnitude of change that a single edge weight update can cause. Based on these, we propose a novel algorithm, DynDens, which outperforms adaptations of existing techniques to this setting and yields meaningful, intuitive results. Our approach is validated by a thorough experimental evaluation on large-scale real and synthetic datasets.","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Blog","Dense subgraph","Entity","Social media","Social network","Synthetic data","User-generated content"],"vec":[0.3976663706,0.1303940974],"nodes":["1761975","1721062","1796624","1704011"]},"cab20ae17048106a84f14a6209f0286609cfa542":{"id":"cab20ae17048106a84f14a6209f0286609cfa542","date":"2017-01-01T12:12:00Z","text":"The Intel Science and Technology Center for Big Data is developing a reference implementation of a Polystore database. The BigDAWG (Big Data Working Group) system supports \u201cmany sizes\u201d of database engines, multiple programming languages and complex analytics for a variety of workloads. Our recent efforts include application of BigDAWG to an ocean metagenomics problem and containerization of BigDAWG. We intend to release an open source BigDAWG v1.0 in the Spring of 2017. In this presentation, we will demonstrate a number of polystore applications developed with oceanographic researchers at MIT and describe our forthcoming open source release of the BigDAWG system.","category_a":"ArXiv","category_b":"Journal","keywords":["Big data","Metagenomics","Open-source software","Programming language","Reference implementation"],"vec":[0.6489577937,-0.4482211035],"nodes":["39425362","1867298","38449161","7485473","1787375","3257323","2033016","2682150","7946702","1695715"]},"aad2c96c46064604c5ef729a491a47d12727ae9e":{"id":"aad2c96c46064604c5ef729a491a47d12727ae9e","date":"2004-01-01T12:12:00Z","text":"The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992a; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine psychological issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is evaluated (van Zaanen, 2000; Clark, 2001; Klein and Manning, 2002). This paper falls into the latter category; we will be inducing models of linguistic constituency and dependency with the goal of recovering linguistically plausible structures. We make no claims as to the congitive plausibility of the induction mechanisms we present here, however the ability of these systems to recover substantial linguistic patterns from surface yields alone does speak to the strength of support for these patterns in the data, and hence to undermine arguments based on \u201cthe poverty of the stimulus\u201d (Chomsky, 1965). 2 Distributional Syntax Induction","category_a":"","category_b":"Others","keywords":["Carroll Morgan (computer scientist)","Entity\u2013relationship model","Equivalence (formal languages)","Languages","Linguistics","Mathematical induction","Natural language","Plausibility structure","Stage level 1","Steve Omohundro","Treebank","emotional dependency","sentence"],"vec":[-0.4324450041,-0.5288676752],"nodes":["38666915","1812612"]},"1826fbbf12e399024296c1dfcc5758c1c810574c":{"id":"1826fbbf12e399024296c1dfcc5758c1c810574c","date":"2004-01-01T12:12:00Z","text":"We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the model\u2019s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.","category_a":"EMNLP","category_b":"ComLing","keywords":["Algorithm","Baseline (configuration management)","Computational complexity theory","Context-free grammar","Context-free language","Eisenstein's criterion","Parsing","Stochastic context-free grammar","Support vector machine"],"vec":[-0.3134212549,-0.4584715966],"nodes":["1685978","38666915","1806149","1736370","1812612"]},"2a0cec7f0f8b63f182ea0c52cb935580acabafcc":{"id":"2a0cec7f0f8b63f182ea0c52cb935580acabafcc","date":"2016-01-01T12:12:00Z","text":"In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read\/write mechanisms. We investigate the mechanisms and effects for learning to read and write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.","category_a":"","category_b":"Others","keywords":["Addressing scheme","Experiment","Feedforward neural network","MNIST database","Memory address","Memory cell (binary)","Non-deterministic Turing machine","Nonlinear system","Turing machine","Turing test","X86"],"vec":[-0.291140318,1.3999168861],"nodes":["35301496","35715776","1979489","1751762"]},"ceef5d5b82b822308a07998f19c6cca7ce0a80a6":{"id":"ceef5d5b82b822308a07998f19c6cca7ce0a80a6","date":"2018-01-01T12:12:00Z","text":"We extend the neural Turing machine (NTM) model into a dynamic neural Turing machine (D-NTM) by introducing trainable address vectors. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies, including both linear and nonlinear ones. We implement the D-NTM with both continuous and discrete read and write mechanisms. We investigate the mechanisms and effects of learning to read and write into a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU controller. We provide extensive analysis of our model and compare different variations of neural Turing machines on this task. We show that our model outperforms long short-term memory and NTM variants. We provide further experimental results on the sequential [Formula: see text]MNIST, Stanford Natural Language Inference, associative recall, and copy tasks.","category_a":"Neural computation","category_b":"Others","keywords":["Cloning Vectors","Controllers","Inference"],"vec":[-0.290576678,1.4013600653],"nodes":["35301496","35715776","1979489","1751762"]},"4b2283dd97a853812ca053547b4e067b2fa54512":{"id":"4b2283dd97a853812ca053547b4e067b2fa54512","date":"2002-01-01T12:12:00Z","text":"Matching Pursuit algorithms learn a function that is a weighted sum of basis functions, by sequentially appending functions to an initially empty basis, to approximate a target function in the least-squares sense. We show how matching pursuit can be extended to use non-squared error loss functions, and how it can be used to build kernel-based solutions to machine learning problems, while keeping control of the sparsity of the solution. We present a version of the algorithm that makes an optimal choice of both the next basis and the weights of all the previously chosen bases. Finally, links to boosting algorithms and RBF training procedures, as well as an extensive experimental comparison with SVMs for classification are given, showing comparable results with typically much sparser models.","category_a":"Machine Learning","category_b":"Others","keywords":["Algorithm","Approximation algorithm","Basis function","Kernel (operating system)","Least squares","Loss function","Machine learning","Matching pursuit","Mean squared error","Radial basis function","Sparse matrix","Weight function"],"vec":[-0.3136667877,0.286615618],"nodes":["1724875","1751762"]},"25a782a03c249d315b04a68cb1a3663ff8c893d5":{"id":"25a782a03c249d315b04a68cb1a3663ff8c893d5","date":"2003-01-01T12:12:00Z","text":"A plethora of data sources contain data entities that could be ordered according to a variety of attributes associated with the entities. Such orderings result effectively in a ranking of the entities according to the values in the attribute domain. Commonly, users correlate such sources for query processing purposes through join operations. In query processing, it is desirable to incorporate user preferences towards specific attributes or their values. A way to incorporate such preferences, is by utilizing scoring functions that combine user preferences and attribute values and return a numerical score for each tuple in the join result. Then, a target query, which we refer to as top-k join query, seeks to identify the tuples in the join result with the highest scores. In this paper, we propose a novel technique, which we refer to as ranked join index, to efficiently answer top-k join queries for arbitrary, user specified, preferences and a large class of scoring functions. Our rank join index requires small space (compared to the entire join result) and provides guarantees for its performance. Moreover, our proposal provides a graceful tradeoff between its space requirements and worst case search performance. We supplement our analytical results, with a thorough experimental evaluation using a variety of real and synthetic data sets, demonstrating that in comparison to other viable approaches, our technique offers significant performance benefits.","category_a":"ICDE","category_b":"DB","keywords":["Attribute domain","Best, worst and average case","Database","Entity","Join (SQL)","Numerical analysis","Relational algebra","Scoring functions for docking","Synthetic data","User (computing)","Value (ethics)"],"vec":[1.1435705831,0.0861009252],"nodes":["1701195","1725167","1751420","1721062","1704011"]},"654a3e53fb41d8168798ee0ee61dfab73739b1ed":{"id":"654a3e53fb41d8168798ee0ee61dfab73739b1ed","date":"2015-01-01T12:12:00Z","text":"Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. In this paper we focus on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description, and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.","category_a":"Multimedia","category_b":"Others","keywords":["Artificial neural network","Convolutional neural network","Deep learning","Encoder","Foreach loop","Input\/output","Machine translation","Recurrent neural network","Speech recognition","Video clip"],"vec":[-0.1084577976,0.0276538663],"nodes":["1979489","1760871","1751762"]},"8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac":{"id":"8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac","date":"2013-01-01T12:12:00Z","text":"We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout\u2019s fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR100, and SVHN.","category_a":"ICML","category_b":"ML","keywords":["Approximation algorithm","Benchmark (computing)","Dropout (neural networks)","MNIST database","Program optimization"],"vec":[-0.4516527163,0.3861621234],"nodes":["34740554","1923596","1778734","1760871","1751762"]},"0b98d8f90c57c5aabf2bc264d47730cb2dfc897f":{"id":"0b98d8f90c57c5aabf2bc264d47730cb2dfc897f","date":"2002-01-01T12:12:00Z","text":"The similarity between objects is a fundamental element of many learning algorithms. Most non-parametric methods take this similarity to be fixed, but much recent work has shown the advantages of learning it, in particular to exploit the local invariances in the data or to capture the possibly non-linear manifold on which most of the data lies. We propose a new non-parametric kernel density estimation method which captures the local structure of an underlying manifold through the leading eigenvectors of regularized local covariance matrices. Experiments in density estimation show significant improvements with respect to Parzen density estimators. The density estimators can also be used within Bayes classifiers, yielding classification rates similar to SVMs and much superior to the Parzen classifier.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Experiment","Kernel density estimation","Machine learning","Microsoft Windows","Nonlinear system"],"vec":[-0.4460463506,0.4770482824],"nodes":["1724875","1751762"]},"2f207e27cad923b81253542a6f76439d49d87925":{"id":"2f207e27cad923b81253542a6f76439d49d87925","date":"2015-01-01T12:12:00Z","text":"GPS-enabled devices are now ubiquitous, from airplanes and cars to smartphones and wearable technology. This has resulted in a wealth of data about the movements of individuals and populations, which can be analyzed for useful information to aid in city and traffic planning, disaster preparedness and so on. However, the places that people go can disclose extremely sensitive information about them, and thus their use needs to be filtered through privacy preserving mechanisms. This turns out to be a highly challenging task: raw trajectories are highly detailed, and typically no pair is alike. Previous attempts fail either to provide adequate privacy protection, or to remain sufficiently faithful to the original behavior. This paper presents DPT, a system to synthesize mobility data based on raw GPS trajectories of individuals while ensuring strong privacy protection in the form of \u03b5-differential privacy. DPT makes a number of novel modeling and algorithmic contributions including (i) discretization of raw trajectories using hierarchical reference systems (at multiple resolutions) to capture individual movements at differing speeds, (ii) adaptive mechanisms to select a small set of reference systems and construct prefix tree counts privately, and (iii) use of direction-weighted sampling for improved utility. While there have been prior attempts to solve the subproblems required to generate synthetic trajectories, to the best of our knowledge, ours is the first system that provides an end-to-end solution. We show the efficacy of our synthetic trajectory generation system using an extensive empirical evaluation.","category_a":"VLDB","category_b":"DB","keywords":["Discretization","Electron mobility","End-to-end principle","GPS navigation device","Information sensitivity","Population","Privacy","Sampling (signal processing)","Smartphone","Synthetic data","Trajectory optimization","Trie","Wearable technology"],"vec":[-0.0601333513,0.137812181],"nodes":["2950127","1709589","2357165","1738196","1704011"]},"5b88525bca03b88dfffc6e478127ddd72518d813":{"id":"5b88525bca03b88dfffc6e478127ddd72518d813","date":"2003-01-01T12:12:00Z","text":"An organization\u2019s data records are often noisy because of transcription errors, incomplete information, lack of standard formats for textual data or combinations thereof. A fundamental task in a data cleaning system is matching textual attributes that refer to the same entity (e.g., organization name or address). This matching can be effectively performed via the cosine similarity metric from the information retrieval field. For robustness and scalability, these \u201ctext joins\u201d are best done inside an RDBMS, which is where the data is likely to reside. Unfortunately, computing an exact answer to a text join can be expensive. In this paper, we propose an approximate, samplingbased text join execution strategy that can be robustly executed in a standard, unmodified RDBMS.","category_a":"ICDE","category_b":"DB","keywords":["Approximation algorithm","Cosine similarity","Information retrieval","Medical transcription","Scalability","Sputter cleaning","Text corpus"],"vec":[0.9853379657,0.1120281807],"nodes":["1684012","2942126","1721062","1704011"]},"dfd790fd02afea1de60cba77a3b6b98640bea491":{"id":"dfd790fd02afea1de60cba77a3b6b98640bea491","date":"2001-01-01T12:12:00Z","text":"As databases have expanded in scope to storing string data (XML documents, product catalogs), it has become increasingly important to search databases based on matching substrings, often on multiple, correlated dimensions. While string B-trees are I\/O optimal in one dimension, no index structure with non-trivial query bounds is known for two-dimensional substring indexing.\nIn this paper, we present a technique for two-dimensional substring indexing based on a reduction to the geometric problem of identifying common colors in two ranges containing colored points. We develop an I\/O efficient algorithm for solving the common colors problem, and use it to obtain an I\/O efficient (poly-logarithmic query time) algorithm for the two-dimensional substring indexing problem. Our techniques result in a family of secondary memory index structures that trade space for time, with no loss of accuracy. We show how our technique can be practically realized using a combination of string B-trees and R-trees.","category_a":"PODS","category_b":"Others","keywords":["Algorithm","Auxiliary memory","B-tree","Color","Database","P (complexity)","R-tree","Substring","XML"],"vec":[1.1207001389,0.0689531558],"nodes":["1681278","1721062","1711192","1704011"]},"5158991247802f372b1646ef8751db46c307a2c8":{"id":"5158991247802f372b1646ef8751db46c307a2c8","date":"2003-01-01T12:12:00Z","text":"We discuss two named-entity recognition models which use characters and character -grams either exclusively or as an important part of their data representation. The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features. Our best model achieves an overall F of 86.07% on the English test data (92.31% on the development data). This number represents a 25% error reduction over the same model without word-internal (substring) features.","category_a":"CoNLL","category_b":"Others","keywords":["Data (computing)","Hidden Markov model","Markov model","Maximum-entropy Markov model","Named entity","Named-entity recognition","Substring","Test data"],"vec":[0.0804834961,-0.1467656263],"nodes":["38666915","1855302","2623915","1812612"]},"2cb3bd0cff91c0afcbfb2cb10ce30313e0f70133":{"id":"2cb3bd0cff91c0afcbfb2cb10ce30313e0f70133","date":"2016-01-01T12:12:00Z","text":"We propose a structured prediction architecture, which exploits the local generic features extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed architecture, called ReSeg, is based on the recently introduced ReNet model for image classification. We modify and extend it to perform the more challenging task of semantic segmentation. Each ReNet layer is composed of four RNN that sweep the image horizontally and vertically in both directions, encoding patches or activations, and providing relevant global information. Moreover, ReNet layers are stacked on top of pre-trained convolutional layers, benefiting from generic local features. Upsampling layers follow ReNet layers to recover the original image resolution in the final predictions. The proposed ReSeg architecture is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse, Oxford Flower, and CamVid, achieving stateof-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applications in other structured prediction problems. The source code and model hyperparameters are available on https:\/\/github.com\/fvisin\/reseg.","category_a":"CVPR","category_b":"ComVis","keywords":["Artificial neural network","Computer vision","Convolutional neural network","Image resolution","Neural Networks","Recurrent neural network","Structured prediction","Upsampling"],"vec":[-0.5527527868,0.3379781292],"nodes":["2077146","3995639","1979489","1745043","34796161","2182706","1751762","1760871"]},"16a333a43d587802f95b5ec11de6c99314ae0c77":{"id":"16a333a43d587802f95b5ec11de6c99314ae0c77","date":"2012-01-01T12:12:00Z","text":"We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models\u2019 Transfer Learning Challenge.","category_a":"ICML","category_b":"ML","keywords":["Boltzmann machine","Feature learning","Graphics processing unit","Latent variable","NIPS","Neural coding","Outline of object recognition","Restricted Boltzmann machine","Slab allocation","Sparse matrix","Supervised learning","The Spike (1997)"],"vec":[-0.5096293153,0.5782440413],"nodes":["34740554","1760871","1751762"]},"0a82cb606e561ca6f43697ca4df4f449b82ddba6":{"id":"0a82cb606e561ca6f43697ca4df4f449b82ddba6","date":"2014-01-01T12:12:00Z","text":"Restricted Boltzmann Machines (RBMs) are one of the fundamental building blocks of deep learning. Approximate maximum likelihood training of RBMs typically necessitates sampling from these models. In many training scenarios, computationally efficient Gibbs sampling procedures are crippled by poor mixing. In this work we propose a novel method of sampling from Boltzmann machines that demonstrates a computationally efficient way to promote mixing. Our approach leverages an under-appreciated property of deep generative models such as the Deep Belief Network (DBN), where Gibbs sampling from deeper levels of the latent variable hierarchy results in dramatically increased ergodicity. Our approach is thus to train an auxiliary latent hierarchical model, based on the DBN. When used in conjunction with parallel-tempering, the method is asymptotically guaranteed to simulate samples from the target RBM. Experimental results confirm the effectiveness of this sampling strategy in the context of RBM training.","category_a":"ArXiv","category_b":"Journal","keywords":["Computational complexity theory","Deep belief network","Deep learning","Ergodicity","Gibbs sampling","Hierarchical database model","Latent variable","Parallel tempering","Restricted Boltzmann machine","Sampling (signal processing)"],"vec":[-0.2902510285,0.49476148],"nodes":["2755582","37232249","1760871","1751762"]},"2b35a5bccb678532412bcb471c16d6208353cf62":{"id":"2b35a5bccb678532412bcb471c16d6208353cf62","date":"2010-01-01T12:12:00Z","text":"Restricted Boltzmann Machines (RBM) have attracted a lot of attention of late, as one the principle building blocks of deep networks. Training RBMs remains problematic however, because of the intractibility of their partition function. The maximum likelihood gradient requires a very robust sampler which can accurately sample from the model despite the loss of ergodicity often incurred during learning. While using Parallel Tempering in the negative phase of Stochastic Maximum Likelihood (SML-PT) helps address the issue, it imposes a trade-off between computational complexity and high ergodicity, and requires careful hand-tuning of the temperatures. In this paper, we show that this trade-off is unnecessary. The choice of optimal temperatures can be automated by minimizing average return time (a concept first proposed by (Katzgraber et al., 2006)) while chains can be spawned dynamically, as needed, thus minimizing the computational overhead. We show on a synthetic dataset, that this results in better likelihood scores.","category_a":"ArXiv","category_b":"Journal","keywords":["Computational complexity theory","Ergodicity","Gradient","Overhead (computing)","Parallel tempering","Partition function (mathematics)","Restricted Boltzmann machine","Sampling (signal processing)","Synthetic data"],"vec":[-0.2246522359,0.6062224041],"nodes":["2755582","1760871","1751762"]},"16cd112b8f3283a5f915b57c3e56ad10ffb45b09":{"id":"16cd112b8f3283a5f915b57c3e56ad10ffb45b09","date":"2009-01-01T12:12:00Z","text":"The recent rise in popularity of social networks, such as Facebook and MySpace, has created large quantities of data about interactions within these networks. Such data contains many private details about individuals so anonymization is required prior to attempts to make the data more widely available for scientific research. Prior work has considered simple graph data to be anonymized by removing all non-graph information and adding or deleting some edges. Since social network data is richer in details about the users and their interactions, loss of details due to anonymization limits the possibility for analysis. We present a new set of techniques for anonymizing social network data based on grouping the entities into classes, and masking the mapping between entities and the nodes that represent them in the anonymized graph. Our techniques allow queries over the rich data to be evaluated with high accuracy while guaranteeing resilience to certain types of attack. To prevent inference of interactions, we rely on a critical \u201csafety condition\u201d when forming these classes. We demonstrate utility via empirical data from social networking settings. We give examples of complex queries that may be posed and show that they can be answered over the anonymized data efficiently and accurately.","category_a":"VLDB","category_b":"DB","keywords":["Data anonymization","Entity","Graph (discrete mathematics)","Interaction","Social network"],"vec":[1.2008754077,0.6210089252],"nodes":["1709589","1704011","1738297","1778687"]},"437b328a1a62eeb85aaa87514a13010a4df315ee":{"id":"437b328a1a62eeb85aaa87514a13010a4df315ee","date":"1996-01-01T12:12:00Z","text":"We present novel algorithms for the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. In addition to its obvious potential in query optimization, this problem is important in many applications, such as data warehousing, very large transaction recording systems, global information systems and mobile computing, where access to local or cached materialized views may be cheaper than access to the underlying database. Our contributions are the following: First, we show that in the case where the query has grouping and aggregation but the views do not, a view is usable in answering a query only if there is an isomorphism between the view and a portion of the query. Second, when the views also have grouping and aggregation we identify conditions under which the aggregation information present in a view is sufficient to perform the aggregation computations required in the query. The algorithms we describe for rewriting a query also consider the case in which the rewritten query may be a union of single-block queries. Our approach is a semantic one, in that it detects when the information existing in a view is sufficient to answer a query. In contrast, previous work performed syntactic transformations on the query such that the definition of the view would be a sub-part of the definition of the query. Consequently, these methods can only detect usages of views in limited cases.","category_a":"VLDB","category_b":"DB","keywords":["Algorithm","Computation","Global information system","Information system","Materialized view","Mobile computing","Program optimization","Query optimization","Rewriting","SQL"],"vec":[0.9829053219,0.0774085017],"nodes":["1704011","35612310","1735239","1770962"]},"237202f570c6990f0c33c4db17b5df61d5ee6899":{"id":"237202f570c6990f0c33c4db17b5df61d5ee6899","date":"2015-01-01T12:12:00Z","text":"Recent works on end-to-end neural network-based architectures for machine translation have shown promising results for English-French and English-German translation. Unlike these language pairs, however, in the majority of scenarios, there is a lack of high quality parallel corpora. In this work, we focus on applying neural machine translation to challenging\/low-resource languages Turkish and low-resource domains such as parallel corpora of Chinese chat messages. In particular, we investigated how to leverage abundant monolingual data for these low-resource translation tasks. Without the use of external alignment tools, we obtained up to a 1.96 BLEU score improvement with our proposed method compared to the previous best result in Turkish-to-English translation on the IWLST 2014 dataset. On Chinese-toEnglish translation by using the OpenMT 2015 dataset, we were able to obtain up to a 1.59 BLEU score improvement over phrase-based and hierarchical phrase-based baselines.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","BLEU","Display resolution","End-to-end principle","Machine translation","Neural machine translation","Parallel text","Text corpus"],"vec":[-1.0003888225,-0.0282456012],"nodes":["1854385","2345617","36303818","1979489","2934336","2232222","2076086","1680886","1751762"]},"36818eaf6376aeeaffed2523d28bebae7c9db8d7":{"id":"36818eaf6376aeeaffed2523d28bebae7c9db8d7","date":"2017-01-01T12:12:00Z","text":"Sequential data often possesses hierarchical structures with complex dependencies between sub-sequences, such as found between the utterances in a dialogue. To model these dependencies in a generative framework, we propose a neural networkbased generative architecture, with stochastic latent variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with other recent neural-network architectures. We evaluate the model performance through a human evaluation study. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate both the generation of meaningful, long and diverse responses and maintaining dialogue state.","category_a":"AAAI","category_b":"AI","keywords":["Artificial neural network","Encoder","Experiment","Latent variable","Span and div"],"vec":[-0.4432551497,0.4285106207],"nodes":["35224828","2041695","2054294","1778839","1723772","1760871","1751762"]},"30e2c349636920f96e72882e2e2c1d22b4739127":{"id":"30e2c349636920f96e72882e2e2c1d22b4739127","date":"2016-01-01T12:12:00Z","text":"Human: Is that mole developing on the face leads to some risk towards the skin? Mirror: Yes. Human: I don\u2019t know if I was not aware of the fact until now. Mirror: Shall I go ahead and fixup an appointment with the dermatologist on this weekend: Saturday 11am? Mirror: Sounds Good! Please check your email the appointment confirmation. In this paper we propose a system (a health mirror) an application of internet of health (IOH)","category_a":"","category_b":"Others","keywords":["Augmented reality","Convolutional neural network","Email","Human factors and ergonomics","Malignant neoplasm of skin","Nevus","Sound - physical agent","melanoma"],"vec":[-0.111731161,-0.1389687454],"nodes":["40522962","19094822","1695689","1701686","37748887","39978391","3142556","36923789","1838674","1761978","2657155","1805076","24792872","1893833","1694199","2143297","2354728","3216322","2319608","2192178","8342699","2888806","2285165","2638806","35928019","3109481","2556428","5968520","39668247",0]},"0ed7adf241b11c34fe1354240a1b3cd9cd8a3705":{"id":"0ed7adf241b11c34fe1354240a1b3cd9cd8a3705","date":"2003-01-01T12:12:00Z","text":"In [GIJ01a, GIJ01b] we described how to use q-grams in an RDBMS to perform approximate string joins. We also showed how to implement the approximate join using plain SQL queries. Specifically, we described three filters, count filter, position filter, and length filter, which can be used to execute efficiently the approximate join. The intuition behind the count filter was that strings that are similar have many q-grams in common. In particular, two strings s1 and s2 can have up to max{|s1|, |s2|} + q \u2212 1 common q-grams. When s1 = s2, they have exactly that many q-grams in common. When s1 and s2 are within edit distance k, they share at least (max{|s1|, |s2|} + q \u2212 1) \u2212 kq q-grams, since kq is the maximum number of q-grams that can be affected by k edit distance operations. We implemented count filter in the HAVING clause of the SQL statement in Figure 1. String pairs without enough q-grams in common are filtered out from the result. Unfortunately, this implementation of the count filter is problematic when kq is greater than or equal to max{|s1|, |s2|} + q \u2212 1. In this case, two strings can be within edit distance k and still not share any q-grams. In such a case, the SQL statement in Figure 1 will fail to identify s1 and s2 as being within edit distance k, since there will be no q-grams from this string pair to join and count. Hence, in this case the result returned by the Figure 1 query is incomplete and suffers from \u201cfalse negatives,\u201d in contrast to our claim to the contrary in [GIJ01a, GIJ01b]. In general, the string pairs that are omitted are pairs of short strings. Even when these strings match within small edit distance, the match tends to be meaningless (e.g., \u201cIBM\u201d matches \u201cACM\u201d within edit distance 2). However, when it is absolutely necessary to have no false negatives, we can make the appropriate modifications to the SQL query in Figure 1 so that it produces the correct results. Since the false negatives are only pairs of short strings, we can join all pairs of these small strings, using only the length filter, and UNION the result with the result of the SQL query described in [GIJ01a, GIJ01b]. We list the modified query in Figure 2.","category_a":"","category_b":"Others","keywords":["Aclarubicin","Approximation algorithm","Chamaecyparis lawsoniana","Edit distance","Grams","Inclusion Body Myositis (disorder)","Intuition","Mental Suffering","Question (inquiry)","SQL","Select (SQL)","Structured Query Language","gram"],"vec":[0.0447235228,-0.0228445872],"nodes":["1684012","2942126","1735239","1721062","1711192","1704011"]},"3b2697d76f035304bfeb57f6a682224c87645065":{"id":"3b2697d76f035304bfeb57f6a682224c87645065","date":"2015-01-01T12:12:00Z","text":"The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5\u00a0years of the challenge, and propose future directions and improvements.","category_a":"Journal of Computer Vision","category_b":"Others","keywords":["Benchmark (computing)","Computer vision","Ground truth","ImageNet","Object detection","Outline of object recognition"],"vec":[-0.2164621751,-0.0651115141],"nodes":["2192178","8342699","2888806","2285165","2638806","35928019","3109481","2354728","2556428","35609041","39668247","3216322"]},"91277a62af159aa44e03482711149658576cae36":{"id":"91277a62af159aa44e03482711149658576cae36","date":"1998-01-01T12:12:00Z","text":"Lists of entities must often be speciied in many real-world applications such as customer lists, electronic distribution lists and access control lists. These lists are typically spec-iied through explicit enumeration, frequently aided by re-cursive expansion. In this paper, we discuss the declarative speciication and extraction of members of such lists as queries over a directory that maintains information both about individuals and about lists, and identify key features that the directory must support to manage lists in a exible manner. X.500 is the industry standard for modeling information about individuals in a directory, and LDAP is the proposed standard for accessing directory information. We have designed and built a system to represent and manage lists in the X.500 information model, and developed ee-ciently evaluable extensions to the LDAP query language for the location and expansion of lists. We describe the system architecture and the query evaluation algorithm of this system. Our system is deployed for use in the speciication and expansion of (organizational and personal) electronic messaging (e-mail, voice mail and FAX) distribution lists at AT&T Labs. 1 Motivation Lists of entities must often be speciied in many real-world applications such as prospective customer lists, lists of network resources, manufacturing parts lists, electronic distribution lists, and access control lists. These lists are typically speciied through explicit enumeration, frequently aided by recursive expansion: Prospective Customer Lists: Companies often use customer lists to market their products and services to targeted lists of prospective customers. For example, a pharmaceutical company may maintain a prospective customer list doctors, that includes addresses of doctors to whom it sends promotional mailings, and samples of new medicines. Electronic Distribution Lists: Distribution lists are the primary mechanism of sending an e-mail message to an intended list of recipients, without the sender having to know or enumerate explicitly their individual electronic mail addresses. For example, the distribution list dbworld maintained at the Computer Sciences Department at the University of Wisconsin{Madison includes most researchers in the world interested in databases. Access Control Lists: Secure systems often have lists of users with speciic access rights. For example, in the UNIX le system, read\/write\/execute access rights to individual documents associated with the HighFlier project of a corporation can be provided to members of the highflier access control list (or group), allowing for controlled sharing of information. The utility of these lists would be considerably enhanced if they could be used to specify \u2026","category_a":"CIKM","category_b":"Others","keywords":["Access control","Access control list","Algorithm","Database","Digital distribution","Email","Entity","Enumerated type","Fax","Information model","Lightweight Directory Access Protocol","Prospective search","Query language","Recursion","Systems architecture","Technical standard","Unix","X.500"],"vec":[0.0460530173,-0.0291215644],"nodes":["1735239","39344484","1704011","1778524"]},"683422744d6a8f5e6bac38756f1f51a8ed16e2e3":{"id":"683422744d6a8f5e6bac38756f1f51a8ed16e2e3","date":"2005-01-01T12:12:00Z","text":"We present a prototype system, SPIDER, developed at AT&amp;T Labs-Research, which supports flexible string attribute value matching in large databases. We discuss the design principles on which SPIDER is based, describe the basic techniques encompassed by the tool and provide a description of the demo.","category_a":"SIGMOD","category_b":"DB","keywords":["Database","Demo (computer programming)","The Mother of All Demos"],"vec":[0.8639148656,0.0473140253],"nodes":["1721062","2440762","1704011"]},"3e4290774195402d31661d3c6b1f1da1bc55f5ae":{"id":"3e4290774195402d31661d3c6b1f1da1bc55f5ae","date":"2007-01-01T12:12:00Z","text":"Declarative data quality has been an active research topic. The fundamental principle behind a declarative approach to data quality is the use of declarative statements to realize data quality primitives on top of any relational data source. A primary advantage of such an approach is the ease of use and integration with existing applications. Over the last few years several similarity predicates have been proposed for common quality primitives (approximate selections, joins, etc) and have been fully expressed using declarative SQL statements. In this paper we propose new similarity predicates along with their declarative realization, based on notions of probabilistic information retrieval. In particular we show how language models and hidden Markov models can be utilized as similarity predicates for data quality and present their full declarative instantiation. We also show how other scoring methods from information retrieval, can be utilized in a similar setting. We then present full declarative specifications of previously proposed similarity predicates in the literature, grouping them into classes according to their primary characteristics. Finally, we present a thorough performance and accuracy study comparing a large number of similarity predicates for data cleaning operations. We quantify both their runtime performance as well as their accuracy for several types of common quality problems encountered in operational databases.","category_a":"SIGMOD","category_b":"DB","keywords":["Approximation algorithm","Data quality","Database","Declarative programming","Hidden Markov model","Information retrieval","Language model","Program optimization","SQL","Sputter cleaning","Universal instantiation","Usability"],"vec":[0.9859967459,0.1092911317],"nodes":["36206298","1728091","1721062","1719119","1704011"]},"95a4edd3b7967da6c39bea61dc846052cd3a4837":{"id":"95a4edd3b7967da6c39bea61dc846052cd3a4837","date":"2013-01-01T12:12:00Z","text":"","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Deep learning"],"vec":[-0.1899234326,0.000680411],"nodes":["1854385","1979489","1996134","1751762"]},"4fadcd3a0cb63edd255383f41ed3e280495333b2":{"id":"4fadcd3a0cb63edd255383f41ed3e280495333b2","date":"2014-01-01T12:12:00Z","text":"Graph analytics is getting increasingly popular these days and there is a deluge of new systems for graph analytics. However, it is not clear how good or bad are the relational databases for graph analytics. In this talk, I will share our experiences with graph analytics on relational databases. Contrary to the popular belief, modern relational databases can have very good performance over graph analytics. Furthermore, we can offer better (and efficient) programming interfaces for expressing graph queries in relational databases, thereby not forcing the users to SQL.","category_a":"","category_b":"Others","keywords":["Database","Experience","Graph - visual representation","Published Database","Relational database","SQL","Structured Query Language"],"vec":[0.6119078848,-0.3915997012],"nodes":["2153832","2033016","2313625","1695715"]},"0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3":{"id":"0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3","date":"2010-01-01T12:12:00Z","text":"We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of fixations and it must combine the \u201cglimpse\u201d at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classification datasets, showing that it can perform at least as well as a model trained on whole images.","category_a":"NIPS","category_b":"ML","keywords":["Boltzmann machine","Computer vision","Image resolution","Pixel","Synthetic data"],"vec":[-0.0790880749,-0.0723837296],"nodes":["1777528","1695689"]},"0375dbfedebc209b1f276f629c8958d37b48cac6":{"id":"0375dbfedebc209b1f276f629c8958d37b48cac6","date":"2016-01-01T12:12:00Z","text":"Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.","category_a":"ArXiv","category_b":"Journal","keywords":["Artificial neural network","Backpropagation","Cache (computing)","Computation","End-to-end principle","Question answering","Reinforcement learning","Scalability","Softmax function"],"vec":[0.3244248515,0.0028810049],"nodes":["2251963","3103594","1777528","1724875","1699108","1751762"]},"dd24d28c60d432cf95f5b8a64ba3dec75fcf93b7":{"id":"dd24d28c60d432cf95f5b8a64ba3dec75fcf93b7","date":"2011-01-01T12:12:00Z","text":"With the advent of crowdsourcing services it has become quite cheap and reason-<lb>ably effective to get a dataset labeled by multiple annotators in a short amount of<lb>time. Various methods have been proposed to estimate the consensus labels by<lb>correcting for the bias of annotators with different kinds of expertise. Often we<lb>have low quality annotators or spammers\u2013annotators who assign labels randomly<lb>(e.g., without actually looking at the instance). Spammers can make the cost of<lb>acquiring labels very expensive and can potentially degrade the quality of the con-<lb>sensus labels. In this paper we formalize the notion of a spammer and define<lb>a score which can be used to rank the annotators\u2014with the spammers having a<lb>score close to zero and the good annotators having a high score close to one. 1 Spammers in crowdsourced labeling tasks Annotating an unlabeled dataset is one of the bottlenecks in using supervised learning to build good<lb>predictive models. Getting a dataset labeled by experts can be expensive and time consuming. With<lb>the advent of crowdsourcing services (Amazon\u2019s Mechanical Turk being a prime example) it has<lb>become quite easy and inexpensive to acquire labels from a large number of annotators in a short<lb>amount of time (see [8], [10], and [11] for some computer vision and natural language processing<lb>case studies). One drawback of most crowdsourcing services is that we do not have tight control<lb>over the quality of the annotators. The annotators can come from a diverse pool including genuine<lb>experts, novices, biased annotators, malicious annotators, and spammers. Hence in order to get good<lb>quality labels requestors typically get each instance labeled by multiple annotators and these multiple<lb>annotations are then consolidated either using a simple majority voting or more sophisticated meth-<lb>ods that model and correct for the annotator biases [3, 9, 6, 7, 14] and\/or task complexity [2, 13, 12]. In this paper we are interested in ranking annotators based on how spammer like each annotator is.<lb>In our context a spammer is a low quality annotator who assigns random labels (maybe because the<lb>annotator does not understand the labeling criteria, does not look at the instances when labeling, or<lb>maybe a bot pretending to be a human annotator). Spammers can significantly increase the cost of<lb>acquiring annotations (since they need to be paid) and at the same time decrease the accuracy of the<lb>final consensus labels. A mechanism to detect and eliminate spammers is a desirable feature for any<lb>crowdsourcing market place. For example one can give monetary bonuses to good annotators and<lb>deny payments to spammers. The main contribution of this paper is to formalize the notion of a spammer for binary, categorical,<lb>and ordinal labeling tasks. More specifically we define a scalar metric which can be used to rank the<lb>annotators\u2014with the spammers having a score close to zero and the good annotators having a score<lb>close to one (see Figure 4). We summarize the multiple parameters corresponding to each annotator<lb>into a single score indicative of how spammer like the annotator is. While this spammer score was<lb>implicit for binary labels in earlier works [3, 9, 2, 6] the extension to categorical and ordinal labels is<lb>novel and is quite different from the accuracy computed from the confusion rate matrix. An attempt<lb>to quantify the quality of the workers based on the confusion matrix was recently made by [4] where<lb>they transformed the observed labels into posterior soft labels based on the estimated confusion","category_a":"","category_b":"Others","keywords":["Amazon Mechanical Turk","Amazona","Arabic numeral 0","COMEFROM","Computer vision","Confusion","Confusion matrix","Crowdsourcing","Crowdsourcing","HLA-DQ Antigens","Labels (device)","Lb substance","Money","Money","Natural language","Natural language processing","Ordinal data","Predictive modelling","Quantitation","Silo (dataset)","Spamming","Supervised learning","The Turk","Tracer","Video game bot","Viral phylodynamics","payment"],"vec":[-0.1210250126,-0.0427593871],"nodes":["1756036","40519282","40286585","39929781",0,"1701538","1812612","4260659","29289564","2095043","1987508","40634817"]},"f59d3e260ae4c7e145b3a2732524ea74cf5deac2":{"id":"f59d3e260ae4c7e145b3a2732524ea74cf5deac2","date":"2015-01-01T12:12:00Z","text":"","category_a":"ArXiv","category_b":"Journal","keywords":[],"vec":[-0.1899583845,0.0006806696],"nodes":["1685369","2482072","1979489","1732563","1751762"]},"18758118e78ca7f908021c55196fcba12dbb6283":{"id":"18758118e78ca7f908021c55196fcba12dbb6283","date":"2014-01-01T12:12:00Z","text":"The spike-and-slab restricted Boltzmann machine (ssRBM) is defined to have both a real-valued &#x201C;slab&#x201D; variable and a binary &#x201C;spike&#x201D; variable associated with each unit in the hidden layer. The model uses its slab variables to model the conditional covariance of the observation-thought to be important in capturing the statistical properties of natural images. In this paper, we present the canonical ssRBM framework together with some extensions. These extensions highlight the flexibility of the spike-and-slab RBM as a platform for exploring more sophisticated probabilistic models of high dimensional data in general and natural image data in particular. Here, we introduce the subspace-ssRBM focused on the task of learning invariant features. We highlight the behaviour of the ssRBM and its extensions through experiments with the MNIST digit recognition task and the CIFAR-10 object classification task.","category_a":"Pattern Analysis and Machine Intelligence","category_b":"Others","keywords":["Boltzmann machine","Digit - number character","Experiment","MNIST database","Models, Statistical","Random subspace method","Restricted Boltzmann machine","Slab allocation","Sparse matrix","The Spike (1997)"],"vec":[-0.3787011427,0.4141964597],"nodes":["1760871","2755582","32837403","1751762"]},"1671a665c636bec7d2eaff137d74e9b7f074892f":{"id":"1671a665c636bec7d2eaff137d74e9b7f074892f","date":"2012-01-01T12:12:00Z","text":"Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning.","category_a":"Journal of Machine Learning Research","category_b":"Others","keywords":["Artificial neural network","Assistive technology","Boltzmann machine","Computation","Computer multitasking","Gradient","Initialization (programming)","Performance","Preprocessor","Restricted Boltzmann machine","Semi-supervised learning"],"vec":[-0.3011548513,0.4425428377],"nodes":["1777528","2035708","1996134","1751762"]},"23c2e6c185aa7f4274dc53196fa130dcd969988b":{"id":"23c2e6c185aa7f4274dc53196fa130dcd969988b","date":"2017-01-01T12:12:00Z","text":"Generative adversarial networks (GANs, Goodfellow et al., 2014) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training, and we demonstrate this on Celeba, Large-scale Scene Understanding (LSUN) bedrooms, and Imagenet without conditioning.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Boundary scan","Decision boundary","Discrete mathematics","Discriminator","Generative adversarial networks","Gradient","ImageNet","Natural language","Natural language generation","Text-based (computing)"],"vec":[-0.2575603983,0.5857903219],"nodes":["40482726","12782441","4098005","1979489","1751762"]},"05fd1da7b2e34f86ec7f010bef068717ae964332":{"id":"05fd1da7b2e34f86ec7f010bef068717ae964332","date":"2009-01-01T12:12:00Z","text":"Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.","category_a":"Journal of Machine Learning Research","category_b":"Others","keywords":["Algorithm","Artificial neural network","Autoencoder","Bayesian network","Causality","Deep belief network","Deep learning","Experiment","Generative model","Gradient","Greedy algorithm","High- and low-level","Initialization (programming)","Layer (electronics)","Matrix regularization","Maxima and minima","Neural Networks","Nonlinear system","Online and offline","Optimization problem","Program optimization","Restricted Boltzmann machine","Unsupervised learning"],"vec":[-0.3839032503,0.6356696568],"nodes":["1777528","1751762","2373952","3087941"]},"d5b5db41929d791b4e911a5c7b0e29f60ee23253":{"id":"d5b5db41929d791b4e911a5c7b0e29f60ee23253","date":"2006-01-01T12:12:00Z","text":"Data quality is a serious concern in every data management application, and a variety of quality measures have been proposed, including accuracy, freshness and completeness, to capture the common sources of data quality degradation. We identify and focus attention on a novel measure, column heterogeneity, that seeks to quantify the data quality problems that can arise when merging data from different sources. We identify desiderata that a column heterogeneity measure should intuitively satisfy, and discuss a promising direction of research to quantify database column heterogeneity based on using a novel combination of cluster entropy and soft clustering. Finally, we present a few preliminary experimental results, using diverse data sets of semantically different types, to demonstrate that this approach appears to provide a robust mechanism for identifying and quantifying database column heterogeneity.","category_a":"CleanDB","category_b":"Others","keywords":["Cluster analysis","Data quality","Database","Replay attack"],"vec":[0.9965553845,0.1063348058],"nodes":["21526180","1721062","1693070","1704011","1747652"]},"a88966cdaeddd15d0a3de365a8f0a5931aebd756":{"id":"a88966cdaeddd15d0a3de365a8f0a5931aebd756","date":"2007-01-01T12:12:00Z","text":"It has been widely observed that different NLP applications require different sense granularities in order to best exploit word sense distinctions, and that for many applications WordNet senses are too fine-grained. In contrast to previously proposed automatic methods for sense clustering, we formulate sense merging as a supervised learning problem, exploiting human-labeled sense clusterings as training data. We train a discriminative classifier over a wide variety of features derived from WordNet structure, corpus-based evidence, and evidence from other lexical resources. Our learned similarity measure outperforms previously proposed automatic methods for sense clustering on the task of predicting human sense merging judgments, yielding an absolute F-score improvement of 4.1% on nouns, 13.6% on verbs, and 4.0% on adjectives. Finally, we propose a model for clustering sense taxonomies using the outputs of our classifier, and we make available several automatically sense-clustered WordNets of various sense granularities.","category_a":"EMNLP","category_b":"ComLing","keywords":["Cluster analysis","Natural language processing","Pattern recognition","Similarity measure","Supervised learning","Test set","Word sense","WordNet"],"vec":[-0.6723121266,-0.6062432927],"nodes":["40187504","28612243","1746807","1701538"]},"1d4d14e78eb1536b84f6e851a67ecc5a6c0ae07f":{"id":"1d4d14e78eb1536b84f6e851a67ecc5a6c0ae07f","date":"2005-01-01T12:12:00Z","text":"We present a system for textual inference (the task of inferring whether a sentence follows from another text) that uses learning and a logical-formula semantic representation of the text. More precisely, our system begins by parsing and then transforming sentences into a logical formula-like representation similar to the one used by (Harabagiu et al., 2000). An abductive theorem prover then tries to find the minimum \u201ccost\u201d set of assumptions necessary to show that one statement follows from the other. These costs reflect how likely different assumptions are, and are learned automatically using information from syntactic\/semantic features and from linguistic resources such as WordNet. If one sentence follows from the other given only highly plausible, low cost assumptions, then we conclude that it can be inferred. Our approach can be viewed as combining statistical machine learning and classical logical reasoning, in the hope of marrying the robustness and scalability of learning with the preciseness and elegance of logical theorem proving. We give experimental results from the recent PASCAL RTE 2005 challenge competition on recognizing textual inferences, where a system using this inference algorithm achieved the highest confidence weighted score.","category_a":"AAAI","category_b":"AI","keywords":["Abductive reasoning","Algorithm","Automated theorem proving","Machine learning","Parsing","Pascal","Scalability","Well-formed formula","WordNet"],"vec":[-0.42618,-0.3553557554],"nodes":["2979876","1701538","1812612"]},"aa3d3c206b9d1726b9c387a00ddd2da2a9803fb6":{"id":"aa3d3c206b9d1726b9c387a00ddd2da2a9803fb6","date":"2014-01-01T12:12:00Z","text":"Training of the neural autoregressive density estimator (NADE) can be viewed as doing one step of probabilistic inference on missing values in data. We propose a new model that extends this inference scheme to multiple steps, arguing that it is easier to learn to improve a reconstruction in k steps rather than to learn to reconstruct in a single inference step. The proposed model is an unsupervised building block for deep learning that combines the desirable properties of NADE and multi-prediction training: (1) Its test likelihood can be computed analytically, (2) it is easy to generate independent samples from it, and (3) it uses an inference engine that is a superset of variational inference for Boltzmann machines. The proposed NADE-k is competitive with the state-of-the-art in density estimation on the two datasets tested.","category_a":"NIPS","category_b":"ML","keywords":["Autoregressive model","Calculus of variations","Deep learning","Inference engine","Missing data","Unsupervised learning","Value (ethics)"],"vec":[-0.3749418752,0.4323121318],"nodes":["2785022","1685369","1979489","1751762"]},"62c76ca0b2790c34e85ba1cce09d47be317c7235":{"id":"62c76ca0b2790c34e85ba1cce09d47be317c7235","date":"2013-01-01T12:12:00Z","text":"Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we \u201cback-propagate\u201d through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of conditional computation, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.","category_a":"ArXiv","category_b":"Journal","keywords":["Algorithm","Algorithmic efficiency","Approximation theory","Artificial neural network","Artificial neuron","Computation","Deep learning","Gradient","Heuristic","Image gradient","Loss function","Multiplicative noise","Neuron","Sigmoid function","Sparse matrix","Stochastic process"],"vec":[-0.3696326019,0.5157507872],"nodes":["1751762","1947598","1760871"]},"2b7ef95822a4d577021df16607bf7b4a4514eb4b":{"id":"2b7ef95822a4d577021df16607bf7b4a4514eb4b","date":"2012-01-01T12:12:00Z","text":"Recent work in unsupervised feature learning has focused on the goal of discovering high-level features from unlabeled images. Much progress has been made in this direction, but in most cases it is still standard to use a large amount of labeled data in order to construct detectors sensitive to object classes or other complex patterns in the data. In this paper, we aim to test the hypothesis that unsupervised feature learning methods, provided with only unlabeled data, can learn high-level, invariant features that are sensitive to commonly-occurring objects. Though a handful of prior results suggest that this is possible when each object class accounts for a large fraction of the data (as in many labeled datasets), it is unclear whether something similar can be accomplished when dealing with completely unlabeled data. A major obstacle to this test, however, is scale: we cannot expect to succeed with small datasets or with small numbers of learned features. Here, we propose a large-scale feature learning system that enables us to carry out this experiment, learning 150,000 features from tens of millions of unlabeled images. Based on two scalable clustering algorithms (K-means and agglomerative clustering), we find that our simple system can discover features sensitive to a commonly occurring object class (human faces) and can also combine these into detectors invariant to significant global distortions like large translations and scale.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Cluster analysis","Distortion","Emergence","Feature learning","High- and low-level","K-means clustering","Unsupervised learning"],"vec":[-0.7568189764,0.1419822143],"nodes":["5574038","2354728","1701538"]},"1e4d40e94fa57419a1236a1866a0c3ef357f8b78":{"id":"1e4d40e94fa57419a1236a1866a0c3ef357f8b78","date":"2004-01-01T12:12:00Z","text":"XML has a tree-structured data model, which is used to uniformly represent structured as well as semi-structured data, and also enable concise query specification in XQuery, via the use of its XPath (twig) patterns. This in turn can leverage the recently developed technology of structural join algorithms to evaluate the query efficiently. In this paper, we identify a fundamental tension in XML data modeling: (i) data represented as deep trees (which can make effective use of twig patterns) are often un-normalized, leading to update anomalies, while (ii) normalized data tends to be shallow, resulting in heavy use of expensive value-based joins in queries.Our solution to this data modeling problem is a novel multi-colored trees (MCT) logical data model, which is an evolutionary extension of the XML data model, and permits trees with multi-colored nodes to signify their participation in multiple hierarchies. This adds significant semantic structure to individual data nodes. We extend XQuery expressions to navigate between structurally related nodes, taking color into account, and also to create new colored trees as restructurings of an MCT database. While MCT serves as a significant evolutionary extension to XML as a logical data model, one of the key roles of XML is for information exchange. To enable exchange of MCT information, we develop algorithms for optimally serializing an MCT database as XML. We discuss alternative physical representations for MCT databases, using relational and native XML databases, and describe an implementation on top of the Timber native XML database. Experimental evaluation, using our prototype implementation, shows that not only are MCT queries\/updates more succinct and easier to express than equivalent shallow tree XML queries, but they can also be significantly more efficient to evaluate than equivalent deep and shallow tree XML queries\/updates.","category_a":"SIGMOD","category_b":"DB","keywords":["Algorithm","Data model","Data modeling","Database","Information exchange","Join (SQL)","Logical data model","Mobile data terminal","Relational algebra","Semi-structured data","Serialization","Twig","XML","XML database","XPath","XQuery"],"vec":[0.9830727818,0.0115228681],"nodes":["1735239","1708593","1789318","1704011","3301260"]},"1e9e87fc99430a82621810b3ce7db51e339be315":{"id":"1e9e87fc99430a82621810b3ce7db51e339be315","date":"2014-01-01T12:12:00Z","text":"Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DTRNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.","category_a":"TACL","category_b":"Others","keywords":["Artificial neural network","Bag-of-words model","Baseline (configuration management)","Dependency grammar","FITS","Kernel method","Recurrent neural network","The Sentence"],"vec":[-0.5639473676,-0.4930736993],"nodes":["2166511","2354728","2827616","1812612","1701538"]},"7060f6062ba1cbe9502eeaaf13779aa1664224bb":{"id":"7060f6062ba1cbe9502eeaaf13779aa1664224bb","date":"2017-01-01T12:12:00Z","text":"Microtask crowdsourcing is increasingly critical to the creation of extremely large datasets. As a result, crowd workers spend weeks or months repeating the exact same tasks, making it necessary to understand their behavior over these long periods of time. We utilize three large, longitudinal datasets of nine million annotations collected from Amazon Mechanical Turk to examine claims that workers fatigue or satisfice over these long periods, producing lower quality work. We find that, contrary to these claims, workers are extremely stable in their quality over the entire period. To understand whether workers set their quality based on the task's requirements for acceptance, we then perform an experiment where we vary the required quality for a large crowdsourcing task. Workers did not adjust their quality based on the acceptance threshold: workers who were above the threshold continued working at their usual quality level, and workers below the threshold self-selected themselves out of the task. Capitalizing on this consistency, we demonstrate that it is possible to predict workers' long-term quality using just a glimpse of their quality on the first five tasks.","category_a":"CSCW","category_b":"HCI","keywords":["Amazon Mechanical Turk","Crowdsourcing","Requirement","The Turk"],"vec":[0.397649942,-0.9269461875],"nodes":["35163655","2580593","3216322","35609041"]},"2d02141fd8c263d9c9a05d704c2d3e38525a7167":{"id":"2d02141fd8c263d9c9a05d704c2d3e38525a7167","date":"2004-01-01T12:12:00Z","text":"Semantic taxonomies such as WordNet provide a rich source of knowledge for natural language processing applications, but are expensive to build, maintain, and extend. Motivated by the problem of automatically constructing and extending such taxonomies, in this paper we present a new algorithm for automatically learning hypernym (is-a) relations from text. Our method generalizes earlier work that had relied on using small numbers of hand-crafted regular expression patterns to identify hypernym pairs. Using \u201cdependency path\u201d features extracted from parse trees, we introduce a general-purpose formalization and generalization of these patterns. Given a training set of text containing known hypernym pairs, our algorithm automatically extracts useful dependency paths and applies them to new corpora to identify novel pairs. On our evaluation task (determining whether two nouns in a news article participate in a hypernym relationship), our automatically extracted database of hypernyms attains both higher precision and higher recall than WordNet.","category_a":"NIPS","category_b":"ML","keywords":["Algorithm","Is-a","Natural language processing","Parse tree","Parsing","Regular expression","Taxonomy (general)","Text corpus","WordNet"],"vec":[-0.639961286,-0.6171556897],"nodes":["40187504","1746807","1701538"]},"02227c94dd41fe0b439e050d377b0beb5d427cda":{"id":"02227c94dd41fe0b439e050d377b0beb5d427cda","date":"2011-01-01T12:12:00Z","text":"Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.","category_a":"","category_b":"Others","keywords":["Benchmark (computing)","Computer vision","Digit structure","Feature learning","Google Street View","Human reliability","Machine learning","Optical character recognition","Tracer","Unsupervised learning","machine","photograph"],"vec":[-0.1332694406,0.1481003045],"nodes":["34180232","37446964","5574038","1726358","22401706","1701538"]},"0a5eace1ead92c5b1c47ccc89d1daf7bb5c5acd8":{"id":"0a5eace1ead92c5b1c47ccc89d1daf7bb5c5acd8","date":"2015-01-01T12:12:00Z","text":"Increasingly parallel systems promise a remedy for the current stagnation of single-core performance. However, the battle to find the most appropriate architecture for the resulting massively parallel systems is still ongoing. Currently, there are two active contenders: Massively Parallel Single Instruction Multiple Threads (SIMT) systems such as GPGPUs and Many Core Single Instruction Multiple Data (SIMD) systems such as Intel's Xeon Phi. While the former is more versatile, the latter is an efficient, time-tested technology with a clear migration path. In this study, we provide a data management perspective to the debate: we study the implementation and performance of a set of common data management operations on an SIMT device (an Nvidia GTX 780) and compare it to a Many Core SIMD system (an Intel Xeon Phi). We interpret the results to pinpoint architectural decisions and tradeoffs that lead to suboptimal performance and point out potential areas for improvement in the next generation of these devices.","category_a":"DaMoN","category_b":"Others","keywords":["GeForce 700 series","General-purpose computing on graphics processing units","Next-generation network","SIMD","Single instruction, multiple threads","Single-core","Systems design","Xeon Phi"],"vec":[0.2552423847,-0.280073839],"nodes":["2436756","2033016","1695715"]},"878634c30842b5812c56fe772719424bab69e7ad":{"id":"878634c30842b5812c56fe772719424bab69e7ad","date":"2016-01-01T12:12:00Z","text":"We extend neural Turing machine (NTM) model into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read\/write mechanisms. We investigate the mechanisms and effects of learning to read and write into a memory through experiments on Facebook bAbI tasks using both a feedforward and GRUcontroller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We have done extensive analysis of our model and different variations of NTM on bAbI task. We also provide further experimental results on sequential pMNIST, Stanford Natural Language Inference, associative recall and copy tasks.","category_a":"ArXiv","category_b":"Journal","keywords":["Addressing scheme","Experiment","Feedforward neural network","Memory address","Memory cell (binary)","Natural language","Non-deterministic Turing machine","Nonlinear system","Turing machine","Turing test","X86"],"vec":[-0.2920322539,1.4009880776],"nodes":["1854385","2251963","1979489","1751762"]},"00a10855b9ab0f2c04226b7a05a7371dea26090b":{"id":"00a10855b9ab0f2c04226b7a05a7371dea26090b","date":"2011-01-01T12:12:00Z","text":"Markov Random Fields (MRFs) have proven very powerful both as density estimators and feature extractors for classification. However, their use is often limited by an inability to estimate the partition function Z. In this paper, we exploit the gradient descent training procedure of restricted Boltzmann machines (a type of MRF) to track the log partition function during learning. Our method relies on two distinct sources of information: (1) estimating the change \u2206Z incurred by each gradient update, (2) estimating the difference in Z over a small set of tempered distributions using bridge sampling. The two sources of information are then combined using an inference procedure similar to Kalman filtering. Learning MRFs through Tempered Stochastic Maximum Likelihood, we can estimate Z using no more temperatures than are required for learning. Comparing to both exact values and estimates using annealed importance sampling (AIS), we show on several datasets that our method is able to accurately track the log partition function. In contrast to AIS, our method provides this estimate at each time-step, at a computational cost similar to that required for training alone.","category_a":"NIPS","category_b":"ML","keywords":["Algorithmic efficiency","Distribution (mathematics)","Gradient","Gradient descent","Importance sampling","Kalman filter","Markov random field","Multiprogram Research Facility","Partition function (mathematics)","Restricted Boltzmann machine","Sampling (signal processing)"],"vec":[-0.2951755099,0.5190840942],"nodes":["2755582","1760871","1751762"]},"75cbe27efe1c8b255102f641feba1871176c6c20":{"id":"75cbe27efe1c8b255102f641feba1871176c6c20","date":"2016-01-01T12:12:00Z","text":"Organizations are often faced with the challenge of providing data management solutions for large, heterogenous datasets that may have different underlying data and programming models. For example, a medical dataset may have unstructured text, relational data, time series waveforms and imagery. Trying to fit such datasets in a single data management system can have adverse performance and efficiency effects. As a part of the Intel Science and Technology Center on Big Data, we are developing a polystore system designed for such problems. BigDAWG (short for the Big Data Analytics Working Group) is a polystore system designed to work on complex problems that naturally span across different processing or storage engines. BigDAWG provides an architecture that supports diverse database systems working with different data models, support for the competing notions of location transparency and semantic completeness via islands and a middleware that provides a uniform multi-island interface. Initial results from a prototype of the BigDAWG system applied to a medical dataset validate polystore concepts. In this article, we will describe polystore databases, the current BigDAWG architecture and its application on the MIMIC II medical dataset, initial performance results and our future development plans.","category_a":"HPEC","category_b":"Others","keywords":["Big data","Data hub","Data model","Database","Database engine","MIMIC Simulator","Middleware","Span and div","Time series"],"vec":[0.661044215,-0.4704096289],"nodes":["1867298","3468483","38449161","1787375","40604484","3257323","2033016","2682150","1695715"]}},"nodes":{"1701538":{"id":"1701538","name":"Andrew Y. Ng","vec":[-0.8443900689,-0.7329755387],"weight":64,"email":"","org":"","sent":[],"received":[],"docs":["3bff03b7b0b0c4e8f6384dbb2a95e4338d156524","27e38351e48fe4b7da2775bf94341738bc4da07e","764ecb30e4ab5b1ad0cebec96019d9753c0c7f62","06554235c2c9361a14c0569206b58a355a63f01b","182015c5edff1956cbafbcb3e7bbe294aa54f9fc","26196511e307ec89466af06751a66ee2d95b6305","1e9e3694954822d720570c5c53136c8814b2c4be","420a94968bd8671c8071d9c735a095ee475d0b20","2b7ef95822a4d577021df16607bf7b4a4514eb4b","81b3b3fe994a9eda6d3f9d2149aa4492d1933975","12244deb997152492d96c6246ec21b2b9804800d","098a170095b1e84b3a8eb673adfe6408fc0dcff5","72dbacd0dbd157d881d284e922e21fcc5a2a4eeb","29cb27e32d56f39b9fa5c5bf62225580530a014c","146b3649b693ae591c8953c0aae5264512c26ea3","961f6d5bb9b43833d7387e377b25d7a68118b23b","f71c25642f53c52823aeef5d215993470991cf82","60f22ad725f041fff81d6371242485bbe5c3ebb6","3d913755fcda4fde4e0a1365299429038e256c74","79d1e429c241d0aa47a2194246256a5bc79585bc","02227c94dd41fe0b439e050d377b0beb5d427cda","13e10f545bd600b5bd8c36b29bcecec4a61d1b7f","2febe441afece9e3f427fa4d541887fb89d0088f","2329a46590b2036d508097143e65c1b77e571e8c","142f38642629b9d268999ad876af482177d36697","6c96ca73b381c4d8191ad73ea2fd9272ff0799c4","26cb14c9d22cf946314d685fe3541ef9f641e429","4cf66c66ca404ea3b5474e78e6cf94f6bfd05b0d","1e3d36a9184feec956371fe3fb03eb34747e4844","013613f6f3da2094a2fb590d8608ad0b44798baf","2d02141fd8c263d9c9a05d704c2d3e38525a7167","167abf2c9eda9ce21907fcc188d2e41da37d9f0b","1d4d14e78eb1536b84f6e851a67ecc5a6c0ae07f","9d4e4136353d6ed07cfb61565151e0e6e8fd7cf6","0ad8d00860f3b65527a40377dd96b55564515167","69de7827bb2d7508d0f15ce634d9594f8d0bb4b4","007d73c91a1bf90d72eb59fbdd8791a4b009f363","12806c298e01083a79db77927530367d85939907","62a4a6abc2a52889db51b56ef082d02784266985","2d5069a99bfa0b47c095bbb5cefd6dba974f72a7","6f6b397267946e3faaac4e5fa587e1876fe69da2","6f568d757d2c1ab42f2006faa25690b74c3d2d44","5fc69f93422b11c944b2d53e9d2f93295eca3d19","053912e76e50c9f923a1fc1c173f1365776060cc","8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092","124acb2ceccd1d1cd9b4715e00d24eab31099aa8","8e0eacf11a22b9705a262e908f17b1704fd21fa7","60abc9657a01a128c5503673609bfe8e7637c00b","0bddc08769e9ff5d42ed36336f69bb3b1f42e716","2337ff38e6cfb09e28c0958f07e2090c993ef6e8","dd24d28c60d432cf95f5b8a64ba3dec75fcf93b7","0bfbdafdfbcc268860fe54ae4d8f08d487bcc762","62dcea12832692d2acdb46cb343ab03a04898542","180920cd652b27cd9207fb797107b19d46e1100a","50d53cc562225549457cbc782546bfbe1ac6f0cf","7f917de89d395e7dba3f2c5cb54b4a13c06a8dee","3ec4a5d29a3d897595f8f02aff0984d249c179c4","0c23ebb3abf584fa5e0fde558584befc94fb5ea2","db82c8e83d2ef333250126655511498b8114b18e","1828eaa8750ee188111b92deae5c7f67323e723f","a88966cdaeddd15d0a3de365a8f0a5931aebd756","1e9e87fc99430a82621810b3ce7db51e339be315","209ba9f612f34583a23b75c0e8dadc410c400bb2","45d814078179b7ac69086d8c7f40576244258549"],"categories_a":["EMNLP","ICML","ACL","ISER","ICML","EMNLP","ArXiv","","Robotics and Automation","","NIPS","","","","","Commun. ACM","INTERSPEECH","ArXiv","ArXiv","ICML","ICML","ArXiv","NIPS","ICML","Computer Speech & Language","ISER","","Neural Networks: Tricks of the Trade","FSR","ICML","NIPS","ICML","Robotics: Science and Systems","EMNLP","ArXiv","ArXiv","ArXiv","ICML","NIPS","ACL","ArXiv","ACL","EMNLP","I. J. Robotics Res.","EMNLP","NIPS","ICPR","","Robotics and Automation","Encyclopedia of Machine Learning","NIPS","AISTATS","Intelligent Robots and Systems","NIPS","ICML","Document Analysis and Recognition","HLT-NAACL","","EMNLP","AAAI","NIPS","TACL","NIPS",""],"categories_b":["ComLing","ML","ComLing","Others","ML","ComLing","Journal","Others","AI","Others","ML","Others","Others","Others","Others","Journal","Others","Journal","Journal","ML","ML","Journal","ML","ML","Others","Others","Others","Others","Others","ML","ML","ML","Others","ComLing","Journal","Journal","Journal","ML","ML","ComLing","Journal","ComLing","ComLing","Others","ComLing","ML","Others","Others","AI","Others","ML","ML","Others","ML","ML","Others","ComLing","Others","ComLing","AI","ML","Others","ML","Others"]},"5574038":{"id":"5574038","name":"Adam Coates","vec":[-0.7762889066,-0.4590462532],"weight":31,"email":"","org":"","sent":[],"received":[],"docs":["182015c5edff1956cbafbcb3e7bbe294aa54f9fc","1e9e3694954822d720570c5c53136c8814b2c4be","420a94968bd8671c8071d9c735a095ee475d0b20","2b7ef95822a4d577021df16607bf7b4a4514eb4b","12244deb997152492d96c6246ec21b2b9804800d","72dbacd0dbd157d881d284e922e21fcc5a2a4eeb","29cb27e32d56f39b9fa5c5bf62225580530a014c","f71c25642f53c52823aeef5d215993470991cf82","60f22ad725f041fff81d6371242485bbe5c3ebb6","13e10f545bd600b5bd8c36b29bcecec4a61d1b7f","2329a46590b2036d508097143e65c1b77e571e8c","6c96ca73b381c4d8191ad73ea2fd9272ff0799c4","26cb14c9d22cf946314d685fe3541ef9f641e429","4cf66c66ca404ea3b5474e78e6cf94f6bfd05b0d","1e3d36a9184feec956371fe3fb03eb34747e4844","013613f6f3da2094a2fb590d8608ad0b44798baf","0ad8d00860f3b65527a40377dd96b55564515167","69de7827bb2d7508d0f15ce634d9594f8d0bb4b4","007d73c91a1bf90d72eb59fbdd8791a4b009f363","12806c298e01083a79db77927530367d85939907","6f568d757d2c1ab42f2006faa25690b74c3d2d44","5fc69f93422b11c944b2d53e9d2f93295eca3d19","053912e76e50c9f923a1fc1c173f1365776060cc","8e0eacf11a22b9705a262e908f17b1704fd21fa7","60abc9657a01a128c5503673609bfe8e7637c00b","0bddc08769e9ff5d42ed36336f69bb3b1f42e716","0bfbdafdfbcc268860fe54ae4d8f08d487bcc762","180920cd652b27cd9207fb797107b19d46e1100a","7f917de89d395e7dba3f2c5cb54b4a13c06a8dee","1828eaa8750ee188111b92deae5c7f67323e723f","02227c94dd41fe0b439e050d377b0beb5d427cda"],"categories_a":["ICML","ISER","ICML","Robotics and Automation","","","Commun. ACM","ArXiv","ICML","ICML","ICML","ISER","","Neural Networks: Tricks of the Trade","FSR","ICML","Robotics: Science and Systems","ArXiv","ArXiv","ICML","I. J. Robotics Res.","NIPS","ICPR","Robotics and Automation","Encyclopedia of Machine Learning","AISTATS","Intelligent Robots and Systems","NIPS","Document Analysis and Recognition","NIPS",""],"categories_b":["ML","Others","ML","AI","Others","Others","Journal","Journal","ML","ML","ML","Others","Others","Others","Others","ML","Others","Journal","Journal","ML","Others","ML","Others","AI","Others","ML","Others","ML","Others","ML","Others"]},"1746807":{"id":"1746807","name":"Daniel Jurafsky","vec":[-0.8154760367,-0.2978378557],"weight":33,"email":"","org":"","sent":[],"received":[],"docs":["44d6ffe6535f75ccfd4a71a72ac2982077afcb1c","1297074f0e7d6c663c55f7580ca603473842eec9","bfe947a04e9716fb6a581d9609f448aa5daa50f1","45d814078179b7ac69086d8c7f40576244258549","22825681ceb3f065098eb71ca9346b458b566252","26196511e307ec89466af06751a66ee2d95b6305","d579adf7a2c5cce3bb17482bb757f15bff45b131","5640ce300f3bea8347824e887f4b688b0f364873","2dde68fc4b1af7f21e18d7b9da5f964539f9b515","90df2b8696ad2ff4db8178fded1d17303f917d92","35e024714f0702f8b77c8c2a916dedb91931708a","7da23d5b7c26dc1d6ac98682a74a8dd6a18bf309","79d1e429c241d0aa47a2194246256a5bc79585bc","506ccb2fe5b756b35381b750fb996d014fbc8014","0e18e9dcfceedabcf14e6a615862c53ce8c0293d","b90196005b0f1eb0953758c16869633d7ca3399d","2d02141fd8c263d9c9a05d704c2d3e38525a7167","41a0c983e59c8fc5100b94c0802d74a64f86c741","9d4e4136353d6ed07cfb61565151e0e6e8fd7cf6","fff3fa15f1fd07484edf8f355eb53e58d0ed8ccf","022029f1211970dbf29824f6510fe6dc7718190e","2f87364ce9255e345b35d431b00ba7271329cba8","2d5069a99bfa0b47c095bbb5cefd6dba974f72a7","6f6b397267946e3faaac4e5fa587e1876fe69da2","62a4a6abc2a52889db51b56ef082d02784266985","71898bd32548b98ceecf71bb30b9616e4be40b43","5cceeb8a36f62cce92056b8e055ec9aaa7c3074c","124acb2ceccd1d1cd9b4715e00d24eab31099aa8","710822be03043849bc016acc987695a3b8b34ecf","62dcea12832692d2acdb46cb343ab03a04898542","a88966cdaeddd15d0a3de365a8f0a5931aebd756","0a832c69048ca30d4f528d221ba9899590a45f34","16c7d4559b977d10b2a4d73fc3a9a972367e6f11"],"categories_a":["CNLP","ArXiv","EMNLP","TAC","INTERSPEECH","HLT-NAACL","Machine Translation","HLT-NAACL","ArXiv","ACL","Computer Speech & Language","NAACL","","ArXiv","EACL","LREC","ArXiv","Speech Communication","HLT-NAACL","ASRU","EACL","EMNLP","CNLP","CNLP","ACL","EMNLP","ACL","ACL","CoNLL","LREC","HLT-NAACL","EMNLP","NIPS"],"categories_b":["Others","Journal","ComLing","Others","Others","ComLing","Others","ComLing","Journal","ComLing","Others","ComLing","Others","Journal","Others","Others","Journal","Others","ComLing","Others","Others","ComLing","Others","Others","ComLing","ComLing","ComLing","ComLing","Others","Others","ComLing","ComLing","ML"]},"1724875":{"id":"1724875","name":"Pascal Vincent","vec":[-0.4652251246,0.6115013037],"weight":51,"email":"","org":"","sent":[],"received":[],"docs":["6e3f2fe073ba009336c0f13636bb5d137ac276f6","83b625ae40c921c47255da5f2e24266e75a48d9b","0bed90488e8950e5bea85ffca8b48fab3b67c1a1","2ae139b247057c02cda352f6661f46f7feb38e45","a5eb54e8a30406ac723ba258111239e156e25523","528c397b0e523c9a257d67bb05409b83b4cdd33d","0add4da8389bd54958221def3ea16575ee9a007f","013cd20c0eaffb9cab80875a43086e0c3224fe20","4b2283dd97a853812ca053547b4e067b2fa54512","0bb754879154b0a10ee9966636348d831081e151","f9c431f58565f874f76a024add2aa80717ec5cf5","01ad77475dedda66e3961f707a19fe588ef48f39","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","3b118bbb7c33eb8ca4605170e1bff2d459488df6","e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","e39d1345a5aef8a5ee32c0a774de877b903de50c","e27d81521dc4e8b6ea93947c05ffccf06784f569","7184a8b7be5ae88bca900542b0ab2c27b019f0ce","3fd5307b2705971e543d16e7350651945b257f14","373cf414cc038516a2cff11d7caafa3ff1031c6d","5075636b9e0425358204211706adde1ecb5ba60b","3b2bf65ebee91249d1045709200a51d157b0176e","0b98d8f90c57c5aabf2bc264d47730cb2dfc897f","0d060f1edd7e79865f1a48a9c874439c4b10caea","195d0a8233a7a46329c742eaff56c276f847fadc","400f6f4304b1c12efb22acf7e80a1784015cb23a","8225f757f6bb34624aa8e9658e9af295432c29b5","6b570069f14c7588e066f7138e1f21af59d62e61","6bf0414dae4f10c7e54fb9e5e8af5d0d0cab290b","161ba68b34bf2ec77384c9f1de04e9631d08af50","2964d30862d0402b0d0ad4a427067f69e4a52130","07c43a3ff15f2104022f2b1ca8ec4128a930b414","3e1638d1ce6ec0d18c82fb3235a521db0a559da2","5dc1fd136278c61771fbc43473c9e21638f0f46f","4700e447d1a0cddda978cd87177301790bb58efd","3d3b8fe8b5dd2d518f3a186f63553eb26a49968f","ccf415df5a83b343dae261286d29a40e8b80e6c6","60fdfa95dee2dc3b125f1552aa5c080542707e8f","225721565af848f8360a8b01f7cc6d28dcc8e240","3bf1d8b8ef045df1787c703fb6874f01709597da","0375dbfedebc209b1f276f629c8958d37b48cac6","f8c8619ea7d68e604e40b814b40c72888a755e95","1683b929bd2c1fd5f326ee9501ef24c2b613b5ef","65d994fb778a8d9e0f632659fb33a082949a50d3","51691435d646c5b9152162f328d29511755328ba","8a9a10170ee907acb3e582742bec5fa09116f302","541c68e2c65f6dce6179801c9f92dc7803dc71b5","70fa7c2a8a12509da107fe82b04d5e422120984f","0e48b9fc023866b55219b26ec40ad88633020d2e","27211ed68a7a00f1df0121fa1890a1b2acdd1a88","17307b4925959a15b2af55bdbb861df41ab879d2"],"categories_a":["NIPS","KDD","ICPRAM","Multimodal User Interfaces","Pattern Analysis and Machine Intelligence","ICMI","ArXiv","AISTATS","","","ArXiv","NIPS","AISTATS","","ArXiv","AISTATS","","NIPS","","ICML","NIPS","ICPRAM","Neural Computation","","ICML","","CNN","Journal of Machine Learning Research","ArXiv","KDD","ICML","ArXiv","","ISMIR","ArXiv","Neural Computation","ICML","NIPS","NIPS","Acoustics, Speech and Signal Processing","Journal of Machine Learning Research","NIPS","ECCV","","NIPS","ISMIR","ICML","","Machine Learning","NIPS","ArXiv"],"categories_b":["ML","AI","Others","Others","Others","Others","Journal","ML","Others","Others","Journal","ML","ML","Others","Journal","ML","Others","ML","Others","ML","ML","Others","ML","Others","ML","Others","Others","Others","Journal","AI","ML","Journal","Others","Others","Journal","ML","ML","ML","ML","Others","Others","ML","ComVis","Others","ML","Others","ML","Others","Others","ML","Journal"]},"1728478":{"id":"1728478","name":"Donald A. Norman","vec":[-0.1377496923,1.4013600653],"weight":1,"email":"","org":"","sent":[],"received":[],"docs":["4d8d303fd622cf3bd0899bfe532fbee41202e718"],"categories_a":["",""],"categories_b":["Others","Others"]},"38666915":{"id":"38666915","name":"Dan Klein","vec":[-0.3544321735,-0.7184337897],"weight":31,"email":"","org":"","sent":[],"received":[],"docs":["02537a4d8781435a40f768f479a68e7a613dd26f","aad2c96c46064604c5ef729a491a47d12727ae9e","205b9f4891a2ead886604f161a44b3aed483609a","f21fcb8a04f764eb482479823a4933b09534a9b2","3b98dc29eb5f95470d12d19ae528674129ca0411","5158991247802f372b1646ef8751db46c307a2c8","1680eaa31f30911e7460ff694b51794391bc7514","91ff05cf0512789727dd010270e95dfc02301318","b9392e6965137b2f81c81ff17addf3096faa4ffa","10c51faad92ff64e93c3688f62edf9756dca20ad","50654221b831a14e3ddad4579accc9c92685f29a","d2ac9f33f16b421c3c5f38fddb8cdd8d30d7f886","1007e2bfb377757a75f51a0edaf745edeabf3757","043e91679d7def20ece119117757ae7178d9eec9","0bdaf705c237a53b8db50dbdb62f1451774cb2a3","1826fbbf12e399024296c1dfcc5758c1c810574c","1e6f61c574fad2460d06d6f52295922f5c99857a","1c7647fd9ab65e46e821bb0e045535500e2584d4","fe994dc0a8d9a365c5dc34e573dbf7c481b978df","376eec3b84d31d672a3633ed06498fc9a9776f93","304d859dd545ab119ef1a6141d73a9c3797526d4","16ce9e96e5cf0b8fd3aeee137d548ff903711b97","2982952a884a8d27fbcd800c010d8f4af564181b","1aeee8b379e6fcf10d33c15df78e135b16b24d5e","4def24ede97126c946c58e8af355a87a6775e268","7797d9e0e444246ba61d7e16cbbd654b07c117e8","06d0a9697a0f0242dbdeeff08ec5266b74bfe457","afa5f554eba815dc34be4409a595a53f514cb0bc","3fa58a0dcbf7392e02f744a0f729ba8010b7334c","2d28108e25ba7b43f831210db064b1582d265503","6e1fd106a17d67078a1d0ab8d5ef4f64d0feec1e"],"categories_a":["ACL","","CoNLL","NIPS","HLT-NAACL","ICML","ACL","IWPT","Pattern Recognition","CAI","","ICML","HLT-NAACL","","ACL","","HLT-NAACL","SENSEVAL","NIPS","CAI","","ACL","","EMNLP","","ACL","","","","EMNLP","CoNLL"],"categories_b":["ComLing","Others","Others","ML","ComLing","ML","ComLing","Others","Others","AI","Others","ML","ComLing","Others","ComLing","Others","ComLing","Others","ML","AI","Others","ComLing","Others","ComLing","Others","ComLing","Others","Others","Others","ComLing","Others"]},"1735239":{"id":"1735239","name":"H. V. Jagadish","vec":[0.8630666473,-0.1668819201],"weight":32,"email":"","org":"","sent":[],"received":[],"docs":["20dd934a841f775ed18e0d2f8e887f363cbb9099","bf1d7df019f8c5355758a65002dedc9690539819","dc4a6027f4b868e544d2659181e52fa2e716a517","0e16487103cbe1d9bf2ce04ea5718c3fe9f08cf8","d0f04a8e3a21b0142f46067b98530ec833672e93","25241d105a39e9933c03298579248fc0874352d5","219309d6413a29656178f99186157ceeb0932020","019902292dff81eae20f3e87970dd7a1151d9405","277fc66c995fe0c5b79e68b328259159cb412c85","91277a62af159aa44e03482711149658576cae36","1513c758ce244032348b751c40a100846e9aa321","3c7da5d87776db54d04978700f01f3c41156d3e2","5f4abcc6526ca79fb0d5df14527a9a5ce1c12621","3f099575d82b0248c6be9b042c286b3d81f01e63","124f6f240ba622cd74a9a0ea554ec2a5011eaadf","3c43d0c9a1d7864e7d819179940d857f8633d4ce","64b05d23afb0bd5754fcb70d87233c9b4326eef2","55c3597ebe9ce20c3f2374be4dda465726335403","0e6c65b8c366d6f925b8221e01aceddbf931ef19","af5a145735128e1aa4782f8eb8981ff5e3292b2a","7cc937645ddee844b7171b88b587a8d9af98c9bc","620d6ebb2e7604f6dd838210a6847239bc2670dd","1d6156b17ef02f510394f78c3c57b9a1d93af2b4","437b328a1a62eeb85aaa87514a13010a4df315ee","43fc0302e46e91ffcbae49a2aae1a32dd0e927d9","3549ff6774d1dde5e6c80b2e97bc5b6ce578f677","0ed7adf241b11c34fe1354240a1b3cd9cd8a3705","1e4d40e94fa57419a1236a1866a0c3ef357f8b78","314547de686f67e48c075ef8bbe6fe3a1d22dc92","2a92f8fb49dcae62af89d0c1cb9a3001b1be7f5a","1ec8bcda29ae8159b0beb989fef08f6c332e4993","0bfdae2b923805e2453b2affcb1ba05994889d77"],"categories_a":["","SIGMOD","VLDB","ICDE","SIGMOD","Commun. ACM","SIGMOD","VLDB","Data Engineering","VLDB","VLDB","VLDB","PODS","ICDE","VLDB","","DBPL","VLDB","ACM Trans. Database Syst.","IEEE Data Eng. Bull.","EDBT","","SIGMOD","EDBT","SIGMOD","SIGMOD","ICDE","ACM Trans. Database Syst.","VLDB","","CIKM","SIGMOD"],"categories_b":["Others","DB","DB","DB","DB","Journal","DB","DB","DB","DB","DB","DB","Others","DB","DB","Others","Others","DB","Others","DB","Others","Others","DB","Others","DB","DB","DB","Others","DB","Others","Others","DB"]},"1704011":{"id":"1704011","name":"Divesh Srivastava","vec":[0.9955500334,0.2100717154],"weight":121,"email":"","org":"","sent":[],"received":[],"docs":["5cb40de2c796b9644346821c4cb600fd05e8e727","25241d105a39e9933c03298579248fc0874352d5","6e3fbbeec1f3ad2a8efe9a7207ba92bee322b78e","79ed6d5157b73449dc663516fc2c8dfcb917b99e","45923f515e07b0250643350f8e4c19a74593dc91","160cb0d0c0930f2782d8caa212d892845fc02aaf","422955d1f193e70bebb546b33d53564421c5085a","d5b5db41929d791b4e911a5c7b0e29f60ee23253","7584a7f5e9ace396f2d9725cd9d9072a27c1b5a6","908f7db1b21067ee9f9442bccce4787d9ad5634f","124f6f240ba622cd74a9a0ea554ec2a5011eaadf","dfd790fd02afea1de60cba77a3b6b98640bea491","43b39679a5a37585e31260fc426120f4fe00538d","40e6f827c72158e4b0a7dcc20f18448b4dd7e042","47ec99aed698b07b24fdec85e4dbf584c4d2e707","0e6c65b8c366d6f925b8221e01aceddbf931ef19","8e370946562e09f8895f8454a7719848cb176d79","80fe1abae2594a2cb5466d3646abbc57fb13d144","946232a2fe2549a2d2f52612b09ed8d66a74f34b","7cc937645ddee844b7171b88b587a8d9af98c9bc","620d6ebb2e7604f6dd838210a6847239bc2670dd","1d6156b17ef02f510394f78c3c57b9a1d93af2b4","d8c11242130cc02c9f9b7a9506c303ab05e2daea","54636e41ef8dc33f818aec7b9b02731eb75650d6","cb3e11ff1f5f03aa58e47e08640647ca133faab2","25a782a03c249d315b04a68cb1a3663ff8c893d5","1b74e7b2547373ff3df6fc293be7bed49075053d","7509931a7cafe9b0a61337d141d79d76eadaaeed","0ed7adf241b11c34fe1354240a1b3cd9cd8a3705","441a89d393eba5091afe76fbaa7c35cbac1c31dc","16cd112b8f3283a5f915b57c3e56ad10ffb45b09","c05d7d5ad5b154e5118e8e996a26898b6854eb6e","20dd934a841f775ed18e0d2f8e887f363cbb9099","bf1d7df019f8c5355758a65002dedc9690539819","70325f4d1f7502bd06fa54484535b36c8b0c1290","0bbf348983a12f21c1607b8a963357c33f18f120","58cfb95d06b6f4ccbf79ed1436c50187d1657252","219309d6413a29656178f99186157ceeb0932020","1600e8c878a2f42c71c753a353b8554b37ecf093","2185c1d1e465dd443b7ccb47c9b2d520da65a319","7400fe80e9e5670fd3e9de000a4afb2e6cc034fc","0dbac3fa87f3288454bb1c5a24e97c9fa4401cbe","1780dab3ed9253b596cb68a10cf3c2d9b8e5015f","323011da81c396825df6ec185ac17a21f2272ea0","39371e655d27527c74b6d18080dcd3107273ba8f","a9d41b228cd14532fcabb87d65739a8213f69299","16cfb80ff6321fa0e277e0316398742475dcdc84","5f4abcc6526ca79fb0d5df14527a9a5ce1c12621","3f099575d82b0248c6be9b042c286b3d81f01e63","9f5f5ecf0ee423f8493e3a600c6af9a7f8720d8e","14577a88dec08a9199ec4f7a878956a4e55c3dd5","19a2c31570534fbdde8290413ac465b3831e9e99","211e4b486a85f5250ef68aef8a4422811ec7a932","0ec2c62121f8543864c84348835883df564aea14","99126d196c0539e3014b61d91538a29ee2de8c54","be15028db5dc7f50cbca96d9e3d264802f385873","4893c08f1c76fe4eb7cad2bb496da526b0fa5dd3","43fc0302e46e91ffcbae49a2aae1a32dd0e927d9","368fe0571a5611c2de27379747adff2066245937","3e4290774195402d31661d3c6b1f1da1bc55f5ae","273121804df13771df2a6890f50e54e7ec419f75","87f34d2e08f9f079c189bdb1db1a4d750080a8cc","260697ac804faf844241de404bab6e9460223c2f","245cd97fc34fad6d6e7efb373874e5a42576adfe","0e16487103cbe1d9bf2ce04ea5718c3fe9f08cf8","35c14fd2453b0dc02a02dcf538fbd775e4993e81","d0f04a8e3a21b0142f46067b98530ec833672e93","5a5d6b0a9a7036c5417a618925c8e03d786e7d74","6556cea7f91c1221dd2693aba0d3dd45d2e09e6d","277fc66c995fe0c5b79e68b328259159cb412c85","411b6492957d5367ef0df2ac926793fec0ca74ed","1513c758ce244032348b751c40a100846e9aa321","29360559cbb9d8d4012c94262f3a79ea259abe2f","5b88525bca03b88dfffc6e478127ddd72518d813","9b1725d100e06bfd43ed5e996288e8c55c7d510b","46cd50a49397e37e5d83cb536f4b92a40293a467","7466d1a809d69435f29cebdd651029566eb702f7","3c43d0c9a1d7864e7d819179940d857f8633d4ce","55c3597ebe9ce20c3f2374be4dda465726335403","40709c0e27d0fe6f9265b6c29847e63392f4001d","1d8bfe40212695fdd05fd1e297f9584a30f477c4","af5a145735128e1aa4782f8eb8981ff5e3292b2a","232d3305b2c4c68a57b357c2cdea7c754891ebcb","0899c2c3c31cd555425008d38a1a671f3e37724a","437b328a1a62eeb85aaa87514a13010a4df315ee","78b729049a0135dc75a021ce5bbc127902253fde","3549ff6774d1dde5e6c80b2e97bc5b6ce578f677","3598b9eb49e226c1f49f123a058c1896459af339","374668781c1aa05391a900cc2841eb31c7bbce6a","771a8edc3a434af7e1af3b0c9424c7e199e4c646","2a92f8fb49dcae62af89d0c1cb9a3001b1be7f5a","4d82a5741b03a0a134380400d42552c53c89ac83","1ec8bcda29ae8159b0beb989fef08f6c332e4993","2dfbd860542f79240f8f58cc042db1dfa562053a","31b58ddd4a558f7d9e41e19ef75993668ee69119","0bfdae2b923805e2453b2affcb1ba05994889d77","0d43fe67820c2f2a2e8adda04be04579fa85582c","157852dd14a9b518bf62fa6511be0f02f5d04a79","61845763cd9c8865870b17f6efa034e8fa862978","dc4a6027f4b868e544d2659181e52fa2e716a517","7fee2107608d10a494a469dbf01c807a9a95b09d","5394ab5bf4b66bfb52f111525d6141a3226ba883","2f207e27cad923b81253542a6f76439d49d87925","0aa8c4caef4ad23da2e1def1f22b74badd5be4e2","08e5cce87b6af2ae0ee09b58b86d81aa229b46d0","23d1255f1a0453ba96c6e6ef054903086f3656df","91277a62af159aa44e03482711149658576cae36","1298f6dd80c11766a3ff8ae1a4c21e12eadb7c17","3c7da5d87776db54d04978700f01f3c41156d3e2","4cc6eb1716cabda5a55a8ed103f8e503717ac434","64b05d23afb0bd5754fcb70d87233c9b4326eef2","683422744d6a8f5e6bac38756f1f51a8ed16e2e3","704dfc1636911592c5ea02a198077d2586bb8d23","33e46d74b066402f0ae912627dff5b700006f9b2","3d6e2d4c1cde661fea2c8b1c93528aa24bb87aff","5d458a1bff91aa598fcc47711e5cfd7a6dfa559d","0c7650ed95c7d5fc90d769d5e34fd519313e8383","1e4d40e94fa57419a1236a1866a0c3ef357f8b78","208edd765ff1fa2320dc87d2746b5b7460303bd8","0d60fce590359780f62017e472fd4523a5de3f5f","023db6fa5c87852885dcee9531a1c843063c2347"],"categories_a":["SIGMOD","VLDB","","","VLDB","Data Engineering","Data Engineering","SIGMOD","VLDB","SIGMOD","ICDE","SIGMOD","Data Engineering","Data Engineering","SIGMOD","VLDB","VLDB","ICDE","Data Engineering","SIGMOD","Computer Networks","SIGMOD","VLDB","SSPS","ICDE","Data Engineering","VLDB","VLDB","VLDB","","VLDB","Data Engineering","VLDB","VLDB","ICDT","VLDB","Data Engineering","WWW","ICDEW","VLDB","SIGMOD","ICDE","PODS","ICDE","SSDBM","ICDEW","VLDB","Data Engineering","Stream Data Management","SIGMOD","SIGMOD","","Data Engineering","VLDB","DBPL","Data Engineering","SIGMOD","WWW","SIGMOD","PODS","VLDB","PODS","VLDB","Knowledge and Data Engineering","ICDE","SIGMOD","VLDB","ICDE","PODS","Data Engineering","VLDB","ACM Trans. Database Syst.","ArXiv","","ArXiv","Knowledge and Data Engineering","IEEE Data Eng. Bull.","EDBT","ICDE","DMSN","","XSym","SIGMOD","SIGMOD","VLDB","EDBT","WWW","ICDE","ACM Trans. Database Syst.","SIGMOD","ICDM","ICDE","ICDE","SIGMOD","","ICDE","WOSN","ICDE","KDD","VLDB","ACM Trans. Database Syst.","ArXiv","SIGMOD","CIKM","ACM Trans. Database Syst.","ICDE","Data Engineering","","VLDB","ICDE","VLDB","ICDE","PODS","VLDB","VLDB","","CIKM","SIGMOD","SIGMOD","CleanDB","SIGMOD"],"categories_b":["DB","DB","Others","Others","DB","DB","DB","DB","DB","DB","DB","DB","DB","DB","DB","DB","DB","DB","DB","DB","Others","DB","DB","Others","DB","DB","DB","DB","DB","Others","DB","DB","DB","DB","Others","DB","DB","Others","Others","DB","DB","DB","Others","DB","Others","Others","DB","DB","Others","DB","DB","Others","DB","DB","Others","DB","DB","Others","DB","Others","DB","Others","DB","Others","DB","DB","DB","DB","Others","DB","DB","Others","Journal","Others","Journal","Others","DB","Others","DB","Others","Others","Others","DB","DB","DB","Others","Others","DB","Others","DB","Others","DB","DB","DB","Others","DB","Others","DB","AI","DB","Others","Journal","DB","Others","Others","DB","DB","Others","DB","DB","DB","DB","Others","DB","DB","Others","Others","DB","DB","Others","DB"]},"3216322":{"id":"3216322","name":"Li Fei-Fei","vec":[-0.1358683317,-0.3621045525],"weight":15,"email":"","org":"","sent":[],"received":[],"docs":["87a937dff0ecffefd8a7e4ca4ce068aca9731a0a","098abe1eee6208b693daafe7183f017f9e71e409","7060f6062ba1cbe9502eeaaf13779aa1664224bb","65c978a97f54cf255f01c6846d6c51b37c61f836","01a903739564f575b81c87f7a9e2cb7b609f7ada","19150b001031cc6d964e83cd28553004f653cc24","38211dc39e41273c0007889202c69f841e02248a","5edf85b3068a0c95d5901623c7fd793ac1c92b38","3b2697d76f035304bfeb57f6a682224c87645065","115808104b2a9c3ab6e2e60582ab7e33b937b754","30e2c349636920f96e72882e2e2c1d22b4739127","0b4d3e59a0107f0dad22e74054bab1cf1ad9c32e","2606e6a5759c030e259ebf3f4261b9c04a36a609","0c881ea63ff12d85bc3192ce61f37abf701fdf38","9fe93ec9c2f8ca5ddbc46c050ba4216c09154980"],"categories_a":["EMNLP","ArXiv","CVPR","CHI","CVPR","CVPR","CHI","ECCV","CVPR","Journal of Computer Vision","CVPR","ArXiv","","Journal of Computer Vision","CSCW"],"categories_b":["ComLing","Journal","ComVis","HCI","ComVis","ComVis","HCI","ComVis","ComVis","Others","ComVis","Journal","Others","Others","HCI"]},"2166511":{"id":"2166511","name":"Richard Socher","vec":[-0.4358078653,-0.3409455194],"weight":29,"email":"","org":"","sent":[],"received":[],"docs":["3bff03b7b0b0c4e8f6384dbb2a95e4338d156524","27e38351e48fe4b7da2775bf94341738bc4da07e","84458cd529206ec4897b90be191b089e8966ddc4","0b753d3c9fb80145c59f987749ebbde80a0de475","06554235c2c9361a14c0569206b58a355a63f01b","5aee809e16ec6d065e828fa04968b2d189c29e71","0a22ce29e0fd1b0303ede3f5a94e1fe70764e455","683e5c3112c63aabaedafb7ba24f994baf07e0a4","961f6d5bb9b43833d7387e377b25d7a68118b23b","bd4ddde7f9af802805fac652b44146698f0bda6e","0825788b9b5a18e3dfea5b0af123b5e939a4f564","142f38642629b9d268999ad876af482177d36697","65ad0e876216ea034b7958f016456e32666bc5c6","17a40c553bdef82ccfc1bcc978e8e9d23230bfe5","098abe1eee6208b693daafe7183f017f9e71e409","4e88de2930a4435f737c3996287a90ff87b95c59","167abf2c9eda9ce21907fcc188d2e41da37d9f0b","00a28138c74869cfb8236a18a4dbe3a896f7a812","0c881ea63ff12d85bc3192ce61f37abf701fdf38","0d3233d858660aff451a6c2561a05378ed09725a","8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092","7797d9e0e444246ba61d7e16cbbd654b07c117e8","38211dc39e41273c0007889202c69f841e02248a","209ba9f612f34583a23b75c0e8dadc410c400bb2","1e9e87fc99430a82621810b3ce7db51e339be315","50d53cc562225549457cbc782546bfbe1ac6f0cf","3ec4a5d29a3d897595f8f02aff0984d249c179c4","0c23ebb3abf584fa5e0fde558584befc94fb5ea2","81b3b3fe994a9eda6d3f9d2149aa4492d1933975"],"categories_a":["ACL","AISTATS","EMNLP","NIPS","","EMNLP","ArXiv","NIPS","NIPS","ICML","NIPS","CoNLL","EMNLP","","NIPS","ACL","","CVPR","","","HLT-NAACL","CVPR","","","CVPR","EMNLP","ACL","","TACL"],"categories_b":["ComLing","ML","ComLing","ML","Others","ComLing","Journal","ML","ML","ML","ML","Others","ComLing","Others","ML","ComLing","Others","ComVis","Others","Others","ComLing","ComVis","Others","Others","ComVis","ComLing","ComLing","Others","Others"]},"1709589":{"id":"1709589","name":"Graham Cormode","vec":[1.0310169709,0.5206970544],"weight":33,"email":"","org":"","sent":[],"received":[],"docs":["70325f4d1f7502bd06fa54484535b36c8b0c1290","0bbf348983a12f21c1607b8a963357c33f18f120","7fee2107608d10a494a469dbf01c807a9a95b09d","35c14fd2453b0dc02a02dcf538fbd775e4993e81","260697ac804faf844241de404bab6e9460223c2f","5394ab5bf4b66bfb52f111525d6141a3226ba883","2f207e27cad923b81253542a6f76439d49d87925","6e3fbbeec1f3ad2a8efe9a7207ba92bee322b78e","08e5cce87b6af2ae0ee09b58b86d81aa229b46d0","23d1255f1a0453ba96c6e6ef054903086f3656df","1298f6dd80c11766a3ff8ae1a4c21e12eadb7c17","79ed6d5157b73449dc663516fc2c8dfcb917b99e","0d60fce590359780f62017e472fd4523a5de3f5f","39371e655d27527c74b6d18080dcd3107273ba8f","7466d1a809d69435f29cebdd651029566eb702f7","7584a7f5e9ace396f2d9725cd9d9072a27c1b5a6","47ec99aed698b07b24fdec85e4dbf584c4d2e707","14577a88dec08a9199ec4f7a878956a4e55c3dd5","1d8bfe40212695fdd05fd1e297f9584a30f477c4","211e4b486a85f5250ef68aef8a4422811ec7a932","704dfc1636911592c5ea02a198077d2586bb8d23","80fe1abae2594a2cb5466d3646abbc57fb13d144","33e46d74b066402f0ae912627dff5b700006f9b2","99126d196c0539e3014b61d91538a29ee2de8c54","3d6e2d4c1cde661fea2c8b1c93528aa24bb87aff","4893c08f1c76fe4eb7cad2bb496da526b0fa5dd3","5d458a1bff91aa598fcc47711e5cfd7a6dfa559d","0c7650ed95c7d5fc90d769d5e34fd519313e8383","374668781c1aa05391a900cc2841eb31c7bbce6a","771a8edc3a434af7e1af3b0c9424c7e199e4c646","16cd112b8f3283a5f915b57c3e56ad10ffb45b09","31b58ddd4a558f7d9e41e19ef75993668ee69119","023db6fa5c87852885dcee9531a1c843063c2347"],"categories_a":["SIGMOD","SIGMOD","Data Engineering","Data Engineering","VLDB","ICDT","VLDB","Data Engineering","ICDEW","SSDBM","SIGMOD","VLDB","Data Engineering","SIGMOD","PODS","VLDB","ICDE","SIGMOD","ArXiv","ArXiv","WWW","ACM Trans. Database Syst.","ICDE","","WOSN","ICDE","KDD","ArXiv","CIKM","ICDE","","VLDB","VLDB"],"categories_b":["DB","DB","DB","DB","DB","Others","DB","DB","Others","Others","DB","DB","DB","DB","Others","DB","DB","DB","Journal","Journal","Others","Others","DB","Others","Others","DB","AI","Journal","Others","DB","Others","DB","DB"]},"1979489":{"id":"1979489","name":"Kyunghyun Cho","vec":[-0.6528437989,0.4857081873],"weight":48,"email":"","org":"","sent":[],"received":[],"docs":["0b544dfe355a5070b60986319a3f51fb45d1348e","7e463877264e70d53c844cf4b1bf3b15baec8cfb","4856e7719e566f2466369ae2031afb07c934d4d3","0a49b4de21363d86599d4a058aaf4f5aed019495","7b4cd55ce7d7558eecb8d39f25a05cfd01ce4715","032e9974cedb31f5c6e354626760e54e5ebf1e3c","25eb839f39507fe6983ad3e692b2f8d93a5cb0cc","237202f570c6990f0c33c4db17b5df61d5ee6899","0851f0dad6de06dab86c82feb34dfa5b807129dc","2cb3bd0cff91c0afcbfb2cb10ce30313e0f70133","89677d5cb68a4c020f4604e99b9a72f6c4532581","78d0ed0106446bfe78851e0ab3e7dd50e586c951","071b16f25117fb6133480c6259227d54fc2a5ea0","2c27683f6ecf8f2e94ff3254fa2630f90e3aacf9","35254a19bac666c291646e3640014bf0bb2ce11b","95a4edd3b7967da6c39bea61dc846052cd3a4837","654a3e53fb41d8168798ee0ee61dfab73739b1ed","0d3fc5945c3ee608af29080c00032b9afd232c36","0d75052f1d7350fa035a35566555ce7b65d1cd2f","6b570069f14c7588e066f7138e1f21af59d62e61","c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d","25f0625a92f6054b11057423111f9285c78376fe","18cc17c06e34baaa3e196db07e20facdbb17026d","23c2e6c185aa7f4274dc53196fa130dcd969988b","aa3d3c206b9d1726b9c387a00ddd2da2a9803fb6","e93a59b37f3d6282ebd4e5bad61924f6d75385d9","087f06a00bc3cbaa2aa16baf8a8a98eb50358d16","a3bd824b2d0077e1e2f97bb73942151d711cdf93","20a599c5f152c3f135b9b55a667e64c93ec8d477","2a0cec7f0f8b63f182ea0c52cb935580acabafcc","03b18dcde7ba5bb0e87b2bdb68ab7af951daf162","f59d3e260ae4c7e145b3a2732524ea74cf5deac2","533ee188324b833e059cb59b654e6160776d5812","135c89b491f82bd4fd7de175ac778207f598342b","fd7447a976968cd4190c65edef8482f4f8e0cab9","06d51deada6e771a2571807a47dd991120c8dd1a","1eb654f295d0e98e71bfbbe18c8a0cad3f6d15e9","ceef5d5b82b822308a07998f19c6cca7ce0a80a6","26c8d040bef85ad6dde55a8f71af936fb38356ad","e50c2e4b8f7df21763f47c8941aa560c75d133b1","878634c30842b5812c56fe772719424bab69e7ad","5e3b036c44f0c3b0b5eb1e99ef78644dcabcf2f0","4a350352b2fb426530693d42f45effd049537cab","146f6f6ed688c905fb6e346ad02332efd5464616","6bf6a3fd2c4c17c4326b81424ce19aba0a4b9c42","0a6193e3693356ae19c3ec3ed7c1375260a9e4a1","3f43314011db0e92e629d6ee6cf6a918e1e258bb","5115f3ff1ac45486a50ad3834a40490f9ca4bcea"],"categories_a":["ArXiv","EMNLP","ICCV","EMNLP","ICML","Neurocomputing","ArXiv","ACL","KDD","Computer Speech & Language","NIPS","TACL","ArXiv","ArXiv","ArXiv","ICML","ArXiv","ArXiv","RecSys","Computer Speech & Language","ArXiv","NIPS","ArXiv","NIPS","","ArXiv","EMNLP","ArXiv","ArXiv","ArXiv","EMNLP","HLT-NAACL","Computer Speech & Language","BMVC","ACL","Machine Translation","WMT","KDD","","Neural computation","Multimedia","CVPR","ArXiv","ArXiv","ArXiv","ArXiv","NIPS","ArXiv"],"categories_b":["Journal","ComLing","Others","ComLing","ML","Others","Journal","ComLing","AI","Others","ML","Others","Journal","Journal","Journal","ML","Journal","Journal","Others","Others","Journal","ML","Journal","ML","Others","Journal","ComLing","Journal","Journal","Journal","ComLing","ComLing","Others","Others","ComLing","Others","Others","AI","Others","Others","Others","ComVis","Journal","Journal","Journal","Journal","ML","Journal"]},"1695715":{"id":"1695715","name":"Michael Stonebraker","vec":[0.8197548361,-0.4355332823],"weight":42,"email":"","org":"","sent":[],"received":[],"docs":["3845947052d99e45f0f3e7789b66c6582d098e48","4e649f746c007f858492a504ee2da156b3d25e36","1b4f5bb49dc95340a66c75e1c4c719f0f96439c8","0d6aa509c98b197a1f410f31e6fa760fde40d8a1","0658b1922d6a30a0191bb5fe13b0a9e43e49c999","75cbe27efe1c8b255102f641feba1871176c6c20","0c325c32039656541760b2d8f02be4636e026785","1fd8bf7f21e2f117ce00ea9cf3005d4aa37f155a","988cbe6332ce9b202ed3d8c6d03c678e9f7c3e2f","0bb7e5432be8ae6799eddf88d35e525b4b1744f7","3df5b6b3b2b648f3d8224322e6a0f127850df017","73f31354cc9058ddc2e47a1c585b753e1592c1bf","4fadcd3a0cb63edd255383f41ed3e280495333b2","2009b3c05dd9084a0a1c609abcedd81713ed7150","019902292dff81eae20f3e87970dd7a1151d9405","0f41cd1792db9ff879fdfffc746cf5a01adf207f","315cd76e4a34b8fe27e20345abcd4fc27c7ee1ab","266228767a0d4aed021754b17947c926f4f8881b","133eacaf0ad25b8364cb4510007d9363298e8adf","0a5eace1ead92c5b1c47ccc89d1daf7bb5c5acd8","190b27eb221eec46b97373d80c5b96d95c4153c3","011f7f9ba9e6f9bc7f05994271725bc0fc9c3b94","4ca3c1040a4bc0d1ae200d26d1c18fbc6df0e95a","1e185688f3c5e73641633459dbc17b30d34fd307","57e979025374da67fae37fbb81bbadecee68cc08","77ae592ec975dfb14d494667e0b019af19da9075","8c8b44029fbdac1572ae47b8eaab3929c9987098","2525c025f11aec60cff428271ca851381b92008f","700e1d1b7c54711e96645691438d7fd9fda229ef","9315cf87a57a346c416a98984b8bf82d3b85e679","b2bf34c0c0007145a389e014b7ddaa3daa76f332","cab20ae17048106a84f14a6209f0286609cfa542","cc4296eecd15b83a3c9b9d3a84e91327bd14c7bd","d07a50bb119b594a6f8f9d9c574cbdfdfaa586d3","5578cabaef7b5dfc88443626e74d2e04951818f7","136eefe33796c388a15d25ca03cb8d5077d14f37","18c021c9cce95ed5615a060f590b8388b604e7c5","412a9e54bbb31e12d008a9579994e009c5b40b46","207def18c67fa8024741b7ae3cdc655b57f2053f","5272c3e3c06481eb705241a6ed45f1d3905a1459","265efd16c16dc942705246a57337d323722003cb","314547de686f67e48c075ef8bbe6fe3a1d22dc92"],"categories_a":["VLDB","","VLDB","CIDR","SIGMOD","Commun. ACM","VLDB","Data Stream Management","ArXiv","","HPEC","VLDB","VLDB","SIGMOD","VLDB","ArXiv","Commun. ACM","","SIGMOD","VLDB","Data Engineering","VLDB","ICDE","ArXiv","SIGMOD","","SIGMOD","SIGMOD","Commun. ACM","VLDB","SIGMOD","VLDB","SIGMOD","","SIGMOD","","VLDB","Data Engineering","ArXiv","","DaMoN","HPEC"],"categories_b":["DB","Others","DB","Others","DB","Journal","DB","Others","Journal","Others","Others","DB","DB","DB","DB","Journal","Journal","Others","DB","DB","DB","DB","DB","Journal","DB","Others","DB","DB","Journal","DB","DB","DB","DB","Others","DB","Others","DB","DB","Journal","Others","Others","Others"]},"35609041":{"id":"35609041","name":"Michael S. Bernstein","vec":[0.2066195989,-0.5672670828],"weight":15,"email":"","org":"","sent":[],"received":[],"docs":["87a937dff0ecffefd8a7e4ca4ce068aca9731a0a","7060f6062ba1cbe9502eeaaf13779aa1664224bb","76765f06161c2f1b153056b86ce665102414ec73","65c978a97f54cf255f01c6846d6c51b37c61f836","01a903739564f575b81c87f7a9e2cb7b609f7ada","71439278086b7fc84eedc8aeba377747e00b8403","19150b001031cc6d964e83cd28553004f653cc24","bd212586b54cb959c5d5058a4e16ee6b2cdae1cc","6edc18adf0b57400726b8fc76b3428173d118826","5edf85b3068a0c95d5901623c7fd793ac1c92b38","ecdd236a1a31afcb2a3480a4c7dd5aa65b44551d","3b2697d76f035304bfeb57f6a682224c87645065","115808104b2a9c3ab6e2e60582ab7e33b937b754","0b4d3e59a0107f0dad22e74054bab1cf1ad9c32e","9fe93ec9c2f8ca5ddbc46c050ba4216c09154980"],"categories_a":["SIGMOD","ArXiv","CVPR","CHI","","CHI","CVPR","","CHI","ECCV","Journal of Computer Vision","ArXiv","SIGMOD","Journal of Computer Vision","CSCW"],"categories_b":["DB","Journal","ComVis","HCI","Others","HCI","ComVis","Others","HCI","ComVis","Others","Journal","DB","Others","HCI"]},"2033016":{"id":"2033016","name":"Samuel Madden","vec":[0.5983948769,-0.4091214514],"weight":46,"email":"","org":"","sent":[],"received":[],"docs":["3845947052d99e45f0f3e7789b66c6582d098e48","4e649f746c007f858492a504ee2da156b3d25e36","1b4f5bb49dc95340a66c75e1c4c719f0f96439c8","0d6aa509c98b197a1f410f31e6fa760fde40d8a1","0658b1922d6a30a0191bb5fe13b0a9e43e49c999","75cbe27efe1c8b255102f641feba1871176c6c20","0c325c32039656541760b2d8f02be4636e026785","1fd8bf7f21e2f117ce00ea9cf3005d4aa37f155a","988cbe6332ce9b202ed3d8c6d03c678e9f7c3e2f","ecdd236a1a31afcb2a3480a4c7dd5aa65b44551d","0bb7e5432be8ae6799eddf88d35e525b4b1744f7","3df5b6b3b2b648f3d8224322e6a0f127850df017","73f31354cc9058ddc2e47a1c585b753e1592c1bf","4fadcd3a0cb63edd255383f41ed3e280495333b2","2009b3c05dd9084a0a1c609abcedd81713ed7150","0f41cd1792db9ff879fdfffc746cf5a01adf207f","315cd76e4a34b8fe27e20345abcd4fc27c7ee1ab","71439278086b7fc84eedc8aeba377747e00b8403","133eacaf0ad25b8364cb4510007d9363298e8adf","266228767a0d4aed021754b17947c926f4f8881b","0a5eace1ead92c5b1c47ccc89d1daf7bb5c5acd8","bd212586b54cb959c5d5058a4e16ee6b2cdae1cc","190b27eb221eec46b97373d80c5b96d95c4153c3","011f7f9ba9e6f9bc7f05994271725bc0fc9c3b94","4ca3c1040a4bc0d1ae200d26d1c18fbc6df0e95a","1e185688f3c5e73641633459dbc17b30d34fd307","57e979025374da67fae37fbb81bbadecee68cc08","77ae592ec975dfb14d494667e0b019af19da9075","8c8b44029fbdac1572ae47b8eaab3929c9987098","2525c025f11aec60cff428271ca851381b92008f","700e1d1b7c54711e96645691438d7fd9fda229ef","76765f06161c2f1b153056b86ce665102414ec73","9315cf87a57a346c416a98984b8bf82d3b85e679","b2bf34c0c0007145a389e014b7ddaa3daa76f332","6edc18adf0b57400726b8fc76b3428173d118826","cab20ae17048106a84f14a6209f0286609cfa542","cc4296eecd15b83a3c9b9d3a84e91327bd14c7bd","d07a50bb119b594a6f8f9d9c574cbdfdfaa586d3","5578cabaef7b5dfc88443626e74d2e04951818f7","136eefe33796c388a15d25ca03cb8d5077d14f37","18c021c9cce95ed5615a060f590b8388b604e7c5","412a9e54bbb31e12d008a9579994e009c5b40b46","207def18c67fa8024741b7ae3cdc655b57f2053f","5272c3e3c06481eb705241a6ed45f1d3905a1459","265efd16c16dc942705246a57337d323722003cb","314547de686f67e48c075ef8bbe6fe3a1d22dc92"],"categories_a":["VLDB","","SIGMOD","VLDB","CIDR","SIGMOD","Commun. ACM","VLDB","Data Stream Management","ArXiv","","HPEC","VLDB","VLDB","SIGMOD","VLDB","ArXiv","Commun. ACM","","CHI","SIGMOD","VLDB","","Data Engineering","VLDB","","ICDE","ArXiv","SIGMOD","","SIGMOD","Commun. ACM","VLDB","SIGMOD","VLDB","SIGMOD","","SIGMOD","","VLDB","Data Engineering","SIGMOD","ArXiv","","DaMoN","HPEC"],"categories_b":["DB","Others","DB","DB","Others","DB","Journal","DB","Others","Journal","Others","Others","DB","DB","DB","DB","Journal","Journal","Others","HCI","DB","DB","Others","DB","DB","Others","DB","Journal","DB","Others","DB","Journal","DB","DB","DB","DB","Others","DB","Others","DB","DB","DB","Journal","Others","Others","Others"]},"34740554":{"id":"34740554","name":"Ian J. Goodfellow","vec":[-0.7335400951,0.311203904],"weight":20,"email":"","org":"","sent":[],"received":[],"docs":["1e1e274d9dde08a5e6a02daf86405d1cee5ec5cd","db8c3cfaae04a14c1209d62953029b6fa53e23c7","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","2064c2a33eb0b4c8acd23fd60b98c12d6c1ad61e","855d0f722d75cc56a66a00ede18ace96bafee6bd","6b570069f14c7588e066f7138e1f21af59d62e61","73463ec3391df70d4c38de6a1e963830a85efbe6","6ca1898dac153b8cd500c0c2633675b05d3c638c","6a0e1ce1ea01eb5eccf7c852a26f0e3db85e856e","1979271767a78ec766638b83efcc1fc8e96cc4a2","aeb38c8c4b826ad0dc63911dc5198f8c565298f5","2337ff38e6cfb09e28c0958f07e2090c993ef6e8","8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac","5656fa5aa6e1beeb98703fc53ec112ad227c49ca","2b329183e93cb8c1c20c911c765d9a94f34b5ed5","41f50d08e4c237d0f192f5c09f78d7e1d09d9cef","2f7ad26514bce4df6c8ebc42c90383ef3a974df4","16a333a43d587802f95b5ec11de6c99314ae0c77","046a1302079f56b94c81457bf7fd21c3417a9f72"],"categories_a":["ArXiv","","ArXiv","AAAI","ArXiv","ICONIP","NIPS","Pattern Analysis and Machine Intelligence","ArXiv","NIPS","ICML","ArXiv","ArXiv","ArXiv","NIPS","Nature","ArXiv","ArXiv","ICML","ICML"],"categories_b":["Journal","Others","Journal","AI","Journal","Others","ML","Others","Journal","ML","ML","Journal","Journal","Journal","ML","Others","Journal","Journal","ML","ML"]},"1751762":{"id":"1751762","name":"Yoshua Bengio","vec":[-0.4547877647,0.3463344699],"weight":162,"email":"","org":"","sent":[],"received":[],"docs":["0b544dfe355a5070b60986319a3f51fb45d1348e","2ae139b247057c02cda352f6661f46f7feb38e45","534f6ea4ce0127e5da7f1cafb6334b59ad15b83f","013cd20c0eaffb9cab80875a43086e0c3224fe20","7b4cd55ce7d7558eecb8d39f25a05cfd01ce4715","4b2283dd97a853812ca053547b4e067b2fa54512","f9c431f58565f874f76a024add2aa80717ec5cf5","517c31e5390d1d743aca69d16098be6ca30ebd2d","01ad77475dedda66e3961f707a19fe588ef48f39","0851f0dad6de06dab86c82feb34dfa5b807129dc","05fd1da7b2e34f86ec7f010bef068717ae964332","e27d81521dc4e8b6ea93947c05ffccf06784f569","5d363ec087445a226c8f64db03dd7efaa132e854","17f5c7411eeeeedf25b0db99a9130aa353aee4ba","62c76ca0b2790c34e85ba1cce09d47be317c7235","78c91d969c55a4a61184f81001c376810cdbd541","0808bb50993547a533ea5254e0454024d98c5e2f","373cf414cc038516a2cff11d7caafa3ff1031c6d","221cf7b15aa771f9f9f8c0dc21899e22cd736fb8","3b2bf65ebee91249d1045709200a51d157b0176e","5075636b9e0425358204211706adde1ecb5ba60b","95a4edd3b7967da6c39bea61dc846052cd3a4837","74225938608ccf6a825df576d4dfe2fca9726200","855d0f722d75cc56a66a00ede18ace96bafee6bd","654a3e53fb41d8168798ee0ee61dfab73739b1ed","0d75052f1d7350fa035a35566555ce7b65d1cd2f","03cb609fcfce6c60cbe3eb0dd8254069bf6d7573","1671a665c636bec7d2eaff137d74e9b7f074892f","6ca1898dac153b8cd500c0c2633675b05d3c638c","3e1638d1ce6ec0d18c82fb3235a521db0a559da2","03b18dcde7ba5bb0e87b2bdb68ab7af951daf162","5da15e6b30920ab20ce461232c00276e3b0f0bef","225721565af848f8360a8b01f7cc6d28dcc8e240","2b35a5bccb678532412bcb471c16d6208353cf62","0375dbfedebc209b1f276f629c8958d37b48cac6","1eb654f295d0e98e71bfbbe18c8a0cad3f6d15e9","e50c2e4b8f7df21763f47c8941aa560c75d133b1","65d994fb778a8d9e0f632659fb33a082949a50d3","878634c30842b5812c56fe772719424bab69e7ad","70fa7c2a8a12509da107fe82b04d5e422120984f","54c22be7d4bdcb761c42f2670242c8689e29206f","a4ab5ad02c4ed463e1e8a5c06e8b176ef3dea2ea","7a7a2658df5d66541305962d4c9d43078adadac6","8c3679bab6b379a3f487c68f631f55bb18292bc0","12dd078034f72e4ebd9dfd9f80010d2ae7aaa337","032e9974cedb31f5c6e354626760e54e5ebf1e3c","0bb754879154b0a10ee9966636348d831081e151","237202f570c6990f0c33c4db17b5df61d5ee6899","63cbac5a39cd926a806f60116b845f9bd70f5544","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","60744af2f89291897839852d66582b1b2a0be0bc","389bfe18dc161fda4980ec426ffaccec76a918bb","071b16f25117fb6133480c6259227d54fc2a5ea0","35254a19bac666c291646e3640014bf0bb2ce11b","8225f757f6bb34624aa8e9658e9af295432c29b5","c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d","45e3f381d16e6d6db5449b7a44b7bdf294a8a822","03f34688ef4ee4239464633784235387e9bff4bb","e93a59b37f3d6282ebd4e5bad61924f6d75385d9","67bee729d046662c6ebd9d3d695823c9d820343a","2b6a2adddfbf70ea5cb3d9d749c1ef70db8c230a","4f5d7dc3d41236ed6a42cf4105cc79fa2fb0828d","5796ba3a657261a49ca9333865b8606980111371","5dc1fd136278c61771fbc43473c9e21638f0f46f","3d3b8fe8b5dd2d518f3a186f63553eb26a49968f","ccf415df5a83b343dae261286d29a40e8b80e6c6","3bf1d8b8ef045df1787c703fb6874f01709597da","06d51deada6e771a2571807a47dd991120c8dd1a","2b329183e93cb8c1c20c911c765d9a94f34b5ed5","8a9a10170ee907acb3e582742bec5fa09116f302","4a350352b2fb426530693d42f45effd049537cab","541c68e2c65f6dce6179801c9f92dc7803dc71b5","272665e79d39862fb014377f01b31fd3f91d9fb8","3f43314011db0e92e629d6ee6cf6a918e1e258bb","16a333a43d587802f95b5ec11de6c99314ae0c77","17307b4925959a15b2af55bdbb861df41ab879d2","e21a6a56e24f321286a50f2a1811ae66ef6245f2","1e1e274d9dde08a5e6a02daf86405d1cee5ec5cd","6e3f2fe073ba009336c0f13636bb5d137ac276f6","a5eb54e8a30406ac723ba258111239e156e25523","0add4da8389bd54958221def3ea16575ee9a007f","7e463877264e70d53c844cf4b1bf3b15baec8cfb","00a10855b9ab0f2c04226b7a05a7371dea26090b","2cb3bd0cff91c0afcbfb2cb10ce30313e0f70133","3b118bbb7c33eb8ca4605170e1bff2d459488df6","e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","89677d5cb68a4c020f4604e99b9a72f6c4532581","78d0ed0106446bfe78851e0ab3e7dd50e586c951","2c27683f6ecf8f2e94ff3254fa2630f90e3aacf9","2064c2a33eb0b4c8acd23fd60b98c12d6c1ad61e","400f6f4304b1c12efb22acf7e80a1784015cb23a","2579b2066d0fcbeda5498f5053f201b10a8e254b","0d3fc5945c3ee608af29080c00032b9afd232c36","6b570069f14c7588e066f7138e1f21af59d62e61","6bf0414dae4f10c7e54fb9e5e8af5d0d0cab290b","25f0625a92f6054b11057423111f9285c78376fe","23c2e6c185aa7f4274dc53196fa130dcd969988b","aa3d3c206b9d1726b9c387a00ddd2da2a9803fb6","3e4883a0ab6c5830785b83b5af74fcd63b1c556e","b3610b7650533631ef7a63adf21ec0e722f4d9c0","07c43a3ff15f2104022f2b1ca8ec4128a930b414","20a599c5f152c3f135b9b55a667e64c93ec8d477","6a0e1ce1ea01eb5eccf7c852a26f0e3db85e856e","0a82cb606e561ca6f43697ca4df4f449b82ddba6","10eb7bfa7687f498268bdf74b2f60020a151bdc6","1979271767a78ec766638b83efcc1fc8e96cc4a2","533ee188324b833e059cb59b654e6160776d5812","135c89b491f82bd4fd7de175ac778207f598342b","f8c8619ea7d68e604e40b814b40c72888a755e95","ceef5d5b82b822308a07998f19c6cca7ce0a80a6","00d3fadfadc977ba4b6511bec1b2a3026d877099","51691435d646c5b9152162f328d29511755328ba","146f6f6ed688c905fb6e346ad02332efd5464616","0e48b9fc023866b55219b26ec40ad88633020d2e","0d24a0695c9fc669e643bad51d4e14f056329dec","524b24a3523123785bedccfa0ef6c4857bf21b5f","18758118e78ca7f908021c55196fcba12dbb6283","3df87d901a0e3de3bd6a0c17c5683d753fbeaf64","83b625ae40c921c47255da5f2e24266e75a48d9b","db8c3cfaae04a14c1209d62953029b6fa53e23c7","0bed90488e8950e5bea85ffca8b48fab3b67c1a1","69a80e4c13f94be768cd113d11890e62ec4f1596","528c397b0e523c9a257d67bb05409b83b4cdd33d","4856e7719e566f2466369ae2031afb07c934d4d3","0a49b4de21363d86599d4a058aaf4f5aed019495","25eb839f39507fe6983ad3e692b2f8d93a5cb0cc","e39d1345a5aef8a5ee32c0a774de877b903de50c","7184a8b7be5ae88bca900542b0ab2c27b019f0ce","55657836eb554d4d42ba94db5baa274478a56d8a","3fd5307b2705971e543d16e7350651945b257f14","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","907e35afd9633cd0819e0add854081bae738d320","0b98d8f90c57c5aabf2bc264d47730cb2dfc897f","0d060f1edd7e79865f1a48a9c874439c4b10caea","195d0a8233a7a46329c742eaff56c276f847fadc","46018a894d533813d67322827ca51f78aed6d59e","52d6d89cccb79fc5f8fa3c0e4e9c4021645b70f0","73463ec3391df70d4c38de6a1e963830a85efbe6","161ba68b34bf2ec77384c9f1de04e9631d08af50","087f06a00bc3cbaa2aa16baf8a8a98eb50358d16","2964d30862d0402b0d0ad4a427067f69e4a52130","36818eaf6376aeeaffed2523d28bebae7c9db8d7","a3bd824b2d0077e1e2f97bb73942151d711cdf93","6e7dadd63455c194e3472bb181aaf509f89b9166","0406f0696982b26bdf2a456123439c8ddcf8afb1","2a0cec7f0f8b63f182ea0c52cb935580acabafcc","f59d3e260ae4c7e145b3a2732524ea74cf5deac2","14d904e10cca3f0cb6d9c623db1a50152cec6360","60fdfa95dee2dc3b125f1552aa5c080542707e8f","aeb38c8c4b826ad0dc63911dc5198f8c565298f5","78507d14c925e16d628bc643c75c449267fef64c","1683b929bd2c1fd5f326ee9501ef24c2b613b5ef","26c8d040bef85ad6dde55a8f71af936fb38356ad","8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac","5656fa5aa6e1beeb98703fc53ec112ad227c49ca","41f50d08e4c237d0f192f5c09f78d7e1d09d9cef","296bbeb2c43b183fb7d93c8a309b2c576f8b5fc7","27211ed68a7a00f1df0121fa1890a1b2acdd1a88","2f7ad26514bce4df6c8ebc42c90383ef3a974df4","069340a9fb06268b19e12a59de87547c9750fc79","5115f3ff1ac45486a50ad3834a40490f9ca4bcea","046a1302079f56b94c81457bf7fd21c3417a9f72"],"categories_a":["ArXiv","NIPS","ArXiv","KDD","EMNLP","ArXiv","ICPRAM","Multimodal User Interfaces","EMNLP","","Pattern Analysis and Machine Intelligence","AAAI","ICMI","ICML","ICML","Neurocomputing","ArXiv","ArXiv","AISTATS","","ArXiv","ArXiv","ACL","KDD","","ArXiv","NIPS","Computer Speech & Language","ICML","Medical Image Analysis","AISTATS","AAAI","NIPS","ArXiv","","TACL","","","ArXiv","ArXiv","ArXiv","ICML","ICONIP","NIPS","AISTATS","ArXiv","","NIPS","","ArXiv","ICML","Pattern Analysis and Machine Intelligence","ArXiv","ArXiv","NIPS","NIPS","ArXiv","ICPRAM","Neural Computation","Computer Speech & Language","ArXiv","NIPS","","ArXiv","NIPS","ICML","NIPS","ArXiv","","","CNN","NIPS","ArXiv","","ICML","Journal of Machine Learning Research","ArXiv","","ArXiv","ArXiv","KDD","ICML","ICML","NIPS","EMNLP","ArXiv","ArXiv","","Handbook on Neural Information Processing","ISMIR","AAAI","ArXiv","Computational Intelligence","ArXiv","ArXiv","Neural Computation","EMNLP","HLT-NAACL","INTERSPEECH","Computer Speech & Language","AISTATS","BMVC","ArXiv","ArXiv","Neural Computation","ACL","ICML","NIPS","NIPS","ArXiv","Neural Computation","ArXiv","ICML","NIPS","","Acoustics, Speech and Signal Processing","Journal of Machine Learning Research","AAAI","ACL","ArXiv","NIPS","ECCV","Nature","ICML","Machine Translation","","ArXiv","","WMT","","NIPS","ArXiv","NIPS","ArXiv","ArXiv","ISMIR","ICML","KDD","","","Neural computation","Machine Learning","Multimedia","ICML","NIPS","CVPR","ICML","ArXiv","ArXiv","ArXiv","AAAI","ArXiv","ArXiv","ArXiv","Pattern Analysis and Machine Intelligence","Journal of Machine Learning Research","ArXiv","Journal of Machine Learning Research","NIPS","ArXiv","ArXiv","NIPS"],"categories_b":["Journal","ML","Journal","AI","ComLing","Journal","Others","Others","ComLing","Others","Others","AI","Others","ML","ML","Others","Journal","Journal","ML","Others","Journal","Journal","ComLing","AI","Others","Journal","ML","Others","ML","Others","ML","AI","ML","Journal","Others","Others","Others","Others","Journal","Journal","Journal","ML","Others","ML","ML","Journal","Others","ML","Others","Journal","ML","Others","Journal","Journal","ML","ML","Journal","Others","ML","Others","Journal","ML","Others","Journal","ML","ML","ML","Journal","Others","Others","Others","ML","Journal","Others","ML","Others","Journal","Others","Journal","Journal","AI","ML","ML","ML","ComLing","Journal","Journal","Others","Others","Others","AI","Journal","Others","Journal","Journal","ML","ComLing","ComLing","Others","Others","ML","Others","Journal","Journal","ML","ComLing","ML","ML","ML","Journal","ML","Journal","ML","ML","Others","Others","Others","AI","ComLing","Journal","ML","ComVis","Others","ML","Others","Others","Journal","Others","Others","Others","ML","Journal","ML","Journal","Journal","Others","ML","AI","Others","Others","Others","Others","Others","ML","ML","ComVis","ML","Journal","Journal","Journal","AI","Journal","Journal","Journal","Others","Others","Journal","Others","ML","Journal","Journal","ML"]},"1721062":{"id":"1721062","name":"Nick Koudas","vec":[1.1657190988,0.0226340472],"weight":67,"email":"","org":"","sent":[],"received":[],"docs":["0d43fe67820c2f2a2e8adda04be04579fa85582c","157852dd14a9b518bf62fa6511be0f02f5d04a79","273121804df13771df2a6890f50e54e7ec419f75","87f34d2e08f9f079c189bdb1db1a4d750080a8cc","61845763cd9c8865870b17f6efa034e8fa862978","dc4a6027f4b868e544d2659181e52fa2e716a517","5cb40de2c796b9644346821c4cb600fd05e8e727","245cd97fc34fad6d6e7efb373874e5a42576adfe","58cfb95d06b6f4ccbf79ed1436c50187d1657252","5a5d6b0a9a7036c5417a618925c8e03d786e7d74","25241d105a39e9933c03298579248fc0874352d5","0aa8c4caef4ad23da2e1def1f22b74badd5be4e2","6556cea7f91c1221dd2693aba0d3dd45d2e09e6d","1600e8c878a2f42c71c753a353b8554b37ecf093","2185c1d1e465dd443b7ccb47c9b2d520da65a319","7400fe80e9e5670fd3e9de000a4afb2e6cc034fc","411b6492957d5367ef0df2ac926793fec0ca74ed","0dbac3fa87f3288454bb1c5a24e97c9fa4401cbe","1513c758ce244032348b751c40a100846e9aa321","1780dab3ed9253b596cb68a10cf3c2d9b8e5015f","45923f515e07b0250643350f8e4c19a74593dc91","29360559cbb9d8d4012c94262f3a79ea259abe2f","160cb0d0c0930f2782d8caa212d892845fc02aaf","5b88525bca03b88dfffc6e478127ddd72518d813","422955d1f193e70bebb546b33d53564421c5085a","323011da81c396825df6ec185ac17a21f2272ea0","46cd50a49397e37e5d83cb536f4b92a40293a467","9b1725d100e06bfd43ed5e996288e8c55c7d510b","a9d41b228cd14532fcabb87d65739a8213f69299","16cfb80ff6321fa0e277e0316398742475dcdc84","908f7db1b21067ee9f9442bccce4787d9ad5634f","5f4abcc6526ca79fb0d5df14527a9a5ce1c12621","9f5f5ecf0ee423f8493e3a600c6af9a7f8720d8e","d5b5db41929d791b4e911a5c7b0e29f60ee23253","124f6f240ba622cd74a9a0ea554ec2a5011eaadf","dfd790fd02afea1de60cba77a3b6b98640bea491","4cc6eb1716cabda5a55a8ed103f8e503717ac434","43b39679a5a37585e31260fc426120f4fe00538d","683422744d6a8f5e6bac38756f1f51a8ed16e2e3","40e6f827c72158e4b0a7dcc20f18448b4dd7e042","40709c0e27d0fe6f9265b6c29847e63392f4001d","19a2c31570534fbdde8290413ac465b3831e9e99","8e370946562e09f8895f8454a7719848cb176d79","0ec2c62121f8543864c84348835883df564aea14","232d3305b2c4c68a57b357c2cdea7c754891ebcb","be15028db5dc7f50cbca96d9e3d264802f385873","946232a2fe2549a2d2f52612b09ed8d66a74f34b","0899c2c3c31cd555425008d38a1a671f3e37724a","7cc937645ddee844b7171b88b587a8d9af98c9bc","d8c11242130cc02c9f9b7a9506c303ab05e2daea","54636e41ef8dc33f818aec7b9b02731eb75650d6","cb3e11ff1f5f03aa58e47e08640647ca133faab2","25a782a03c249d315b04a68cb1a3663ff8c893d5","1b74e7b2547373ff3df6fc293be7bed49075053d","78b729049a0135dc75a021ce5bbc127902253fde","43fc0302e46e91ffcbae49a2aae1a32dd0e927d9","3549ff6774d1dde5e6c80b2e97bc5b6ce578f677","3598b9eb49e226c1f49f123a058c1896459af339","7509931a7cafe9b0a61337d141d79d76eadaaeed","208edd765ff1fa2320dc87d2746b5b7460303bd8","0ed7adf241b11c34fe1354240a1b3cd9cd8a3705","4d82a5741b03a0a134380400d42552c53c89ac83","368fe0571a5611c2de27379747adff2066245937","441a89d393eba5091afe76fbaa7c35cbac1c31dc","2dfbd860542f79240f8f58cc042db1dfa562053a","3e4290774195402d31661d3c6b1f1da1bc55f5ae","c05d7d5ad5b154e5118e8e996a26898b6854eb6e"],"categories_a":["VLDB","","VLDB","Data Engineering","Data Engineering","SIGMOD","Data Engineering","VLDB","ICDE","SIGMOD","Computer Networks","SIGMOD","VLDB","SSPS","ICDE","VLDB","VLDB","","","Data Engineering","VLDB","VLDB","WWW","SIGMOD","ICDE","ICDE","ICDEW","Data Engineering","Stream Data Management","SIGMOD","","Data Engineering","WWW","SIGMOD","PODS","VLDB","Knowledge and Data Engineering","ICDE","PODS","Data Engineering","VLDB","ACM Trans. Database Syst.","","Knowledge and Data Engineering","IEEE Data Eng. Bull.","ICDE","DMSN","XSym","SIGMOD","SIGMOD","VLDB","ICDE","ICDM","ICDE","SIGMOD","ICDE","VLDB","SIGMOD","ACM Trans. Database Syst.","Data Engineering","VLDB","ICDE","ICDE","PODS","","SIGMOD","SIGMOD","CleanDB"],"categories_b":["DB","Others","DB","DB","DB","DB","DB","DB","DB","DB","Others","DB","DB","Others","DB","DB","DB","Others","Others","DB","DB","DB","Others","DB","DB","DB","Others","DB","Others","DB","Others","DB","Others","DB","Others","DB","Others","DB","Others","DB","DB","Others","Others","Others","DB","DB","Others","Others","DB","DB","DB","DB","Others","DB","DB","DB","DB","DB","Others","DB","DB","DB","DB","Others","Others","DB","DB","Others"]},"1695689":{"id":"1695689","name":"Geoffrey Hinton","vec":[-0.1656526123,0.062182197],"weight":5,"email":"","org":"","sent":[],"received":[],"docs":["0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3","30e2c349636920f96e72882e2e2c1d22b4739127","94b0e8e97c19ad0977d26e3e355d3ae09ad49365","5da15e6b30920ab20ce461232c00276e3b0f0bef","10eb7bfa7687f498268bdf74b2f60020a151bdc6"],"categories_a":["","ICML","UAI","","NIPS"],"categories_b":["Others","ML","Others","Others","ML"]},"1812612":{"id":"1812612","name":"Christopher D. Manning","vec":[-0.5739398271,-0.4100713632],"weight":86,"email":"","org":"","sent":[],"received":[],"docs":["02537a4d8781435a40f768f479a68e7a613dd26f","44d6ffe6535f75ccfd4a71a72ac2982077afcb1c","764ecb30e4ab5b1ad0cebec96019d9753c0c7f62","bfe947a04e9716fb6a581d9609f448aa5daa50f1","22825681ceb3f065098eb71ca9346b458b566252","683e5c3112c63aabaedafb7ba24f994baf07e0a4","5640ce300f3bea8347824e887f4b688b0f364873","961f6d5bb9b43833d7387e377b25d7a68118b23b","1007e2bfb377757a75f51a0edaf745edeabf3757","2febe441afece9e3f427fa4d541887fb89d0088f","0bdaf705c237a53b8db50dbdb62f1451774cb2a3","1e6f61c574fad2460d06d6f52295922f5c99857a","41a0c983e59c8fc5100b94c0802d74a64f86c741","fff3fa15f1fd07484edf8f355eb53e58d0ed8ccf","2982952a884a8d27fbcd800c010d8f4af564181b","8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092","710822be03043849bc016acc987695a3b8b34ecf","06d0a9697a0f0242dbdeeff08ec5266b74bfe457","3fa58a0dcbf7392e02f744a0f729ba8010b7334c","50d53cc562225549457cbc782546bfbe1ac6f0cf","0a832c69048ca30d4f528d221ba9899590a45f34","205b9f4891a2ead886604f161a44b3aed483609a","06554235c2c9361a14c0569206b58a355a63f01b","1297074f0e7d6c663c55f7580ca603473842eec9","1680eaa31f30911e7460ff694b51794391bc7514","0a22ce29e0fd1b0303ede3f5a94e1fe70764e455","098a170095b1e84b3a8eb673adfe6408fc0dcff5","b9392e6965137b2f81c81ff17addf3096faa4ffa","90df2b8696ad2ff4db8178fded1d17303f917d92","50654221b831a14e3ddad4579accc9c92685f29a","506ccb2fe5b756b35381b750fb996d014fbc8014","043e91679d7def20ece119117757ae7178d9eec9","1826fbbf12e399024296c1dfcc5758c1c810574c","65ad0e876216ea034b7958f016456e32666bc5c6","b90196005b0f1eb0953758c16869633d7ca3399d","1d4d14e78eb1536b84f6e851a67ecc5a6c0ae07f","2606e6a5759c030e259ebf3f4261b9c04a36a609","afa5f554eba815dc34be4409a595a53f514cb0bc","2d28108e25ba7b43f831210db064b1582d265503","1e9e87fc99430a82621810b3ce7db51e339be315","3ec4a5d29a3d897595f8f02aff0984d249c179c4","3bff03b7b0b0c4e8f6384dbb2a95e4338d156524","27e38351e48fe4b7da2775bf94341738bc4da07e","3b98dc29eb5f95470d12d19ae528674129ca0411","0b753d3c9fb80145c59f987749ebbde80a0de475","5aee809e16ec6d065e828fa04968b2d189c29e71","d579adf7a2c5cce3bb17482bb757f15bff45b131","2dde68fc4b1af7f21e18d7b9da5f964539f9b515","7da23d5b7c26dc1d6ac98682a74a8dd6a18bf309","d2ac9f33f16b421c3c5f38fddb8cdd8d30d7f886","bd4ddde7f9af802805fac652b44146698f0bda6e","142f38642629b9d268999ad876af482177d36697","17a40c553bdef82ccfc1bcc978e8e9d23230bfe5","1c7647fd9ab65e46e821bb0e045535500e2584d4","fe994dc0a8d9a365c5dc34e573dbf7c481b978df","4e88de2930a4435f737c3996287a90ff87b95c59","376eec3b84d31d672a3633ed06498fc9a9776f93","2f87364ce9255e345b35d431b00ba7271329cba8","1aeee8b379e6fcf10d33c15df78e135b16b24d5e","71898bd32548b98ceecf71bb30b9616e4be40b43","0d3233d858660aff451a6c2561a05378ed09725a","dd24d28c60d432cf95f5b8a64ba3dec75fcf93b7","0c23ebb3abf584fa5e0fde558584befc94fb5ea2","81b3b3fe994a9eda6d3f9d2149aa4492d1933975","16c7d4559b977d10b2a4d73fc3a9a972367e6f11","aad2c96c46064604c5ef729a491a47d12727ae9e","84458cd529206ec4897b90be191b089e8966ddc4","f21fcb8a04f764eb482479823a4933b09534a9b2","5158991247802f372b1646ef8751db46c307a2c8","91ff05cf0512789727dd010270e95dfc02301318","146b3649b693ae591c8953c0aae5264512c26ea3","10c51faad92ff64e93c3688f62edf9756dca20ad","35e024714f0702f8b77c8c2a916dedb91931708a","3d913755fcda4fde4e0a1365299429038e256c74","0825788b9b5a18e3dfea5b0af123b5e939a4f564","0e18e9dcfceedabcf14e6a615862c53ce8c0293d","167abf2c9eda9ce21907fcc188d2e41da37d9f0b","304d859dd545ab119ef1a6141d73a9c3797526d4","16ce9e96e5cf0b8fd3aeee137d548ff903711b97","00a28138c74869cfb8236a18a4dbe3a896f7a812","022029f1211970dbf29824f6510fe6dc7718190e","4def24ede97126c946c58e8af355a87a6775e268","5cceeb8a36f62cce92056b8e055ec9aaa7c3074c","db82c8e83d2ef333250126655511498b8114b18e","6e1fd106a17d67078a1d0ab8d5ef4f64d0feec1e","209ba9f612f34583a23b75c0e8dadc410c400bb2"],"categories_a":["ACL","EMNLP","EMNLP","ACL","CNLP","AISTATS","EMNLP","","","NIPS","EMNLP","","","CoNLL","","TAC","EMNLP","ArXiv","HLT-NAACL","NIPS","Machine Translation","HLT-NAACL","HLT-NAACL","NIPS","ACL","ICML","ACL","IWPT","NIPS","NAACL","ICML","","Pattern Recognition","NIPS","CoNLL","EMNLP","","NIPS","CAI","ACL","EACL","LREC","","Speech Communication","HLT-NAACL","ASRU","EACL","ICML","","HLT-NAACL","EMNLP","CNLP","CNLP","","ACL","ACL","","HLT-NAACL","EMNLP","SENSEVAL","NIPS","","CAI","","HLT-NAACL","","ACL","ACL","","CoNLL","LREC","ICML","EMNLP","","ACL","EMNLP","","","ACL","","","EMNLP","CoNLL","","AAAI","TACL"],"categories_b":["ComLing","ComLing","ComLing","ComLing","Others","ML","ComLing","Others","Others","ML","ComLing","Others","Others","Others","Others","Others","ComLing","Journal","ComLing","ML","Others","ComLing","ComLing","ML","ComLing","ML","ComLing","Others","ML","ComLing","ML","Others","Others","ML","Others","ComLing","Others","ML","AI","ComLing","Others","Others","Others","Others","ComLing","Others","Others","ML","Others","ComLing","ComLing","Others","Others","Others","ComLing","ComLing","Others","ComLing","ComLing","Others","ML","Others","AI","Others","ComLing","Others","ComLing","ComLing","Others","Others","Others","ML","ComLing","Others","ComLing","ComLing","Others","Others","ComLing","Others","Others","ComLing","Others","Others","AI","Others"]},"1760871":{"id":"1760871","name":"Aaron C. Courville","vec":[-0.680764859,0.5494206647],"weight":74,"email":"","org":"","sent":[],"received":[],"docs":["1e1e274d9dde08a5e6a02daf86405d1cee5ec5cd","8c3679bab6b379a3f487c68f631f55bb18292bc0","524b24a3523123785bedccfa0ef6c4857bf21b5f","6701889c81ad460f53a4d84361cd3d37b4e02743","83b625ae40c921c47255da5f2e24266e75a48d9b","db8c3cfaae04a14c1209d62953029b6fa53e23c7","2ae139b247057c02cda352f6661f46f7feb38e45","534f6ea4ce0127e5da7f1cafb6334b59ad15b83f","7e463877264e70d53c844cf4b1bf3b15baec8cfb","013cd20c0eaffb9cab80875a43086e0c3224fe20","12dd078034f72e4ebd9dfd9f80010d2ae7aaa337","f9c431f58565f874f76a024add2aa80717ec5cf5","517c31e5390d1d743aca69d16098be6ca30ebd2d","00a10855b9ab0f2c04226b7a05a7371dea26090b","2cb3bd0cff91c0afcbfb2cb10ce30313e0f70133","16a333a43d587802f95b5ec11de6c99314ae0c77","63cbac5a39cd926a806f60116b845f9bd70f5544","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","561008cb23d7a38a00806353ba3389c1b95395af","389bfe18dc161fda4980ec426ffaccec76a918bb","60744af2f89291897839852d66582b1b2a0be0bc","069340a9fb06268b19e12a59de87547c9750fc79","17f5c7411eeeeedf25b0db99a9130aa353aee4ba","62c76ca0b2790c34e85ba1cce09d47be317c7235","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","0808bb50993547a533ea5254e0454024d98c5e2f","78c91d969c55a4a61184f81001c376810cdbd541","221cf7b15aa771f9f9f8c0dc21899e22cd736fb8","01cbebbbcb973ae2d1d32a49fa1f1e0738153ba9","2064c2a33eb0b4c8acd23fd60b98c12d6c1ad61e","400f6f4304b1c12efb22acf7e80a1784015cb23a","654a3e53fb41d8168798ee0ee61dfab73739b1ed","46018a894d533813d67322827ca51f78aed6d59e","2579b2066d0fcbeda5498f5053f201b10a8e254b","6b570069f14c7588e066f7138e1f21af59d62e61","18cc17c06e34baaa3e196db07e20facdbb17026d","03f34688ef4ee4239464633784235387e9bff4bb","8b358a216f83edb259decd68127722a356cc8adf","3e4883a0ab6c5830785b83b5af74fcd63b1c556e","67bee729d046662c6ebd9d3d695823c9d820343a","73463ec3391df70d4c38de6a1e963830a85efbe6","2b6a2adddfbf70ea5cb3d9d749c1ef70db8c230a","4f5d7dc3d41236ed6a42cf4105cc79fa2fb0828d","5796ba3a657261a49ca9333865b8606980111371","b3610b7650533631ef7a63adf21ec0e722f4d9c0","6ca1898dac153b8cd500c0c2633675b05d3c638c","36818eaf6376aeeaffed2523d28bebae7c9db8d7","0406f0696982b26bdf2a456123439c8ddcf8afb1","6a0e1ce1ea01eb5eccf7c852a26f0e3db85e856e","5dc1fd136278c61771fbc43473c9e21638f0f46f","0a82cb606e561ca6f43697ca4df4f449b82ddba6","14d904e10cca3f0cb6d9c623db1a50152cec6360","2b35a5bccb678532412bcb471c16d6208353cf62","fd7447a976968cd4190c65edef8482f4f8e0cab9","f8c8619ea7d68e604e40b814b40c72888a755e95","aeb38c8c4b826ad0dc63911dc5198f8c565298f5","cf280435c471ee099148c4eb9eb2e106ccb2b218","78507d14c925e16d628bc643c75c449267fef64c","18758118e78ca7f908021c55196fcba12dbb6283","8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac","5656fa5aa6e1beeb98703fc53ec112ad227c49ca","2b329183e93cb8c1c20c911c765d9a94f34b5ed5","41f50d08e4c237d0f192f5c09f78d7e1d09d9cef","5e3b036c44f0c3b0b5eb1e99ef78644dcabcf2f0","e21a6a56e24f321286a50f2a1811ae66ef6245f2","4a350352b2fb426530693d42f45effd049537cab","146f6f6ed688c905fb6e346ad02332efd5464616","6bf6a3fd2c4c17c4326b81424ce19aba0a4b9c42","0a6193e3693356ae19c3ec3ed7c1375260a9e4a1","0d24a0695c9fc669e643bad51d4e14f056329dec","a4ab5ad02c4ed463e1e8a5c06e8b176ef3dea2ea","7a7a2658df5d66541305962d4c9d43078adadac6","bbb6bef6b2e48f5088f9bc0fd7cf7c07d514ea2a","046a1302079f56b94c81457bf7fd21c3417a9f72"],"categories_a":["ArXiv","ICCV","Multimodal User Interfaces","Pattern Analysis and Machine Intelligence","AAAI","ICMI","ICML","ArXiv","ArXiv","ArXiv","ICML","Medical Image Analysis","AISTATS","AAAI","ArXiv","ArXiv","ArXiv","ICML","ICONIP","NIPS","AISTATS","ArXiv","Pattern Analysis and Machine Intelligence","ArXiv","NIPS","ArXiv","RecSys","ArXiv","NIPS","ArXiv","ICML","ArXiv","NIPS","ArXiv","","ArXiv","ICML","ArXiv","ArXiv","Journal of Computer Vision","NIPS","ArXiv","NIPS","Handbook on Neural Information Processing","ArXiv","AAAI","CVPR","ArXiv","ArXiv","ICML","INTERSPEECH","AISTATS","ArXiv","ACL","ICML","Journal of Machine Learning Research","ArXiv","ECCV","Nature","ArXiv","ArXiv","NIPS","ArXiv","ArXiv","Multimedia","ICML","CVPR","ICML","ArXiv","ArXiv","AAAI","Pattern Analysis and Machine Intelligence","ArXiv","NIPS"],"categories_b":["Journal","Others","Others","Others","AI","Others","ML","Journal","Journal","Journal","ML","Others","ML","AI","Journal","Journal","Journal","ML","Others","ML","ML","Journal","Others","Journal","ML","Journal","Others","Journal","ML","Journal","ML","Journal","ML","Journal","Others","Journal","ML","Journal","Journal","Others","ML","Journal","ML","Others","Journal","AI","ComVis","Journal","Journal","ML","Others","ML","Journal","ComLing","ML","Others","Journal","ComVis","Others","Journal","Journal","ML","Journal","Journal","Others","ML","ComVis","ML","Journal","Journal","AI","Others","Journal","ML"]},"1777528":{"id":"1777528","name":"Hugo Larochelle","vec":[-0.3692551123,0.4646436774],"weight":35,"email":"","org":"","sent":[],"received":[],"docs":["6701889c81ad460f53a4d84361cd3d37b4e02743","3df87d901a0e3de3bd6a0c17c5683d753fbeaf64","0bed90488e8950e5bea85ffca8b48fab3b67c1a1","69a80e4c13f94be768cd113d11890e62ec4f1596","534f6ea4ce0127e5da7f1cafb6334b59ad15b83f","05fd1da7b2e34f86ec7f010bef068717ae964332","561008cb23d7a38a00806353ba3389c1b95395af","e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","5d363ec087445a226c8f64db03dd7efaa132e854","55657836eb554d4d42ba94db5baa274478a56d8a","907e35afd9633cd0819e0add854081bae738d320","221cf7b15aa771f9f9f8c0dc21899e22cd736fb8","3b2bf65ebee91249d1045709200a51d157b0176e","01cbebbbcb973ae2d1d32a49fa1f1e0738153ba9","74225938608ccf6a825df576d4dfe2fca9726200","46018a894d533813d67322827ca51f78aed6d59e","52d6d89cccb79fc5f8fa3c0e4e9c4021645b70f0","18cc17c06e34baaa3e196db07e20facdbb17026d","45e3f381d16e6d6db5449b7a44b7bdf294a8a822","03cb609fcfce6c60cbe3eb0dd8254069bf6d7573","1671a665c636bec7d2eaff137d74e9b7f074892f","8b358a216f83edb259decd68127722a356cc8adf","6e7dadd63455c194e3472bb181aaf509f89b9166","4700e447d1a0cddda978cd87177301790bb58efd","0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3","3bf1d8b8ef045df1787c703fb6874f01709597da","0375dbfedebc209b1f276f629c8958d37b48cac6","cf280435c471ee099148c4eb9eb2e106ccb2b218","94b0e8e97c19ad0977d26e3e355d3ae09ad49365","00d3fadfadc977ba4b6511bec1b2a3026d877099","5e3b036c44f0c3b0b5eb1e99ef78644dcabcf2f0","54c22be7d4bdcb761c42f2670242c8689e29206f","296bbeb2c43b183fb7d93c8a309b2c576f8b5fc7","272665e79d39862fb014377f01b31fd3f91d9fb8","bbb6bef6b2e48f5088f9bc0fd7cf7c07d514ea2a"],"categories_a":["ArXiv","ICCV","","Medical Image Analysis","AISTATS","","ArXiv","ArXiv","ArXiv","","Journal of Machine Learning Research","ICML","Journal of Computer Vision","ArXiv","NIPS","ArXiv","Computational Intelligence","CVPR","Neural Computation","ICML","Neural Computation","ArXiv","ICML","NIPS","","AAAI","NIPS","ICML","UAI","","","NIPS","ArXiv","Journal of Machine Learning Research","Journal of Machine Learning Research"],"categories_b":["Journal","Others","Others","Others","ML","Others","Journal","Journal","Journal","Others","Others","ML","Others","Journal","ML","Journal","Others","ComVis","ML","ML","ML","Journal","ML","ML","Others","AI","ML","ML","Others","Others","Others","ML","Journal","Others","Others"]}},"edges":[{"source":"1751762","target":"1979489","source_pos":[-0.4547877647,0.3463344699],"target_pos":[-0.6528437989,0.4857081873],"weight":43,"docs":["878634c30842b5812c56fe772719424bab69e7ad","35254a19bac666c291646e3640014bf0bb2ce11b","2c27683f6ecf8f2e94ff3254fa2630f90e3aacf9","7b4cd55ce7d7558eecb8d39f25a05cfd01ce4715","23c2e6c185aa7f4274dc53196fa130dcd969988b","2cb3bd0cff91c0afcbfb2cb10ce30313e0f70133","654a3e53fb41d8168798ee0ee61dfab73739b1ed","7e463877264e70d53c844cf4b1bf3b15baec8cfb","237202f570c6990f0c33c4db17b5df61d5ee6899","f59d3e260ae4c7e145b3a2732524ea74cf5deac2","aa3d3c206b9d1726b9c387a00ddd2da2a9803fb6","2a0cec7f0f8b63f182ea0c52cb935580acabafcc","25eb839f39507fe6983ad3e692b2f8d93a5cb0cc","78d0ed0106446bfe78851e0ab3e7dd50e586c951","0d75052f1d7350fa035a35566555ce7b65d1cd2f","20a599c5f152c3f135b9b55a667e64c93ec8d477","25f0625a92f6054b11057423111f9285c78376fe","03b18dcde7ba5bb0e87b2bdb68ab7af951daf162","4856e7719e566f2466369ae2031afb07c934d4d3","5115f3ff1ac45486a50ad3834a40490f9ca4bcea","c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d","1eb654f295d0e98e71bfbbe18c8a0cad3f6d15e9","06d51deada6e771a2571807a47dd991120c8dd1a","0851f0dad6de06dab86c82feb34dfa5b807129dc","e93a59b37f3d6282ebd4e5bad61924f6d75385d9","e50c2e4b8f7df21763f47c8941aa560c75d133b1","95a4edd3b7967da6c39bea61dc846052cd3a4837","032e9974cedb31f5c6e354626760e54e5ebf1e3c","ceef5d5b82b822308a07998f19c6cca7ce0a80a6","0d3fc5945c3ee608af29080c00032b9afd232c36","a3bd824b2d0077e1e2f97bb73942151d711cdf93","6b570069f14c7588e066f7138e1f21af59d62e61","26c8d040bef85ad6dde55a8f71af936fb38356ad","146f6f6ed688c905fb6e346ad02332efd5464616","135c89b491f82bd4fd7de175ac778207f598342b","4a350352b2fb426530693d42f45effd049537cab","071b16f25117fb6133480c6259227d54fc2a5ea0","0a49b4de21363d86599d4a058aaf4f5aed019495","087f06a00bc3cbaa2aa16baf8a8a98eb50358d16","89677d5cb68a4c020f4604e99b9a72f6c4532581","3f43314011db0e92e629d6ee6cf6a918e1e258bb","533ee188324b833e059cb59b654e6160776d5812","0b544dfe355a5070b60986319a3f51fb45d1348e"]},{"source":"34740554","target":"1979489","source_pos":[-0.7335400951,0.311203904],"target_pos":[-0.6528437989,0.4857081873],"weight":1,"docs":["6b570069f14c7588e066f7138e1f21af59d62e61"]},{"source":"1724875","target":"1979489","source_pos":[-0.4652251246,0.6115013037],"target_pos":[-0.6528437989,0.4857081873],"weight":1,"docs":["6b570069f14c7588e066f7138e1f21af59d62e61"]},{"source":"1777528","target":"1979489","source_pos":[-0.3692551123,0.4646436774],"target_pos":[-0.6528437989,0.4857081873],"weight":2,"docs":["18cc17c06e34baaa3e196db07e20facdbb17026d","5e3b036c44f0c3b0b5eb1e99ef78644dcabcf2f0"]},{"source":"1760871","target":"1979489","source_pos":[-0.680764859,0.5494206647],"target_pos":[-0.6528437989,0.4857081873],"weight":11,"docs":["2cb3bd0cff91c0afcbfb2cb10ce30313e0f70133","654a3e53fb41d8168798ee0ee61dfab73739b1ed","7e463877264e70d53c844cf4b1bf3b15baec8cfb","6b570069f14c7588e066f7138e1f21af59d62e61","146f6f6ed688c905fb6e346ad02332efd5464616","4a350352b2fb426530693d42f45effd049537cab","0a6193e3693356ae19c3ec3ed7c1375260a9e4a1","6bf6a3fd2c4c17c4326b81424ce19aba0a4b9c42","fd7447a976968cd4190c65edef8482f4f8e0cab9","18cc17c06e34baaa3e196db07e20facdbb17026d","5e3b036c44f0c3b0b5eb1e99ef78644dcabcf2f0"]},{"source":"1695689","target":"3216322","source_pos":[-0.1656526123,0.062182197],"target_pos":[-0.1358683317,-0.3621045525],"weight":1,"docs":["30e2c349636920f96e72882e2e2c1d22b4739127"]},{"source":"1812612","target":"3216322","source_pos":[-0.5739398271,-0.4100713632],"target_pos":[-0.1358683317,-0.3621045525],"weight":1,"docs":["2606e6a5759c030e259ebf3f4261b9c04a36a609"]},{"source":"1728478","target":"1728478","source_pos":[-0.1377496923,1.4013600653],"target_pos":[-0.1377496923,1.4013600653],"weight":1,"docs":["4d8d303fd622cf3bd0899bfe532fbee41202e718"]},{"source":"1704011","target":"1721062","source_pos":[0.9955500334,0.2100717154],"target_pos":[1.1657190988,0.0226340472],"weight":68,"docs":["273121804df13771df2a6890f50e54e7ec419f75","3e4290774195402d31661d3c6b1f1da1bc55f5ae","2185c1d1e465dd443b7ccb47c9b2d520da65a319","6556cea7f91c1221dd2693aba0d3dd45d2e09e6d","1600e8c878a2f42c71c753a353b8554b37ecf093","8e370946562e09f8895f8454a7719848cb176d79","dc4a6027f4b868e544d2659181e52fa2e716a517","7400fe80e9e5670fd3e9de000a4afb2e6cc034fc","0ed7adf241b11c34fe1354240a1b3cd9cd8a3705","2dfbd860542f79240f8f58cc042db1dfa562053a","a9d41b228cd14532fcabb87d65739a8213f69299","16cfb80ff6321fa0e277e0316398742475dcdc84","3549ff6774d1dde5e6c80b2e97bc5b6ce578f677","245cd97fc34fad6d6e7efb373874e5a42576adfe","45923f515e07b0250643350f8e4c19a74593dc91","87f34d2e08f9f079c189bdb1db1a4d750080a8cc","40709c0e27d0fe6f9265b6c29847e63392f4001d","441a89d393eba5091afe76fbaa7c35cbac1c31dc","78b729049a0135dc75a021ce5bbc127902253fde","5a5d6b0a9a7036c5417a618925c8e03d786e7d74","422955d1f193e70bebb546b33d53564421c5085a","3598b9eb49e226c1f49f123a058c1896459af339","5f4abcc6526ca79fb0d5df14527a9a5ce1c12621","be15028db5dc7f50cbca96d9e3d264802f385873","0dbac3fa87f3288454bb1c5a24e97c9fa4401cbe","0dbac3fa87f3288454bb1c5a24e97c9fa4401cbe","1b74e7b2547373ff3df6fc293be7bed49075053d","43b39679a5a37585e31260fc426120f4fe00538d","4d82a5741b03a0a134380400d42552c53c89ac83","25241d105a39e9933c03298579248fc0874352d5","683422744d6a8f5e6bac38756f1f51a8ed16e2e3","d8c11242130cc02c9f9b7a9506c303ab05e2daea","232d3305b2c4c68a57b357c2cdea7c754891ebcb","9b1725d100e06bfd43ed5e996288e8c55c7d510b","0ec2c62121f8543864c84348835883df564aea14","1513c758ce244032348b751c40a100846e9aa321","157852dd14a9b518bf62fa6511be0f02f5d04a79","368fe0571a5611c2de27379747adff2066245937","58cfb95d06b6f4ccbf79ed1436c50187d1657252","4cc6eb1716cabda5a55a8ed103f8e503717ac434","43fc0302e46e91ffcbae49a2aae1a32dd0e927d9","323011da81c396825df6ec185ac17a21f2272ea0","dfd790fd02afea1de60cba77a3b6b98640bea491","124f6f240ba622cd74a9a0ea554ec2a5011eaadf","5b88525bca03b88dfffc6e478127ddd72518d813","d5b5db41929d791b4e911a5c7b0e29f60ee23253","61845763cd9c8865870b17f6efa034e8fa862978","29360559cbb9d8d4012c94262f3a79ea259abe2f","7509931a7cafe9b0a61337d141d79d76eadaaeed","cb3e11ff1f5f03aa58e47e08640647ca133faab2","19a2c31570534fbdde8290413ac465b3831e9e99","c05d7d5ad5b154e5118e8e996a26898b6854eb6e","0899c2c3c31cd555425008d38a1a671f3e37724a","0d43fe67820c2f2a2e8adda04be04579fa85582c","208edd765ff1fa2320dc87d2746b5b7460303bd8","908f7db1b21067ee9f9442bccce4787d9ad5634f","40e6f827c72158e4b0a7dcc20f18448b4dd7e042","7cc937645ddee844b7171b88b587a8d9af98c9bc","54636e41ef8dc33f818aec7b9b02731eb75650d6","160cb0d0c0930f2782d8caa212d892845fc02aaf","411b6492957d5367ef0df2ac926793fec0ca74ed","46cd50a49397e37e5d83cb536f4b92a40293a467","9f5f5ecf0ee423f8493e3a600c6af9a7f8720d8e","5cb40de2c796b9644346821c4cb600fd05e8e727","25a782a03c249d315b04a68cb1a3663ff8c893d5","0aa8c4caef4ad23da2e1def1f22b74badd5be4e2","1780dab3ed9253b596cb68a10cf3c2d9b8e5015f","946232a2fe2549a2d2f52612b09ed8d66a74f34b"]},{"source":"1721062","target":"1721062","source_pos":[1.1657190988,0.0226340472],"target_pos":[1.1657190988,0.0226340472],"weight":1,"docs":["0dbac3fa87f3288454bb1c5a24e97c9fa4401cbe"]},{"source":"1751762","target":"1724875","source_pos":[-0.4547877647,0.3463344699],"target_pos":[-0.4652251246,0.6115013037],"weight":50,"docs":["0add4da8389bd54958221def3ea16575ee9a007f","e27d81521dc4e8b6ea93947c05ffccf06784f569","27211ed68a7a00f1df0121fa1890a1b2acdd1a88","2ae139b247057c02cda352f6661f46f7feb38e45","225721565af848f8360a8b01f7cc6d28dcc8e240","60fdfa95dee2dc3b125f1552aa5c080542707e8f","195d0a8233a7a46329c742eaff56c276f847fadc","70fa7c2a8a12509da107fe82b04d5e422120984f","3d3b8fe8b5dd2d518f3a186f63553eb26a49968f","65d994fb778a8d9e0f632659fb33a082949a50d3","51691435d646c5b9152162f328d29511755328ba","3b118bbb7c33eb8ca4605170e1bff2d459488df6","0e48b9fc023866b55219b26ec40ad88633020d2e","4b2283dd97a853812ca053547b4e067b2fa54512","013cd20c0eaffb9cab80875a43086e0c3224fe20","e39d1345a5aef8a5ee32c0a774de877b903de50c","a5eb54e8a30406ac723ba258111239e156e25523","3e1638d1ce6ec0d18c82fb3235a521db0a559da2","8225f757f6bb34624aa8e9658e9af295432c29b5","5dc1fd136278c61771fbc43473c9e21638f0f46f","01ad77475dedda66e3961f707a19fe588ef48f39","17307b4925959a15b2af55bdbb861df41ab879d2","0b98d8f90c57c5aabf2bc264d47730cb2dfc897f","3bf1d8b8ef045df1787c703fb6874f01709597da","3fd5307b2705971e543d16e7350651945b257f14","f9c431f58565f874f76a024add2aa80717ec5cf5","2964d30862d0402b0d0ad4a427067f69e4a52130","83b625ae40c921c47255da5f2e24266e75a48d9b","6bf0414dae4f10c7e54fb9e5e8af5d0d0cab290b","373cf414cc038516a2cff11d7caafa3ff1031c6d","0bb754879154b0a10ee9966636348d831081e151","541c68e2c65f6dce6179801c9f92dc7803dc71b5","5075636b9e0425358204211706adde1ecb5ba60b","ccf415df5a83b343dae261286d29a40e8b80e6c6","7184a8b7be5ae88bca900542b0ab2c27b019f0ce","6e3f2fe073ba009336c0f13636bb5d137ac276f6","6b570069f14c7588e066f7138e1f21af59d62e61","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","3b2bf65ebee91249d1045709200a51d157b0176e","07c43a3ff15f2104022f2b1ca8ec4128a930b414","0bed90488e8950e5bea85ffca8b48fab3b67c1a1","528c397b0e523c9a257d67bb05409b83b4cdd33d","e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","8a9a10170ee907acb3e582742bec5fa09116f302","161ba68b34bf2ec77384c9f1de04e9631d08af50","1683b929bd2c1fd5f326ee9501ef24c2b613b5ef","0375dbfedebc209b1f276f629c8958d37b48cac6","f8c8619ea7d68e604e40b814b40c72888a755e95","0d060f1edd7e79865f1a48a9c874439c4b10caea","400f6f4304b1c12efb22acf7e80a1784015cb23a"]},{"source":"34740554","target":"1724875","source_pos":[-0.7335400951,0.311203904],"target_pos":[-0.4652251246,0.6115013037],"weight":2,"docs":["6b570069f14c7588e066f7138e1f21af59d62e61","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4"]},{"source":"1777528","target":"1724875","source_pos":[-0.3692551123,0.4646436774],"target_pos":[-0.4652251246,0.6115013037],"weight":6,"docs":["3bf1d8b8ef045df1787c703fb6874f01709597da","3b2bf65ebee91249d1045709200a51d157b0176e","0bed90488e8950e5bea85ffca8b48fab3b67c1a1","e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","0375dbfedebc209b1f276f629c8958d37b48cac6","4700e447d1a0cddda978cd87177301790bb58efd"]},{"source":"1760871","target":"1724875","source_pos":[-0.680764859,0.5494206647],"target_pos":[-0.4652251246,0.6115013037],"weight":9,"docs":["2ae139b247057c02cda352f6661f46f7feb38e45","013cd20c0eaffb9cab80875a43086e0c3224fe20","5dc1fd136278c61771fbc43473c9e21638f0f46f","f9c431f58565f874f76a024add2aa80717ec5cf5","83b625ae40c921c47255da5f2e24266e75a48d9b","6b570069f14c7588e066f7138e1f21af59d62e61","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","f8c8619ea7d68e604e40b814b40c72888a755e95","400f6f4304b1c12efb22acf7e80a1784015cb23a"]},{"source":"1812612","target":"1701538","source_pos":[-0.5739398271,-0.4100713632],"target_pos":[-0.8443900689,-0.7329755387],"weight":21,"docs":["06554235c2c9361a14c0569206b58a355a63f01b","27e38351e48fe4b7da2775bf94341738bc4da07e","50d53cc562225549457cbc782546bfbe1ac6f0cf","209ba9f612f34583a23b75c0e8dadc410c400bb2","81b3b3fe994a9eda6d3f9d2149aa4492d1933975","764ecb30e4ab5b1ad0cebec96019d9753c0c7f62","1d4d14e78eb1536b84f6e851a67ecc5a6c0ae07f","db82c8e83d2ef333250126655511498b8114b18e","146b3649b693ae591c8953c0aae5264512c26ea3","3ec4a5d29a3d897595f8f02aff0984d249c179c4","142f38642629b9d268999ad876af482177d36697","098a170095b1e84b3a8eb673adfe6408fc0dcff5","1e9e87fc99430a82621810b3ce7db51e339be315","2febe441afece9e3f427fa4d541887fb89d0088f","167abf2c9eda9ce21907fcc188d2e41da37d9f0b","8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092","961f6d5bb9b43833d7387e377b25d7a68118b23b","0c23ebb3abf584fa5e0fde558584befc94fb5ea2","dd24d28c60d432cf95f5b8a64ba3dec75fcf93b7","3bff03b7b0b0c4e8f6384dbb2a95e4338d156524","3d913755fcda4fde4e0a1365299429038e256c74"]},{"source":"1751762","target":"1695689","source_pos":[-0.4547877647,0.3463344699],"target_pos":[-0.1656526123,0.062182197],"weight":2,"docs":["10eb7bfa7687f498268bdf74b2f60020a151bdc6","5da15e6b30920ab20ce461232c00276e3b0f0bef"]},{"source":"35609041","target":"2033016","source_pos":[0.2066195989,-0.5672670828],"target_pos":[0.5983948769,-0.4091214514],"weight":5,"docs":["ecdd236a1a31afcb2a3480a4c7dd5aa65b44551d","bd212586b54cb959c5d5058a4e16ee6b2cdae1cc","76765f06161c2f1b153056b86ce665102414ec73","71439278086b7fc84eedc8aeba377747e00b8403","6edc18adf0b57400726b8fc76b3428173d118826"]},{"source":"1695715","target":"2033016","source_pos":[0.8197548361,-0.4355332823],"target_pos":[0.5983948769,-0.4091214514],"weight":41,"docs":["5272c3e3c06481eb705241a6ed45f1d3905a1459","190b27eb221eec46b97373d80c5b96d95c4153c3","5578cabaef7b5dfc88443626e74d2e04951818f7","700e1d1b7c54711e96645691438d7fd9fda229ef","1e185688f3c5e73641633459dbc17b30d34fd307","9315cf87a57a346c416a98984b8bf82d3b85e679","1b4f5bb49dc95340a66c75e1c4c719f0f96439c8","75cbe27efe1c8b255102f641feba1871176c6c20","0658b1922d6a30a0191bb5fe13b0a9e43e49c999","2009b3c05dd9084a0a1c609abcedd81713ed7150","18c021c9cce95ed5615a060f590b8388b604e7c5","4fadcd3a0cb63edd255383f41ed3e280495333b2","4ca3c1040a4bc0d1ae200d26d1c18fbc6df0e95a","3845947052d99e45f0f3e7789b66c6582d098e48","265efd16c16dc942705246a57337d323722003cb","73f31354cc9058ddc2e47a1c585b753e1592c1bf","cab20ae17048106a84f14a6209f0286609cfa542","0c325c32039656541760b2d8f02be4636e026785","77ae592ec975dfb14d494667e0b019af19da9075","8c8b44029fbdac1572ae47b8eaab3929c9987098","314547de686f67e48c075ef8bbe6fe3a1d22dc92","315cd76e4a34b8fe27e20345abcd4fc27c7ee1ab","266228767a0d4aed021754b17947c926f4f8881b","1fd8bf7f21e2f117ce00ea9cf3005d4aa37f155a","4e649f746c007f858492a504ee2da156b3d25e36","136eefe33796c388a15d25ca03cb8d5077d14f37","412a9e54bbb31e12d008a9579994e009c5b40b46","3df5b6b3b2b648f3d8224322e6a0f127850df017","57e979025374da67fae37fbb81bbadecee68cc08","011f7f9ba9e6f9bc7f05994271725bc0fc9c3b94","0a5eace1ead92c5b1c47ccc89d1daf7bb5c5acd8","988cbe6332ce9b202ed3d8c6d03c678e9f7c3e2f","2525c025f11aec60cff428271ca851381b92008f","133eacaf0ad25b8364cb4510007d9363298e8adf","0bb7e5432be8ae6799eddf88d35e525b4b1744f7","d07a50bb119b594a6f8f9d9c574cbdfdfaa586d3","cc4296eecd15b83a3c9b9d3a84e91327bd14c7bd","0d6aa509c98b197a1f410f31e6fa760fde40d8a1","0f41cd1792db9ff879fdfffc746cf5a01adf207f","b2bf34c0c0007145a389e014b7ddaa3daa76f332","207def18c67fa8024741b7ae3cdc655b57f2053f"]},{"source":"1751762","target":"34740554","source_pos":[-0.4547877647,0.3463344699],"target_pos":[-0.7335400951,0.311203904],"weight":19,"docs":["1e1e274d9dde08a5e6a02daf86405d1cee5ec5cd","6ca1898dac153b8cd500c0c2633675b05d3c638c","db8c3cfaae04a14c1209d62953029b6fa53e23c7","5656fa5aa6e1beeb98703fc53ec112ad227c49ca","2f7ad26514bce4df6c8ebc42c90383ef3a974df4","6a0e1ce1ea01eb5eccf7c852a26f0e3db85e856e","aeb38c8c4b826ad0dc63911dc5198f8c565298f5","1979271767a78ec766638b83efcc1fc8e96cc4a2","855d0f722d75cc56a66a00ede18ace96bafee6bd","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","046a1302079f56b94c81457bf7fd21c3417a9f72","2b329183e93cb8c1c20c911c765d9a94f34b5ed5","16a333a43d587802f95b5ec11de6c99314ae0c77","6b570069f14c7588e066f7138e1f21af59d62e61","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","2064c2a33eb0b4c8acd23fd60b98c12d6c1ad61e","8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac","41f50d08e4c237d0f192f5c09f78d7e1d09d9cef","73463ec3391df70d4c38de6a1e963830a85efbe6"]},{"source":"1701538","target":"34740554","source_pos":[-0.8443900689,-0.7329755387],"target_pos":[-0.7335400951,0.311203904],"weight":1,"docs":["2337ff38e6cfb09e28c0958f07e2090c993ef6e8"]},{"source":"1760871","target":"34740554","source_pos":[-0.680764859,0.5494206647],"target_pos":[-0.7335400951,0.311203904],"weight":16,"docs":["1e1e274d9dde08a5e6a02daf86405d1cee5ec5cd","6ca1898dac153b8cd500c0c2633675b05d3c638c","db8c3cfaae04a14c1209d62953029b6fa53e23c7","5656fa5aa6e1beeb98703fc53ec112ad227c49ca","6a0e1ce1ea01eb5eccf7c852a26f0e3db85e856e","aeb38c8c4b826ad0dc63911dc5198f8c565298f5","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","046a1302079f56b94c81457bf7fd21c3417a9f72","2b329183e93cb8c1c20c911c765d9a94f34b5ed5","16a333a43d587802f95b5ec11de6c99314ae0c77","6b570069f14c7588e066f7138e1f21af59d62e61","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","2064c2a33eb0b4c8acd23fd60b98c12d6c1ad61e","8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac","41f50d08e4c237d0f192f5c09f78d7e1d09d9cef","73463ec3391df70d4c38de6a1e963830a85efbe6"]},{"source":"2166511","target":"38666915","source_pos":[-0.4358078653,-0.3409455194],"target_pos":[-0.3544321735,-0.7184337897],"weight":1,"docs":["7797d9e0e444246ba61d7e16cbbd654b07c117e8"]},{"source":"1812612","target":"38666915","source_pos":[-0.5739398271,-0.4100713632],"target_pos":[-0.3544321735,-0.7184337897],"weight":30,"docs":["1826fbbf12e399024296c1dfcc5758c1c810574c","1680eaa31f30911e7460ff694b51794391bc7514","2d28108e25ba7b43f831210db064b1582d265503","1007e2bfb377757a75f51a0edaf745edeabf3757","1aeee8b379e6fcf10d33c15df78e135b16b24d5e","10c51faad92ff64e93c3688f62edf9756dca20ad","d2ac9f33f16b421c3c5f38fddb8cdd8d30d7f886","b9392e6965137b2f81c81ff17addf3096faa4ffa","f21fcb8a04f764eb482479823a4933b09534a9b2","16ce9e96e5cf0b8fd3aeee137d548ff903711b97","205b9f4891a2ead886604f161a44b3aed483609a","304d859dd545ab119ef1a6141d73a9c3797526d4","2982952a884a8d27fbcd800c010d8f4af564181b","91ff05cf0512789727dd010270e95dfc02301318","fe994dc0a8d9a365c5dc34e573dbf7c481b978df","afa5f554eba815dc34be4409a595a53f514cb0bc","376eec3b84d31d672a3633ed06498fc9a9776f93","50654221b831a14e3ddad4579accc9c92685f29a","02537a4d8781435a40f768f479a68e7a613dd26f","3fa58a0dcbf7392e02f744a0f729ba8010b7334c","043e91679d7def20ece119117757ae7178d9eec9","0bdaf705c237a53b8db50dbdb62f1451774cb2a3","3b98dc29eb5f95470d12d19ae528674129ca0411","06d0a9697a0f0242dbdeeff08ec5266b74bfe457","1e6f61c574fad2460d06d6f52295922f5c99857a","1c7647fd9ab65e46e821bb0e045535500e2584d4","5158991247802f372b1646ef8751db46c307a2c8","aad2c96c46064604c5ef729a491a47d12727ae9e","6e1fd106a17d67078a1d0ab8d5ef4f64d0feec1e","4def24ede97126c946c58e8af355a87a6775e268"]},{"source":"3216322","target":"35609041","source_pos":[-0.1358683317,-0.3621045525],"target_pos":[0.2066195989,-0.5672670828],"weight":10,"docs":["3b2697d76f035304bfeb57f6a682224c87645065","87a937dff0ecffefd8a7e4ca4ce068aca9731a0a","5edf85b3068a0c95d5901623c7fd793ac1c92b38","115808104b2a9c3ab6e2e60582ab7e33b937b754","7060f6062ba1cbe9502eeaaf13779aa1664224bb","01a903739564f575b81c87f7a9e2cb7b609f7ada","0b4d3e59a0107f0dad22e74054bab1cf1ad9c32e","65c978a97f54cf255f01c6846d6c51b37c61f836","9fe93ec9c2f8ca5ddbc46c050ba4216c09154980","19150b001031cc6d964e83cd28553004f653cc24"]},{"source":"1701538","target":"5574038","source_pos":[-0.8443900689,-0.7329755387],"target_pos":[-0.7762889066,-0.4590462532],"weight":31,"docs":["4cf66c66ca404ea3b5474e78e6cf94f6bfd05b0d","2b7ef95822a4d577021df16607bf7b4a4514eb4b","0ad8d00860f3b65527a40377dd96b55564515167","5fc69f93422b11c944b2d53e9d2f93295eca3d19","60abc9657a01a128c5503673609bfe8e7637c00b","0bfbdafdfbcc268860fe54ae4d8f08d487bcc762","8e0eacf11a22b9705a262e908f17b1704fd21fa7","1e9e3694954822d720570c5c53136c8814b2c4be","6f568d757d2c1ab42f2006faa25690b74c3d2d44","0bddc08769e9ff5d42ed36336f69bb3b1f42e716","053912e76e50c9f923a1fc1c173f1365776060cc","420a94968bd8671c8071d9c735a095ee475d0b20","69de7827bb2d7508d0f15ce634d9594f8d0bb4b4","72dbacd0dbd157d881d284e922e21fcc5a2a4eeb","1e3d36a9184feec956371fe3fb03eb34747e4844","180920cd652b27cd9207fb797107b19d46e1100a","60f22ad725f041fff81d6371242485bbe5c3ebb6","1828eaa8750ee188111b92deae5c7f67323e723f","12244deb997152492d96c6246ec21b2b9804800d","007d73c91a1bf90d72eb59fbdd8791a4b009f363","02227c94dd41fe0b439e050d377b0beb5d427cda","29cb27e32d56f39b9fa5c5bf62225580530a014c","26cb14c9d22cf946314d685fe3541ef9f641e429","7f917de89d395e7dba3f2c5cb54b4a13c06a8dee","6c96ca73b381c4d8191ad73ea2fd9272ff0799c4","12806c298e01083a79db77927530367d85939907","f71c25642f53c52823aeef5d215993470991cf82","2329a46590b2036d508097143e65c1b77e571e8c","013613f6f3da2094a2fb590d8608ad0b44798baf","182015c5edff1956cbafbcb3e7bbe294aa54f9fc","13e10f545bd600b5bd8c36b29bcecec4a61d1b7f"]},{"source":"1812612","target":"2166511","source_pos":[-0.5739398271,-0.4100713632],"target_pos":[-0.4358078653,-0.3409455194],"weight":25,"docs":["06554235c2c9361a14c0569206b58a355a63f01b","27e38351e48fe4b7da2775bf94341738bc4da07e","50d53cc562225549457cbc782546bfbe1ac6f0cf","0b753d3c9fb80145c59f987749ebbde80a0de475","209ba9f612f34583a23b75c0e8dadc410c400bb2","5aee809e16ec6d065e828fa04968b2d189c29e71","0825788b9b5a18e3dfea5b0af123b5e939a4f564","81b3b3fe994a9eda6d3f9d2149aa4492d1933975","4e88de2930a4435f737c3996287a90ff87b95c59","3ec4a5d29a3d897595f8f02aff0984d249c179c4","142f38642629b9d268999ad876af482177d36697","84458cd529206ec4897b90be191b089e8966ddc4","17a40c553bdef82ccfc1bcc978e8e9d23230bfe5","1e9e87fc99430a82621810b3ce7db51e339be315","00a28138c74869cfb8236a18a4dbe3a896f7a812","bd4ddde7f9af802805fac652b44146698f0bda6e","65ad0e876216ea034b7958f016456e32666bc5c6","0d3233d858660aff451a6c2561a05378ed09725a","167abf2c9eda9ce21907fcc188d2e41da37d9f0b","8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092","961f6d5bb9b43833d7387e377b25d7a68118b23b","683e5c3112c63aabaedafb7ba24f994baf07e0a4","0c23ebb3abf584fa5e0fde558584befc94fb5ea2","3bff03b7b0b0c4e8f6384dbb2a95e4338d156524","0a22ce29e0fd1b0303ede3f5a94e1fe70764e455"]},{"source":"3216322","target":"2166511","source_pos":[-0.1358683317,-0.3621045525],"target_pos":[-0.4358078653,-0.3409455194],"weight":3,"docs":["0c881ea63ff12d85bc3192ce61f37abf701fdf38","098abe1eee6208b693daafe7183f017f9e71e409","38211dc39e41273c0007889202c69f841e02248a"]},{"source":"1701538","target":"2166511","source_pos":[-0.8443900689,-0.7329755387],"target_pos":[-0.4358078653,-0.3409455194],"weight":13,"docs":["06554235c2c9361a14c0569206b58a355a63f01b","27e38351e48fe4b7da2775bf94341738bc4da07e","50d53cc562225549457cbc782546bfbe1ac6f0cf","209ba9f612f34583a23b75c0e8dadc410c400bb2","81b3b3fe994a9eda6d3f9d2149aa4492d1933975","3ec4a5d29a3d897595f8f02aff0984d249c179c4","142f38642629b9d268999ad876af482177d36697","1e9e87fc99430a82621810b3ce7db51e339be315","167abf2c9eda9ce21907fcc188d2e41da37d9f0b","8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092","961f6d5bb9b43833d7387e377b25d7a68118b23b","0c23ebb3abf584fa5e0fde558584befc94fb5ea2","3bff03b7b0b0c4e8f6384dbb2a95e4338d156524"]},{"source":"1751762","target":"1777528","source_pos":[-0.4547877647,0.3463344699],"target_pos":[-0.3692551123,0.4646436774],"weight":24,"docs":["45e3f381d16e6d6db5449b7a44b7bdf294a8a822","46018a894d533813d67322827ca51f78aed6d59e","296bbeb2c43b183fb7d93c8a309b2c576f8b5fc7","5d363ec087445a226c8f64db03dd7efaa132e854","52d6d89cccb79fc5f8fa3c0e4e9c4021645b70f0","221cf7b15aa771f9f9f8c0dc21899e22cd736fb8","6e7dadd63455c194e3472bb181aaf509f89b9166","00d3fadfadc977ba4b6511bec1b2a3026d877099","05fd1da7b2e34f86ec7f010bef068717ae964332","3bf1d8b8ef045df1787c703fb6874f01709597da","69a80e4c13f94be768cd113d11890e62ec4f1596","272665e79d39862fb014377f01b31fd3f91d9fb8","1671a665c636bec7d2eaff137d74e9b7f074892f","907e35afd9633cd0819e0add854081bae738d320","534f6ea4ce0127e5da7f1cafb6334b59ad15b83f","54c22be7d4bdcb761c42f2670242c8689e29206f","3b2bf65ebee91249d1045709200a51d157b0176e","0bed90488e8950e5bea85ffca8b48fab3b67c1a1","e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","03cb609fcfce6c60cbe3eb0dd8254069bf6d7573","3df87d901a0e3de3bd6a0c17c5683d753fbeaf64","55657836eb554d4d42ba94db5baa274478a56d8a","74225938608ccf6a825df576d4dfe2fca9726200","0375dbfedebc209b1f276f629c8958d37b48cac6"]},{"source":"1695689","target":"1777528","source_pos":[-0.1656526123,0.062182197],"target_pos":[-0.3692551123,0.4646436774],"weight":2,"docs":["94b0e8e97c19ad0977d26e3e355d3ae09ad49365","0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3"]},{"source":"1760871","target":"1777528","source_pos":[-0.680764859,0.5494206647],"target_pos":[-0.3692551123,0.4646436774],"weight":11,"docs":["46018a894d533813d67322827ca51f78aed6d59e","221cf7b15aa771f9f9f8c0dc21899e22cd736fb8","534f6ea4ce0127e5da7f1cafb6334b59ad15b83f","01cbebbbcb973ae2d1d32a49fa1f1e0738153ba9","6701889c81ad460f53a4d84361cd3d37b4e02743","bbb6bef6b2e48f5088f9bc0fd7cf7c07d514ea2a","18cc17c06e34baaa3e196db07e20facdbb17026d","5e3b036c44f0c3b0b5eb1e99ef78644dcabcf2f0","8b358a216f83edb259decd68127722a356cc8adf","cf280435c471ee099148c4eb9eb2e106ccb2b218","561008cb23d7a38a00806353ba3389c1b95395af"]},{"source":"1704011","target":"1709589","source_pos":[0.9955500334,0.2100717154],"target_pos":[1.0310169709,0.5206970544],"weight":33,"docs":["0d60fce590359780f62017e472fd4523a5de3f5f","1298f6dd80c11766a3ff8ae1a4c21e12eadb7c17","7466d1a809d69435f29cebdd651029566eb702f7","31b58ddd4a558f7d9e41e19ef75993668ee69119","16cd112b8f3283a5f915b57c3e56ad10ffb45b09","2f207e27cad923b81253542a6f76439d49d87925","704dfc1636911592c5ea02a198077d2586bb8d23","6e3fbbeec1f3ad2a8efe9a7207ba92bee322b78e","39371e655d27527c74b6d18080dcd3107273ba8f","23d1255f1a0453ba96c6e6ef054903086f3656df","0bbf348983a12f21c1607b8a963357c33f18f120","771a8edc3a434af7e1af3b0c9424c7e199e4c646","023db6fa5c87852885dcee9531a1c843063c2347","35c14fd2453b0dc02a02dcf538fbd775e4993e81","7584a7f5e9ace396f2d9725cd9d9072a27c1b5a6","0c7650ed95c7d5fc90d769d5e34fd519313e8383","70325f4d1f7502bd06fa54484535b36c8b0c1290","5394ab5bf4b66bfb52f111525d6141a3226ba883","7fee2107608d10a494a469dbf01c807a9a95b09d","211e4b486a85f5250ef68aef8a4422811ec7a932","99126d196c0539e3014b61d91538a29ee2de8c54","1d8bfe40212695fdd05fd1e297f9584a30f477c4","374668781c1aa05391a900cc2841eb31c7bbce6a","79ed6d5157b73449dc663516fc2c8dfcb917b99e","80fe1abae2594a2cb5466d3646abbc57fb13d144","3d6e2d4c1cde661fea2c8b1c93528aa24bb87aff","33e46d74b066402f0ae912627dff5b700006f9b2","08e5cce87b6af2ae0ee09b58b86d81aa229b46d0","4893c08f1c76fe4eb7cad2bb496da526b0fa5dd3","260697ac804faf844241de404bab6e9460223c2f","47ec99aed698b07b24fdec85e4dbf584c4d2e707","5d458a1bff91aa598fcc47711e5cfd7a6dfa559d","14577a88dec08a9199ec4f7a878956a4e55c3dd5"]},{"source":"1704011","target":"1735239","source_pos":[0.9955500334,0.2100717154],"target_pos":[0.8630666473,-0.1668819201],"weight":30,"docs":["620d6ebb2e7604f6dd838210a6847239bc2670dd","64b05d23afb0bd5754fcb70d87233c9b4326eef2","219309d6413a29656178f99186157ceeb0932020","dc4a6027f4b868e544d2659181e52fa2e716a517","0ed7adf241b11c34fe1354240a1b3cd9cd8a3705","3549ff6774d1dde5e6c80b2e97bc5b6ce578f677","0bfdae2b923805e2453b2affcb1ba05994889d77","20dd934a841f775ed18e0d2f8e887f363cbb9099","55c3597ebe9ce20c3f2374be4dda465726335403","5f4abcc6526ca79fb0d5df14527a9a5ce1c12621","3c43d0c9a1d7864e7d819179940d857f8633d4ce","1ec8bcda29ae8159b0beb989fef08f6c332e4993","0e6c65b8c366d6f925b8221e01aceddbf931ef19","25241d105a39e9933c03298579248fc0874352d5","0e16487103cbe1d9bf2ce04ea5718c3fe9f08cf8","437b328a1a62eeb85aaa87514a13010a4df315ee","1513c758ce244032348b751c40a100846e9aa321","91277a62af159aa44e03482711149658576cae36","2a92f8fb49dcae62af89d0c1cb9a3001b1be7f5a","d0f04a8e3a21b0142f46067b98530ec833672e93","3c7da5d87776db54d04978700f01f3c41156d3e2","43fc0302e46e91ffcbae49a2aae1a32dd0e927d9","1d6156b17ef02f510394f78c3c57b9a1d93af2b4","277fc66c995fe0c5b79e68b328259159cb412c85","af5a145735128e1aa4782f8eb8981ff5e3292b2a","124f6f240ba622cd74a9a0ea554ec2a5011eaadf","1e4d40e94fa57419a1236a1866a0c3ef357f8b78","bf1d7df019f8c5355758a65002dedc9690539819","7cc937645ddee844b7171b88b587a8d9af98c9bc","3f099575d82b0248c6be9b042c286b3d81f01e63"]},{"source":"2033016","target":"1735239","source_pos":[0.5983948769,-0.4091214514],"target_pos":[0.8630666473,-0.1668819201],"weight":1,"docs":["314547de686f67e48c075ef8bbe6fe3a1d22dc92"]},{"source":"1695715","target":"1735239","source_pos":[0.8197548361,-0.4355332823],"target_pos":[0.8630666473,-0.1668819201],"weight":2,"docs":["314547de686f67e48c075ef8bbe6fe3a1d22dc92","019902292dff81eae20f3e87970dd7a1151d9405"]},{"source":"1721062","target":"1735239","source_pos":[1.1657190988,0.0226340472],"target_pos":[0.8630666473,-0.1668819201],"weight":9,"docs":["dc4a6027f4b868e544d2659181e52fa2e716a517","0ed7adf241b11c34fe1354240a1b3cd9cd8a3705","3549ff6774d1dde5e6c80b2e97bc5b6ce578f677","5f4abcc6526ca79fb0d5df14527a9a5ce1c12621","25241d105a39e9933c03298579248fc0874352d5","1513c758ce244032348b751c40a100846e9aa321","43fc0302e46e91ffcbae49a2aae1a32dd0e927d9","124f6f240ba622cd74a9a0ea554ec2a5011eaadf","7cc937645ddee844b7171b88b587a8d9af98c9bc"]},{"source":"1751762","target":"1760871","source_pos":[-0.4547877647,0.3463344699],"target_pos":[-0.680764859,0.5494206647],"weight":63,"docs":["2579b2066d0fcbeda5498f5053f201b10a8e254b","0406f0696982b26bdf2a456123439c8ddcf8afb1","4f5d7dc3d41236ed6a42cf4105cc79fa2fb0828d","069340a9fb06268b19e12a59de87547c9750fc79","46018a894d533813d67322827ca51f78aed6d59e","78507d14c925e16d628bc643c75c449267fef64c","2ae139b247057c02cda352f6661f46f7feb38e45","1e1e274d9dde08a5e6a02daf86405d1cee5ec5cd","0d24a0695c9fc669e643bad51d4e14f056329dec","17f5c7411eeeeedf25b0db99a9130aa353aee4ba","2cb3bd0cff91c0afcbfb2cb10ce30313e0f70133","654a3e53fb41d8168798ee0ee61dfab73739b1ed","6ca1898dac153b8cd500c0c2633675b05d3c638c","36818eaf6376aeeaffed2523d28bebae7c9db8d7","db8c3cfaae04a14c1209d62953029b6fa53e23c7","e21a6a56e24f321286a50f2a1811ae66ef6245f2","7e463877264e70d53c844cf4b1bf3b15baec8cfb","221cf7b15aa771f9f9f8c0dc21899e22cd736fb8","5656fa5aa6e1beeb98703fc53ec112ad227c49ca","00a10855b9ab0f2c04226b7a05a7371dea26090b","6a0e1ce1ea01eb5eccf7c852a26f0e3db85e856e","524b24a3523123785bedccfa0ef6c4857bf21b5f","aeb38c8c4b826ad0dc63911dc5198f8c565298f5","013cd20c0eaffb9cab80875a43086e0c3224fe20","14d904e10cca3f0cb6d9c623db1a50152cec6360","78c91d969c55a4a61184f81001c376810cdbd541","b3610b7650533631ef7a63adf21ec0e722f4d9c0","5796ba3a657261a49ca9333865b8606980111371","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","5dc1fd136278c61771fbc43473c9e21638f0f46f","046a1302079f56b94c81457bf7fd21c3417a9f72","2b6a2adddfbf70ea5cb3d9d749c1ef70db8c230a","517c31e5390d1d743aca69d16098be6ca30ebd2d","60744af2f89291897839852d66582b1b2a0be0bc","12dd078034f72e4ebd9dfd9f80010d2ae7aaa337","f9c431f58565f874f76a024add2aa80717ec5cf5","0808bb50993547a533ea5254e0454024d98c5e2f","2b329183e93cb8c1c20c911c765d9a94f34b5ed5","83b625ae40c921c47255da5f2e24266e75a48d9b","534f6ea4ce0127e5da7f1cafb6334b59ad15b83f","389bfe18dc161fda4980ec426ffaccec76a918bb","16a333a43d587802f95b5ec11de6c99314ae0c77","a4ab5ad02c4ed463e1e8a5c06e8b176ef3dea2ea","8c3679bab6b379a3f487c68f631f55bb18292bc0","18758118e78ca7f908021c55196fcba12dbb6283","6b570069f14c7588e066f7138e1f21af59d62e61","3e4883a0ab6c5830785b83b5af74fcd63b1c556e","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","03f34688ef4ee4239464633784235387e9bff4bb","146f6f6ed688c905fb6e346ad02332efd5464616","0a82cb606e561ca6f43697ca4df4f449b82ddba6","62c76ca0b2790c34e85ba1cce09d47be317c7235","4a350352b2fb426530693d42f45effd049537cab","2064c2a33eb0b4c8acd23fd60b98c12d6c1ad61e","67bee729d046662c6ebd9d3d695823c9d820343a","f8c8619ea7d68e604e40b814b40c72888a755e95","63cbac5a39cd926a806f60116b845f9bd70f5544","400f6f4304b1c12efb22acf7e80a1784015cb23a","8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac","41f50d08e4c237d0f192f5c09f78d7e1d09d9cef","73463ec3391df70d4c38de6a1e963830a85efbe6","7a7a2658df5d66541305962d4c9d43078adadac6","2b35a5bccb678532412bcb471c16d6208353cf62"]},{"source":"1812612","target":"1746807","source_pos":[-0.5739398271,-0.4100713632],"target_pos":[-0.8154760367,-0.2978378557],"weight":22,"docs":["0e18e9dcfceedabcf14e6a615862c53ce8c0293d","0a832c69048ca30d4f528d221ba9899590a45f34","2f87364ce9255e345b35d431b00ba7271329cba8","2dde68fc4b1af7f21e18d7b9da5f964539f9b515","710822be03043849bc016acc987695a3b8b34ecf","fff3fa15f1fd07484edf8f355eb53e58d0ed8ccf","16c7d4559b977d10b2a4d73fc3a9a972367e6f11","d579adf7a2c5cce3bb17482bb757f15bff45b131","022029f1211970dbf29824f6510fe6dc7718190e","44d6ffe6535f75ccfd4a71a72ac2982077afcb1c","5cceeb8a36f62cce92056b8e055ec9aaa7c3074c","506ccb2fe5b756b35381b750fb996d014fbc8014","5640ce300f3bea8347824e887f4b688b0f364873","b90196005b0f1eb0953758c16869633d7ca3399d","7da23d5b7c26dc1d6ac98682a74a8dd6a18bf309","41a0c983e59c8fc5100b94c0802d74a64f86c741","35e024714f0702f8b77c8c2a916dedb91931708a","bfe947a04e9716fb6a581d9609f448aa5daa50f1","90df2b8696ad2ff4db8178fded1d17303f917d92","71898bd32548b98ceecf71bb30b9616e4be40b43","1297074f0e7d6c663c55f7580ca603473842eec9","22825681ceb3f065098eb71ca9346b458b566252"]},{"source":"1701538","target":"1746807","source_pos":[-0.8443900689,-0.7329755387],"target_pos":[-0.8154760367,-0.2978378557],"weight":11,"docs":["62a4a6abc2a52889db51b56ef082d02784266985","26196511e307ec89466af06751a66ee2d95b6305","2d02141fd8c263d9c9a05d704c2d3e38525a7167","6f6b397267946e3faaac4e5fa587e1876fe69da2","a88966cdaeddd15d0a3de365a8f0a5931aebd756","9d4e4136353d6ed07cfb61565151e0e6e8fd7cf6","124acb2ceccd1d1cd9b4715e00d24eab31099aa8","45d814078179b7ac69086d8c7f40576244258549","79d1e429c241d0aa47a2194246256a5bc79585bc","62dcea12832692d2acdb46cb343ab03a04898542","2d5069a99bfa0b47c095bbb5cefd6dba974f72a7"]}],"category_a_index":{"":["219309d6413a29656178f99186157ceeb0932020","7400fe80e9e5670fd3e9de000a4afb2e6cc034fc","1979271767a78ec766638b83efcc1fc8e96cc4a2","3bf1d8b8ef045df1787c703fb6874f01709597da","cc4296eecd15b83a3c9b9d3a84e91327bd14c7bd","6e3f2fe073ba009336c0f13636bb5d137ac276f6","764ecb30e4ab5b1ad0cebec96019d9753c0c7f62","afa5f554eba815dc34be4409a595a53f514cb0bc","0ad8d00860f3b65527a40377dd96b55564515167","961f6d5bb9b43833d7387e377b25d7a68118b23b","3d913755fcda4fde4e0a1365299429038e256c74","60abc9657a01a128c5503673609bfe8e7637c00b","098a170095b1e84b3a8eb673adfe6408fc0dcff5","10eb7bfa7687f498268bdf74b2f60020a151bdc6","51691435d646c5b9152162f328d29511755328ba","74225938608ccf6a825df576d4dfe2fca9726200","01ad77475dedda66e3961f707a19fe588ef48f39","a5eb54e8a30406ac723ba258111239e156e25523","0dbac3fa87f3288454bb1c5a24e97c9fa4401cbe","190b27eb221eec46b97373d80c5b96d95c4153c3","70fa7c2a8a12509da107fe82b04d5e422120984f","1e9e3694954822d720570c5c53136c8814b2c4be","4e649f746c007f858492a504ee2da156b3d25e36","dc4a6027f4b868e544d2659181e52fa2e716a517","e21a6a56e24f321286a50f2a1811ae66ef6245f2","3fd5307b2705971e543d16e7350651945b257f14","76765f06161c2f1b153056b86ce665102414ec73","907e35afd9633cd0819e0add854081bae738d320","022029f1211970dbf29824f6510fe6dc7718190e","0b753d3c9fb80145c59f987749ebbde80a0de475","25eb839f39507fe6983ad3e692b2f8d93a5cb0cc","16ce9e96e5cf0b8fd3aeee137d548ff903711b97","3b118bbb7c33eb8ca4605170e1bff2d459488df6","ecdd236a1a31afcb2a3480a4c7dd5aa65b44551d","65ad0e876216ea034b7958f016456e32666bc5c6","043e91679d7def20ece119117757ae7178d9eec9","45923f515e07b0250643350f8e4c19a74593dc91","5272c3e3c06481eb705241a6ed45f1d3905a1459","fe994dc0a8d9a365c5dc34e573dbf7c481b978df","af5a145735128e1aa4782f8eb8981ff5e3292b2a","683e5c3112c63aabaedafb7ba24f994baf07e0a4","81b3b3fe994a9eda6d3f9d2149aa4492d1933975","d2ac9f33f16b421c3c5f38fddb8cdd8d30d7f886","5394ab5bf4b66bfb52f111525d6141a3226ba883","bd4ddde7f9af802805fac652b44146698f0bda6e","3df87d901a0e3de3bd6a0c17c5683d753fbeaf64","7797d9e0e444246ba61d7e16cbbd654b07c117e8","315cd76e4a34b8fe27e20345abcd4fc27c7ee1ab","4d8d303fd622cf3bd0899bfe532fbee41202e718","9315cf87a57a346c416a98984b8bf82d3b85e679","f21fcb8a04f764eb482479823a4933b09534a9b2","b9392e6965137b2f81c81ff17addf3096faa4ffa","52d6d89cccb79fc5f8fa3c0e4e9c4021645b70f0","4def24ede97126c946c58e8af355a87a6775e268","54c22be7d4bdcb761c42f2670242c8689e29206f","65d994fb778a8d9e0f632659fb33a082949a50d3","771a8edc3a434af7e1af3b0c9424c7e199e4c646","0a22ce29e0fd1b0303ede3f5a94e1fe70764e455","60fdfa95dee2dc3b125f1552aa5c080542707e8f","aad2c96c46064604c5ef729a491a47d12727ae9e","2a0cec7f0f8b63f182ea0c52cb935580acabafcc","30e2c349636920f96e72882e2e2c1d22b4739127","0ed7adf241b11c34fe1354240a1b3cd9cd8a3705","4fadcd3a0cb63edd255383f41ed3e280495333b2","dd24d28c60d432cf95f5b8a64ba3dec75fcf93b7","02227c94dd41fe0b439e050d377b0beb5d427cda"],"ICMI":["2ae139b247057c02cda352f6661f46f7feb38e45"],"Speech Communication":["35e024714f0702f8b77c8c2a916dedb91931708a"],"Document Analysis and Recognition":["12244deb997152492d96c6246ec21b2b9804800d"],"CleanDB":["d5b5db41929d791b4e911a5c7b0e29f60ee23253"],"KDD":["0d060f1edd7e79865f1a48a9c874439c4b10caea","e93a59b37f3d6282ebd4e5bad61924f6d75385d9","2964d30862d0402b0d0ad4a427067f69e4a52130","80fe1abae2594a2cb5466d3646abbc57fb13d144","2c27683f6ecf8f2e94ff3254fa2630f90e3aacf9"],"ICONIP":["db8c3cfaae04a14c1209d62953029b6fa53e23c7"],"Computer Networks":["946232a2fe2549a2d2f52612b09ed8d66a74f34b"],"Machine Translation":["2f87364ce9255e345b35d431b00ba7271329cba8","032e9974cedb31f5c6e354626760e54e5ebf1e3c"],"Journal of Computer Vision":["561008cb23d7a38a00806353ba3389c1b95395af","0b4d3e59a0107f0dad22e74054bab1cf1ad9c32e","3b2697d76f035304bfeb57f6a682224c87645065"],"VLDB":["46cd50a49397e37e5d83cb536f4b92a40293a467","29360559cbb9d8d4012c94262f3a79ea259abe2f","0bfdae2b923805e2453b2affcb1ba05994889d77","3df5b6b3b2b648f3d8224322e6a0f127850df017","988cbe6332ce9b202ed3d8c6d03c678e9f7c3e2f","1780dab3ed9253b596cb68a10cf3c2d9b8e5015f","3f099575d82b0248c6be9b042c286b3d81f01e63","0c325c32039656541760b2d8f02be4636e026785","d8c11242130cc02c9f9b7a9506c303ab05e2daea","d0f04a8e3a21b0142f46067b98530ec833672e93","441a89d393eba5091afe76fbaa7c35cbac1c31dc","a9d41b228cd14532fcabb87d65739a8213f69299","704dfc1636911592c5ea02a198077d2586bb8d23","124f6f240ba622cd74a9a0ea554ec2a5011eaadf","8e370946562e09f8895f8454a7719848cb176d79","1b4f5bb49dc95340a66c75e1c4c719f0f96439c8","136eefe33796c388a15d25ca03cb8d5077d14f37","33e46d74b066402f0ae912627dff5b700006f9b2","412a9e54bbb31e12d008a9579994e009c5b40b46","64b05d23afb0bd5754fcb70d87233c9b4326eef2","3c7da5d87776db54d04978700f01f3c41156d3e2","77ae592ec975dfb14d494667e0b019af19da9075","3d6e2d4c1cde661fea2c8b1c93528aa24bb87aff","265efd16c16dc942705246a57337d323722003cb","211e4b486a85f5250ef68aef8a4422811ec7a932","19a2c31570534fbdde8290413ac465b3831e9e99","bf1d7df019f8c5355758a65002dedc9690539819","87f34d2e08f9f079c189bdb1db1a4d750080a8cc","273121804df13771df2a6890f50e54e7ec419f75","73f31354cc9058ddc2e47a1c585b753e1592c1bf","207def18c67fa8024741b7ae3cdc655b57f2053f","208edd765ff1fa2320dc87d2746b5b7460303bd8","b2bf34c0c0007145a389e014b7ddaa3daa76f332","0aa8c4caef4ad23da2e1def1f22b74badd5be4e2","2f207e27cad923b81253542a6f76439d49d87925","16cd112b8f3283a5f915b57c3e56ad10ffb45b09","437b328a1a62eeb85aaa87514a13010a4df315ee"],"ISER":["013613f6f3da2094a2fb590d8608ad0b44798baf","0bddc08769e9ff5d42ed36336f69bb3b1f42e716"],"Stream Data Management":["0d43fe67820c2f2a2e8adda04be04579fa85582c"],"DMSN":["9f5f5ecf0ee423f8493e3a600c6af9a7f8720d8e"],"CVPR":["01a903739564f575b81c87f7a9e2cb7b609f7ada","115808104b2a9c3ab6e2e60582ab7e33b937b754","6701889c81ad460f53a4d84361cd3d37b4e02743","0c881ea63ff12d85bc3192ce61f37abf701fdf38","098abe1eee6208b693daafe7183f017f9e71e409","38211dc39e41273c0007889202c69f841e02248a","2cb3bd0cff91c0afcbfb2cb10ce30313e0f70133"],"Knowledge and Data Engineering":["368fe0571a5611c2de27379747adff2066245937","c05d7d5ad5b154e5118e8e996a26898b6854eb6e"],"ECCV":["19150b001031cc6d964e83cd28553004f653cc24","f9c431f58565f874f76a024add2aa80717ec5cf5"],"Encyclopedia of Machine Learning":["180920cd652b27cd9207fb797107b19d46e1100a"],"SIGMOD":["023db6fa5c87852885dcee9531a1c843063c2347","3549ff6774d1dde5e6c80b2e97bc5b6ce578f677","1298f6dd80c11766a3ff8ae1a4c21e12eadb7c17","bd212586b54cb959c5d5058a4e16ee6b2cdae1cc","20dd934a841f775ed18e0d2f8e887f363cbb9099","700e1d1b7c54711e96645691438d7fd9fda229ef","0e16487103cbe1d9bf2ce04ea5718c3fe9f08cf8","245cd97fc34fad6d6e7efb373874e5a42576adfe","157852dd14a9b518bf62fa6511be0f02f5d04a79","18c021c9cce95ed5615a060f590b8388b604e7c5","160cb0d0c0930f2782d8caa212d892845fc02aaf","23d1255f1a0453ba96c6e6ef054903086f3656df","0ec2c62121f8543864c84348835883df564aea14","4ca3c1040a4bc0d1ae200d26d1c18fbc6df0e95a","5d458a1bff91aa598fcc47711e5cfd7a6dfa559d","cb3e11ff1f5f03aa58e47e08640647ca133faab2","260697ac804faf844241de404bab6e9460223c2f","8c8b44029fbdac1572ae47b8eaab3929c9987098","133eacaf0ad25b8364cb4510007d9363298e8adf","5a5d6b0a9a7036c5417a618925c8e03d786e7d74","7cc937645ddee844b7171b88b587a8d9af98c9bc","019902292dff81eae20f3e87970dd7a1151d9405","0e6c65b8c366d6f925b8221e01aceddbf931ef19","908f7db1b21067ee9f9442bccce4787d9ad5634f","266228767a0d4aed021754b17947c926f4f8881b","5578cabaef7b5dfc88443626e74d2e04951818f7","57e979025374da67fae37fbb81bbadecee68cc08","40709c0e27d0fe6f9265b6c29847e63392f4001d","6edc18adf0b57400726b8fc76b3428173d118826","683422744d6a8f5e6bac38756f1f51a8ed16e2e3","3e4290774195402d31661d3c6b1f1da1bc55f5ae","1e4d40e94fa57419a1236a1866a0c3ef357f8b78"],"Multimodal User Interfaces":["5dc1fd136278c61771fbc43473c9e21638f0f46f"],"ICDT":["6e3fbbeec1f3ad2a8efe9a7207ba92bee322b78e"],"LREC":["41a0c983e59c8fc5100b94c0802d74a64f86c741","710822be03043849bc016acc987695a3b8b34ecf"],"Nature":["046a1302079f56b94c81457bf7fd21c3417a9f72"],"NIPS":["225721565af848f8360a8b01f7cc6d28dcc8e240","50d53cc562225549457cbc782546bfbe1ac6f0cf","17307b4925959a15b2af55bdbb861df41ab879d2","26c8d040bef85ad6dde55a8f71af936fb38356ad","1c7647fd9ab65e46e821bb0e045535500e2584d4","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","8a9a10170ee907acb3e582742bec5fa09116f302","14d904e10cca3f0cb6d9c623db1a50152cec6360","373cf414cc038516a2cff11d7caafa3ff1031c6d","167abf2c9eda9ce21907fcc188d2e41da37d9f0b","5656fa5aa6e1beeb98703fc53ec112ad227c49ca","4856e7719e566f2466369ae2031afb07c934d4d3","12dd078034f72e4ebd9dfd9f80010d2ae7aaa337","5115f3ff1ac45486a50ad3834a40490f9ca4bcea","3ec4a5d29a3d897595f8f02aff0984d249c179c4","84458cd529206ec4897b90be191b089e8966ddc4","06554235c2c9361a14c0569206b58a355a63f01b","0808bb50993547a533ea5254e0454024d98c5e2f","8b358a216f83edb259decd68127722a356cc8adf","0bfbdafdfbcc268860fe54ae4d8f08d487bcc762","06d0a9697a0f0242dbdeeff08ec5266b74bfe457","0e48b9fc023866b55219b26ec40ad88633020d2e","5075636b9e0425358204211706adde1ecb5ba60b","2337ff38e6cfb09e28c0958f07e2090c993ef6e8","03cb609fcfce6c60cbe3eb0dd8254069bf6d7573","0bed90488e8950e5bea85ffca8b48fab3b67c1a1","182015c5edff1956cbafbcb3e7bbe294aa54f9fc","27211ed68a7a00f1df0121fa1890a1b2acdd1a88","3e4883a0ab6c5830785b83b5af74fcd63b1c556e","0b98d8f90c57c5aabf2bc264d47730cb2dfc897f","0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3","aa3d3c206b9d1726b9c387a00ddd2da2a9803fb6","2b7ef95822a4d577021df16607bf7b4a4514eb4b","2d02141fd8c263d9c9a05d704c2d3e38525a7167","00a10855b9ab0f2c04226b7a05a7371dea26090b"],"ICML":["29cb27e32d56f39b9fa5c5bf62225580530a014c","524b24a3523123785bedccfa0ef6c4857bf21b5f","0851f0dad6de06dab86c82feb34dfa5b807129dc","053912e76e50c9f923a1fc1c173f1365776060cc","5796ba3a657261a49ca9333865b8606980111371","146f6f6ed688c905fb6e346ad02332efd5464616","6f568d757d2c1ab42f2006faa25690b74c3d2d44","195d0a8233a7a46329c742eaff56c276f847fadc","7f917de89d395e7dba3f2c5cb54b4a13c06a8dee","60f22ad725f041fff81d6371242485bbe5c3ebb6","50654221b831a14e3ddad4579accc9c92685f29a","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","8e0eacf11a22b9705a262e908f17b1704fd21fa7","2579b2066d0fcbeda5498f5053f201b10a8e254b","0c23ebb3abf584fa5e0fde558584befc94fb5ea2","13e10f545bd600b5bd8c36b29bcecec4a61d1b7f","3b2bf65ebee91249d1045709200a51d157b0176e","5da15e6b30920ab20ce461232c00276e3b0f0bef","376eec3b84d31d672a3633ed06498fc9a9776f93","01cbebbbcb973ae2d1d32a49fa1f1e0738153ba9","07c43a3ff15f2104022f2b1ca8ec4128a930b414","534f6ea4ce0127e5da7f1cafb6334b59ad15b83f","6e7dadd63455c194e3472bb181aaf509f89b9166","2febe441afece9e3f427fa4d541887fb89d0088f","6bf0414dae4f10c7e54fb9e5e8af5d0d0cab290b","8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac","16a333a43d587802f95b5ec11de6c99314ae0c77"],"HPEC":["3845947052d99e45f0f3e7789b66c6582d098e48","75cbe27efe1c8b255102f641feba1871176c6c20"],"CHI":["71439278086b7fc84eedc8aeba377747e00b8403","87a937dff0ecffefd8a7e4ca4ce068aca9731a0a","5edf85b3068a0c95d5901623c7fd793ac1c92b38"],"NAACL":["d579adf7a2c5cce3bb17482bb757f15bff45b131"],"EACL":["71898bd32548b98ceecf71bb30b9616e4be40b43","0e18e9dcfceedabcf14e6a615862c53ce8c0293d"],"ACL":["1007e2bfb377757a75f51a0edaf745edeabf3757","209ba9f612f34583a23b75c0e8dadc410c400bb2","03b18dcde7ba5bb0e87b2bdb68ab7af951daf162","5640ce300f3bea8347824e887f4b688b0f364873","1aeee8b379e6fcf10d33c15df78e135b16b24d5e","142f38642629b9d268999ad876af482177d36697","62a4a6abc2a52889db51b56ef082d02784266985","10c51faad92ff64e93c3688f62edf9756dca20ad","506ccb2fe5b756b35381b750fb996d014fbc8014","67bee729d046662c6ebd9d3d695823c9d820343a","1e6f61c574fad2460d06d6f52295922f5c99857a","16c7d4559b977d10b2a4d73fc3a9a972367e6f11","0a49b4de21363d86599d4a058aaf4f5aed019495","1680eaa31f30911e7460ff694b51794391bc7514","4e88de2930a4435f737c3996287a90ff87b95c59"],"AAAI":["60744af2f89291897839852d66582b1b2a0be0bc","2064c2a33eb0b4c8acd23fd60b98c12d6c1ad61e","17f5c7411eeeeedf25b0db99a9130aa353aee4ba","00d3fadfadc977ba4b6511bec1b2a3026d877099","36818eaf6376aeeaffed2523d28bebae7c9db8d7","1d4d14e78eb1536b84f6e851a67ecc5a6c0ae07f"],"Medical Image Analysis":["46018a894d533813d67322827ca51f78aed6d59e"],"ICDEW":["4893c08f1c76fe4eb7cad2bb496da526b0fa5dd3","3598b9eb49e226c1f49f123a058c1896459af339"],"Handbook on Neural Information Processing":["03f34688ef4ee4239464633784235387e9bff4bb"],"WOSN":["0d60fce590359780f62017e472fd4523a5de3f5f"],"Robotics and Automation":["4cf66c66ca404ea3b5474e78e6cf94f6bfd05b0d","1e3d36a9184feec956371fe3fb03eb34747e4844"],"CIDR":["011f7f9ba9e6f9bc7f05994271725bc0fc9c3b94"],"WWW":["78b729049a0135dc75a021ce5bbc127902253fde","1600e8c878a2f42c71c753a353b8554b37ecf093","1d8bfe40212695fdd05fd1e297f9584a30f477c4"],"RecSys":["6bf6a3fd2c4c17c4326b81424ce19aba0a4b9c42"],"IWPT":["91ff05cf0512789727dd010270e95dfc02301318"],"ICPR":["26cb14c9d22cf946314d685fe3541ef9f641e429"],"Intelligent Robots and Systems":["1828eaa8750ee188111b92deae5c7f67323e723f"],"Neural computation":["ceef5d5b82b822308a07998f19c6cca7ce0a80a6"],"ICPRAM":["541c68e2c65f6dce6179801c9f92dc7803dc71b5","e39d1345a5aef8a5ee32c0a774de877b903de50c"],"CoNLL":["6e1fd106a17d67078a1d0ab8d5ef4f64d0feec1e","00a28138c74869cfb8236a18a4dbe3a896f7a812","bfe947a04e9716fb6a581d9609f448aa5daa50f1","5158991247802f372b1646ef8751db46c307a2c8"],"Journal of Machine Learning Research":["e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","400f6f4304b1c12efb22acf7e80a1784015cb23a","1671a665c636bec7d2eaff137d74e9b7f074892f","05fd1da7b2e34f86ec7f010bef068717ae964332"],"CAI":["304d859dd545ab119ef1a6141d73a9c3797526d4","2d28108e25ba7b43f831210db064b1582d265503"],"BMVC":["0d75052f1d7350fa035a35566555ce7b65d1cd2f"],"CIKM":["35c14fd2453b0dc02a02dcf538fbd775e4993e81","91277a62af159aa44e03482711149658576cae36"],"DaMoN":["0a5eace1ead92c5b1c47ccc89d1daf7bb5c5acd8"],"PODS":["2a92f8fb49dcae62af89d0c1cb9a3001b1be7f5a","14577a88dec08a9199ec4f7a878956a4e55c3dd5","6556cea7f91c1221dd2693aba0d3dd45d2e09e6d","7509931a7cafe9b0a61337d141d79d76eadaaeed","dfd790fd02afea1de60cba77a3b6b98640bea491"],"SSDBM":["08e5cce87b6af2ae0ee09b58b86d81aa229b46d0"],"Robotics: Science and Systems":["420a94968bd8671c8071d9c735a095ee475d0b20"],"ISMIR":["7184a8b7be5ae88bca900542b0ab2c27b019f0ce","e27d81521dc4e8b6ea93947c05ffccf06784f569"],"XSym":["54636e41ef8dc33f818aec7b9b02731eb75650d6"],"SENSEVAL":["02537a4d8781435a40f768f479a68e7a613dd26f"],"WMT":["3f43314011db0e92e629d6ee6cf6a918e1e258bb"],"AISTATS":["5aee809e16ec6d065e828fa04968b2d189c29e71","ccf415df5a83b343dae261286d29a40e8b80e6c6","389bfe18dc161fda4980ec426ffaccec76a918bb","4700e447d1a0cddda978cd87177301790bb58efd","83b625ae40c921c47255da5f2e24266e75a48d9b","78c91d969c55a4a61184f81001c376810cdbd541","72dbacd0dbd157d881d284e922e21fcc5a2a4eeb"],"Computer Speech & Language":["e50c2e4b8f7df21763f47c8941aa560c75d133b1","124acb2ceccd1d1cd9b4715e00d24eab31099aa8","a3bd824b2d0077e1e2f97bb73942151d711cdf93","7b4cd55ce7d7558eecb8d39f25a05cfd01ce4715"],"EDBT":["1ec8bcda29ae8159b0beb989fef08f6c332e4993","620d6ebb2e7604f6dd838210a6847239bc2670dd"],"Multimedia":["654a3e53fb41d8168798ee0ee61dfab73739b1ed"],"CSCW":["7060f6062ba1cbe9502eeaaf13779aa1664224bb"],"ICDE":["1d6156b17ef02f510394f78c3c57b9a1d93af2b4","5cb40de2c796b9644346821c4cb600fd05e8e727","2185c1d1e465dd443b7ccb47c9b2d520da65a319","4d82a5741b03a0a134380400d42552c53c89ac83","1513c758ce244032348b751c40a100846e9aa321","374668781c1aa05391a900cc2841eb31c7bbce6a","16cfb80ff6321fa0e277e0316398742475dcdc84","2009b3c05dd9084a0a1c609abcedd81713ed7150","2dfbd860542f79240f8f58cc042db1dfa562053a","411b6492957d5367ef0df2ac926793fec0ca74ed","25241d105a39e9933c03298579248fc0874352d5","70325f4d1f7502bd06fa54484535b36c8b0c1290","61845763cd9c8865870b17f6efa034e8fa862978","47ec99aed698b07b24fdec85e4dbf584c4d2e707","0c7650ed95c7d5fc90d769d5e34fd519313e8383","25a782a03c249d315b04a68cb1a3663ff8c893d5","5b88525bca03b88dfffc6e478127ddd72518d813"],"ACM Trans. Database Syst.":["5f4abcc6526ca79fb0d5df14527a9a5ce1c12621","7584a7f5e9ace396f2d9725cd9d9072a27c1b5a6","3c43d0c9a1d7864e7d819179940d857f8633d4ce","9b1725d100e06bfd43ed5e996288e8c55c7d510b"],"Pattern Analysis and Machine Intelligence":["013cd20c0eaffb9cab80875a43086e0c3224fe20","41f50d08e4c237d0f192f5c09f78d7e1d09d9cef","18758118e78ca7f908021c55196fcba12dbb6283"],"Data Engineering":["43b39679a5a37585e31260fc426120f4fe00538d","4cc6eb1716cabda5a55a8ed103f8e503717ac434","58cfb95d06b6f4ccbf79ed1436c50187d1657252","0bbf348983a12f21c1607b8a963357c33f18f120","55c3597ebe9ce20c3f2374be4dda465726335403","31b58ddd4a558f7d9e41e19ef75993668ee69119","422955d1f193e70bebb546b33d53564421c5085a","99126d196c0539e3014b61d91538a29ee2de8c54","323011da81c396825df6ec185ac17a21f2272ea0","1b74e7b2547373ff3df6fc293be7bed49075053d","2525c025f11aec60cff428271ca851381b92008f","7fee2107608d10a494a469dbf01c807a9a95b09d","0899c2c3c31cd555425008d38a1a671f3e37724a","0f41cd1792db9ff879fdfffc746cf5a01adf207f","232d3305b2c4c68a57b357c2cdea7c754891ebcb"],"CNN":["1683b929bd2c1fd5f326ee9501ef24c2b613b5ef"],"EMNLP":["146b3649b693ae591c8953c0aae5264512c26ea3","2606e6a5759c030e259ebf3f4261b9c04a36a609","06d51deada6e771a2571807a47dd991120c8dd1a","0b544dfe355a5070b60986319a3f51fb45d1348e","3bff03b7b0b0c4e8f6384dbb2a95e4338d156524","b90196005b0f1eb0953758c16869633d7ca3399d","0d3233d858660aff451a6c2561a05378ed09725a","27e38351e48fe4b7da2775bf94341738bc4da07e","1eb654f295d0e98e71bfbbe18c8a0cad3f6d15e9","5cceeb8a36f62cce92056b8e055ec9aaa7c3074c","26196511e307ec89466af06751a66ee2d95b6305","0d3fc5945c3ee608af29080c00032b9afd232c36","db82c8e83d2ef333250126655511498b8114b18e","205b9f4891a2ead886604f161a44b3aed483609a","0825788b9b5a18e3dfea5b0af123b5e939a4f564","1826fbbf12e399024296c1dfcc5758c1c810574c","a88966cdaeddd15d0a3de365a8f0a5931aebd756"],"ICCV":["18cc17c06e34baaa3e196db07e20facdbb17026d"],"Commun. ACM":["314547de686f67e48c075ef8bbe6fe3a1d22dc92","6c96ca73b381c4d8191ad73ea2fd9272ff0799c4","0658b1922d6a30a0191bb5fe13b0a9e43e49c999","0d6aa509c98b197a1f410f31e6fa760fde40d8a1"],"TACL":["89677d5cb68a4c020f4604e99b9a72f6c4532581","1e9e87fc99430a82621810b3ce7db51e339be315"],"Neural Networks: Tricks of the Trade":["007d73c91a1bf90d72eb59fbdd8791a4b009f363"],"UAI":["94b0e8e97c19ad0977d26e3e355d3ae09ad49365"],"ArXiv":["35254a19bac666c291646e3640014bf0bb2ce11b","69a80e4c13f94be768cd113d11890e62ec4f1596","1e1e274d9dde08a5e6a02daf86405d1cee5ec5cd","6a0e1ce1ea01eb5eccf7c852a26f0e3db85e856e","0bb754879154b0a10ee9966636348d831081e151","087f06a00bc3cbaa2aa16baf8a8a98eb50358d16","4f5d7dc3d41236ed6a42cf4105cc79fa2fb0828d","6f6b397267946e3faaac4e5fa587e1876fe69da2","f8c8619ea7d68e604e40b814b40c72888a755e95","65c978a97f54cf255f01c6846d6c51b37c61f836","2b6a2adddfbf70ea5cb3d9d749c1ef70db8c230a","8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092","7e463877264e70d53c844cf4b1bf3b15baec8cfb","6b570069f14c7588e066f7138e1f21af59d62e61","c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d","12806c298e01083a79db77927530367d85939907","517c31e5390d1d743aca69d16098be6ca30ebd2d","533ee188324b833e059cb59b654e6160776d5812","1e185688f3c5e73641633459dbc17b30d34fd307","aeb38c8c4b826ad0dc63911dc5198f8c565298f5","25f0625a92f6054b11057423111f9285c78376fe","2d5069a99bfa0b47c095bbb5cefd6dba974f72a7","221cf7b15aa771f9f9f8c0dc21899e22cd736fb8","4a350352b2fb426530693d42f45effd049537cab","78507d14c925e16d628bc643c75c449267fef64c","0a6193e3693356ae19c3ec3ed7c1375260a9e4a1","1fd8bf7f21e2f117ce00ea9cf3005d4aa37f155a","45e3f381d16e6d6db5449b7a44b7bdf294a8a822","cf280435c471ee099148c4eb9eb2e106ccb2b218","b3610b7650533631ef7a63adf21ec0e722f4d9c0","3d3b8fe8b5dd2d518f3a186f63553eb26a49968f","2329a46590b2036d508097143e65c1b77e571e8c","62dcea12832692d2acdb46cb343ab03a04898542","f71c25642f53c52823aeef5d215993470991cf82","0d24a0695c9fc669e643bad51d4e14f056329dec","6ca1898dac153b8cd500c0c2633675b05d3c638c","79d1e429c241d0aa47a2194246256a5bc79585bc","5e3b036c44f0c3b0b5eb1e99ef78644dcabcf2f0","135c89b491f82bd4fd7de175ac778207f598342b","3e1638d1ce6ec0d18c82fb3235a521db0a559da2","bbb6bef6b2e48f5088f9bc0fd7cf7c07d514ea2a","071b16f25117fb6133480c6259227d54fc2a5ea0","a4ab5ad02c4ed463e1e8a5c06e8b176ef3dea2ea","528c397b0e523c9a257d67bb05409b83b4cdd33d","39371e655d27527c74b6d18080dcd3107273ba8f","fd7447a976968cd4190c65edef8482f4f8e0cab9","d07a50bb119b594a6f8f9d9c574cbdfdfaa586d3","7466d1a809d69435f29cebdd651029566eb702f7","2f7ad26514bce4df6c8ebc42c90383ef3a974df4","7a7a2658df5d66541305962d4c9d43078adadac6","855d0f722d75cc56a66a00ede18ace96bafee6bd","272665e79d39862fb014377f01b31fd3f91d9fb8","63cbac5a39cd926a806f60116b845f9bd70f5544","79ed6d5157b73449dc663516fc2c8dfcb917b99e","2b329183e93cb8c1c20c911c765d9a94f34b5ed5","9fe93ec9c2f8ca5ddbc46c050ba4216c09154980","0406f0696982b26bdf2a456123439c8ddcf8afb1","73463ec3391df70d4c38de6a1e963830a85efbe6","069340a9fb06268b19e12a59de87547c9750fc79","cab20ae17048106a84f14a6209f0286609cfa542","0a82cb606e561ca6f43697ca4df4f449b82ddba6","2b35a5bccb678532412bcb471c16d6208353cf62","237202f570c6990f0c33c4db17b5df61d5ee6899","95a4edd3b7967da6c39bea61dc846052cd3a4837","0375dbfedebc209b1f276f629c8958d37b48cac6","f59d3e260ae4c7e145b3a2732524ea74cf5deac2","23c2e6c185aa7f4274dc53196fa130dcd969988b","62c76ca0b2790c34e85ba1cce09d47be317c7235","878634c30842b5812c56fe772719424bab69e7ad"],"CNLP":["2dde68fc4b1af7f21e18d7b9da5f964539f9b515","44d6ffe6535f75ccfd4a71a72ac2982077afcb1c","1297074f0e7d6c663c55f7580ca603473842eec9"],"TAC":["90df2b8696ad2ff4db8178fded1d17303f917d92"],"Neural Computation":["8225f757f6bb34624aa8e9658e9af295432c29b5","55657836eb554d4d42ba94db5baa274478a56d8a","161ba68b34bf2ec77384c9f1de04e9631d08af50","296bbeb2c43b183fb7d93c8a309b2c576f8b5fc7"],"DBPL":["277fc66c995fe0c5b79e68b328259159cb412c85"],"Computational Intelligence":["5d363ec087445a226c8f64db03dd7efaa132e854"],"IEEE Data Eng. Bull.":["43fc0302e46e91ffcbae49a2aae1a32dd0e927d9"],"I. J. Robotics Res.":["5fc69f93422b11c944b2d53e9d2f93295eca3d19"],"ICDM":["be15028db5dc7f50cbca96d9e3d264802f385873"],"Acoustics, Speech and Signal Processing":["0add4da8389bd54958221def3ea16575ee9a007f"],"HLT-NAACL":["fff3fa15f1fd07484edf8f355eb53e58d0ed8ccf","22825681ceb3f065098eb71ca9346b458b566252","3fa58a0dcbf7392e02f744a0f729ba8010b7334c","7da23d5b7c26dc1d6ac98682a74a8dd6a18bf309","2982952a884a8d27fbcd800c010d8f4af564181b","20a599c5f152c3f135b9b55a667e64c93ec8d477","0bdaf705c237a53b8db50dbdb62f1451774cb2a3","17a40c553bdef82ccfc1bcc978e8e9d23230bfe5","45d814078179b7ac69086d8c7f40576244258549"],"SSPS":["40e6f827c72158e4b0a7dcc20f18448b4dd7e042"],"Neurocomputing":["78d0ed0106446bfe78851e0ab3e7dd50e586c951"],"INTERSPEECH":["9d4e4136353d6ed07cfb61565151e0e6e8fd7cf6","8c3679bab6b379a3f487c68f631f55bb18292bc0"],"Data Stream Management":["0bb7e5432be8ae6799eddf88d35e525b4b1744f7"],"FSR":["69de7827bb2d7508d0f15ce634d9594f8d0bb4b4"],"Pattern Recognition":["3b98dc29eb5f95470d12d19ae528674129ca0411"],"ASRU":["0a832c69048ca30d4f528d221ba9899590a45f34"],"Machine Learning":["4b2283dd97a853812ca053547b4e067b2fa54512"]},"category_b_index":{"Journal":["35254a19bac666c291646e3640014bf0bb2ce11b","69a80e4c13f94be768cd113d11890e62ec4f1596","1e1e274d9dde08a5e6a02daf86405d1cee5ec5cd","6a0e1ce1ea01eb5eccf7c852a26f0e3db85e856e","0bb754879154b0a10ee9966636348d831081e151","087f06a00bc3cbaa2aa16baf8a8a98eb50358d16","4f5d7dc3d41236ed6a42cf4105cc79fa2fb0828d","6f6b397267946e3faaac4e5fa587e1876fe69da2","f8c8619ea7d68e604e40b814b40c72888a755e95","314547de686f67e48c075ef8bbe6fe3a1d22dc92","6c96ca73b381c4d8191ad73ea2fd9272ff0799c4","65c978a97f54cf255f01c6846d6c51b37c61f836","2b6a2adddfbf70ea5cb3d9d749c1ef70db8c230a","8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092","7e463877264e70d53c844cf4b1bf3b15baec8cfb","6b570069f14c7588e066f7138e1f21af59d62e61","c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d","12806c298e01083a79db77927530367d85939907","517c31e5390d1d743aca69d16098be6ca30ebd2d","533ee188324b833e059cb59b654e6160776d5812","1e185688f3c5e73641633459dbc17b30d34fd307","aeb38c8c4b826ad0dc63911dc5198f8c565298f5","25f0625a92f6054b11057423111f9285c78376fe","2d5069a99bfa0b47c095bbb5cefd6dba974f72a7","221cf7b15aa771f9f9f8c0dc21899e22cd736fb8","4a350352b2fb426530693d42f45effd049537cab","78507d14c925e16d628bc643c75c449267fef64c","0a6193e3693356ae19c3ec3ed7c1375260a9e4a1","1fd8bf7f21e2f117ce00ea9cf3005d4aa37f155a","0658b1922d6a30a0191bb5fe13b0a9e43e49c999","45e3f381d16e6d6db5449b7a44b7bdf294a8a822","cf280435c471ee099148c4eb9eb2e106ccb2b218","b3610b7650533631ef7a63adf21ec0e722f4d9c0","3d3b8fe8b5dd2d518f3a186f63553eb26a49968f","2329a46590b2036d508097143e65c1b77e571e8c","62dcea12832692d2acdb46cb343ab03a04898542","f71c25642f53c52823aeef5d215993470991cf82","0d24a0695c9fc669e643bad51d4e14f056329dec","6ca1898dac153b8cd500c0c2633675b05d3c638c","79d1e429c241d0aa47a2194246256a5bc79585bc","5e3b036c44f0c3b0b5eb1e99ef78644dcabcf2f0","135c89b491f82bd4fd7de175ac778207f598342b","3e1638d1ce6ec0d18c82fb3235a521db0a559da2","bbb6bef6b2e48f5088f9bc0fd7cf7c07d514ea2a","071b16f25117fb6133480c6259227d54fc2a5ea0","a4ab5ad02c4ed463e1e8a5c06e8b176ef3dea2ea","528c397b0e523c9a257d67bb05409b83b4cdd33d","39371e655d27527c74b6d18080dcd3107273ba8f","fd7447a976968cd4190c65edef8482f4f8e0cab9","d07a50bb119b594a6f8f9d9c574cbdfdfaa586d3","7466d1a809d69435f29cebdd651029566eb702f7","0d6aa509c98b197a1f410f31e6fa760fde40d8a1","2f7ad26514bce4df6c8ebc42c90383ef3a974df4","7a7a2658df5d66541305962d4c9d43078adadac6","855d0f722d75cc56a66a00ede18ace96bafee6bd","272665e79d39862fb014377f01b31fd3f91d9fb8","63cbac5a39cd926a806f60116b845f9bd70f5544","79ed6d5157b73449dc663516fc2c8dfcb917b99e","2b329183e93cb8c1c20c911c765d9a94f34b5ed5","9fe93ec9c2f8ca5ddbc46c050ba4216c09154980","0406f0696982b26bdf2a456123439c8ddcf8afb1","73463ec3391df70d4c38de6a1e963830a85efbe6","069340a9fb06268b19e12a59de87547c9750fc79","cab20ae17048106a84f14a6209f0286609cfa542","0a82cb606e561ca6f43697ca4df4f449b82ddba6","2b35a5bccb678532412bcb471c16d6208353cf62","237202f570c6990f0c33c4db17b5df61d5ee6899","95a4edd3b7967da6c39bea61dc846052cd3a4837","0375dbfedebc209b1f276f629c8958d37b48cac6","f59d3e260ae4c7e145b3a2732524ea74cf5deac2","23c2e6c185aa7f4274dc53196fa130dcd969988b","62c76ca0b2790c34e85ba1cce09d47be317c7235","878634c30842b5812c56fe772719424bab69e7ad"],"ML":["225721565af848f8360a8b01f7cc6d28dcc8e240","29cb27e32d56f39b9fa5c5bf62225580530a014c","5aee809e16ec6d065e828fa04968b2d189c29e71","524b24a3523123785bedccfa0ef6c4857bf21b5f","0851f0dad6de06dab86c82feb34dfa5b807129dc","053912e76e50c9f923a1fc1c173f1365776060cc","ccf415df5a83b343dae261286d29a40e8b80e6c6","50d53cc562225549457cbc782546bfbe1ac6f0cf","17307b4925959a15b2af55bdbb861df41ab879d2","5796ba3a657261a49ca9333865b8606980111371","389bfe18dc161fda4980ec426ffaccec76a918bb","4700e447d1a0cddda978cd87177301790bb58efd","26c8d040bef85ad6dde55a8f71af936fb38356ad","1c7647fd9ab65e46e821bb0e045535500e2584d4","146f6f6ed688c905fb6e346ad02332efd5464616","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","83b625ae40c921c47255da5f2e24266e75a48d9b","6f568d757d2c1ab42f2006faa25690b74c3d2d44","8a9a10170ee907acb3e582742bec5fa09116f302","195d0a8233a7a46329c742eaff56c276f847fadc","14d904e10cca3f0cb6d9c623db1a50152cec6360","7f917de89d395e7dba3f2c5cb54b4a13c06a8dee","373cf414cc038516a2cff11d7caafa3ff1031c6d","167abf2c9eda9ce21907fcc188d2e41da37d9f0b","60f22ad725f041fff81d6371242485bbe5c3ebb6","8225f757f6bb34624aa8e9658e9af295432c29b5","50654221b831a14e3ddad4579accc9c92685f29a","5656fa5aa6e1beeb98703fc53ec112ad227c49ca","4856e7719e566f2466369ae2031afb07c934d4d3","5c0fe8ba39bda83d6ca3b9705a780809d52a67b4","12dd078034f72e4ebd9dfd9f80010d2ae7aaa337","8e0eacf11a22b9705a262e908f17b1704fd21fa7","5115f3ff1ac45486a50ad3834a40490f9ca4bcea","3ec4a5d29a3d897595f8f02aff0984d249c179c4","2579b2066d0fcbeda5498f5053f201b10a8e254b","0c23ebb3abf584fa5e0fde558584befc94fb5ea2","84458cd529206ec4897b90be191b089e8966ddc4","13e10f545bd600b5bd8c36b29bcecec4a61d1b7f","06554235c2c9361a14c0569206b58a355a63f01b","3b2bf65ebee91249d1045709200a51d157b0176e","5da15e6b30920ab20ce461232c00276e3b0f0bef","0808bb50993547a533ea5254e0454024d98c5e2f","8b358a216f83edb259decd68127722a356cc8adf","376eec3b84d31d672a3633ed06498fc9a9776f93","55657836eb554d4d42ba94db5baa274478a56d8a","01cbebbbcb973ae2d1d32a49fa1f1e0738153ba9","78c91d969c55a4a61184f81001c376810cdbd541","0bfbdafdfbcc268860fe54ae4d8f08d487bcc762","161ba68b34bf2ec77384c9f1de04e9631d08af50","06d0a9697a0f0242dbdeeff08ec5266b74bfe457","07c43a3ff15f2104022f2b1ca8ec4128a930b414","0e48b9fc023866b55219b26ec40ad88633020d2e","5075636b9e0425358204211706adde1ecb5ba60b","2337ff38e6cfb09e28c0958f07e2090c993ef6e8","296bbeb2c43b183fb7d93c8a309b2c576f8b5fc7","534f6ea4ce0127e5da7f1cafb6334b59ad15b83f","03cb609fcfce6c60cbe3eb0dd8254069bf6d7573","72dbacd0dbd157d881d284e922e21fcc5a2a4eeb","0bed90488e8950e5bea85ffca8b48fab3b67c1a1","182015c5edff1956cbafbcb3e7bbe294aa54f9fc","6e7dadd63455c194e3472bb181aaf509f89b9166","2febe441afece9e3f427fa4d541887fb89d0088f","27211ed68a7a00f1df0121fa1890a1b2acdd1a88","3e4883a0ab6c5830785b83b5af74fcd63b1c556e","6bf0414dae4f10c7e54fb9e5e8af5d0d0cab290b","8ec543a9e6b4ae5b3c9f6f938ae5a9bdf77d82ac","0b98d8f90c57c5aabf2bc264d47730cb2dfc897f","16a333a43d587802f95b5ec11de6c99314ae0c77","0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3","aa3d3c206b9d1726b9c387a00ddd2da2a9803fb6","2b7ef95822a4d577021df16607bf7b4a4514eb4b","2d02141fd8c263d9c9a05d704c2d3e38525a7167","00a10855b9ab0f2c04226b7a05a7371dea26090b"],"ComLing":["1007e2bfb377757a75f51a0edaf745edeabf3757","146b3649b693ae591c8953c0aae5264512c26ea3","2606e6a5759c030e259ebf3f4261b9c04a36a609","209ba9f612f34583a23b75c0e8dadc410c400bb2","06d51deada6e771a2571807a47dd991120c8dd1a","0b544dfe355a5070b60986319a3f51fb45d1348e","3bff03b7b0b0c4e8f6384dbb2a95e4338d156524","03b18dcde7ba5bb0e87b2bdb68ab7af951daf162","b90196005b0f1eb0953758c16869633d7ca3399d","0d3233d858660aff451a6c2561a05378ed09725a","fff3fa15f1fd07484edf8f355eb53e58d0ed8ccf","22825681ceb3f065098eb71ca9346b458b566252","3fa58a0dcbf7392e02f744a0f729ba8010b7334c","5640ce300f3bea8347824e887f4b688b0f364873","1aeee8b379e6fcf10d33c15df78e135b16b24d5e","d579adf7a2c5cce3bb17482bb757f15bff45b131","27e38351e48fe4b7da2775bf94341738bc4da07e","142f38642629b9d268999ad876af482177d36697","7da23d5b7c26dc1d6ac98682a74a8dd6a18bf309","1eb654f295d0e98e71bfbbe18c8a0cad3f6d15e9","2982952a884a8d27fbcd800c010d8f4af564181b","5cceeb8a36f62cce92056b8e055ec9aaa7c3074c","62a4a6abc2a52889db51b56ef082d02784266985","26196511e307ec89466af06751a66ee2d95b6305","0d3fc5945c3ee608af29080c00032b9afd232c36","20a599c5f152c3f135b9b55a667e64c93ec8d477","10c51faad92ff64e93c3688f62edf9756dca20ad","506ccb2fe5b756b35381b750fb996d014fbc8014","0bdaf705c237a53b8db50dbdb62f1451774cb2a3","db82c8e83d2ef333250126655511498b8114b18e","67bee729d046662c6ebd9d3d695823c9d820343a","17a40c553bdef82ccfc1bcc978e8e9d23230bfe5","1e6f61c574fad2460d06d6f52295922f5c99857a","16c7d4559b977d10b2a4d73fc3a9a972367e6f11","0a49b4de21363d86599d4a058aaf4f5aed019495","205b9f4891a2ead886604f161a44b3aed483609a","1680eaa31f30911e7460ff694b51794391bc7514","0825788b9b5a18e3dfea5b0af123b5e939a4f564","45d814078179b7ac69086d8c7f40576244258549","4e88de2930a4435f737c3996287a90ff87b95c59","1826fbbf12e399024296c1dfcc5758c1c810574c","a88966cdaeddd15d0a3de365a8f0a5931aebd756"],"AI":["0d060f1edd7e79865f1a48a9c874439c4b10caea","60744af2f89291897839852d66582b1b2a0be0bc","e93a59b37f3d6282ebd4e5bad61924f6d75385d9","4cf66c66ca404ea3b5474e78e6cf94f6bfd05b0d","2064c2a33eb0b4c8acd23fd60b98c12d6c1ad61e","304d859dd545ab119ef1a6141d73a9c3797526d4","2964d30862d0402b0d0ad4a427067f69e4a52130","17f5c7411eeeeedf25b0db99a9130aa353aee4ba","2d28108e25ba7b43f831210db064b1582d265503","1e3d36a9184feec956371fe3fb03eb34747e4844","00d3fadfadc977ba4b6511bec1b2a3026d877099","80fe1abae2594a2cb5466d3646abbc57fb13d144","2c27683f6ecf8f2e94ff3254fa2630f90e3aacf9","36818eaf6376aeeaffed2523d28bebae7c9db8d7","1d4d14e78eb1536b84f6e851a67ecc5a6c0ae07f"],"Others":["219309d6413a29656178f99186157ceeb0932020","7400fe80e9e5670fd3e9de000a4afb2e6cc034fc","541c68e2c65f6dce6179801c9f92dc7803dc71b5","18cc17c06e34baaa3e196db07e20facdbb17026d","2dde68fc4b1af7f21e18d7b9da5f964539f9b515","5dc1fd136278c61771fbc43473c9e21638f0f46f","013613f6f3da2094a2fb590d8608ad0b44798baf","1979271767a78ec766638b83efcc1fc8e96cc4a2","013cd20c0eaffb9cab80875a43086e0c3224fe20","2ae139b247057c02cda352f6661f46f7feb38e45","78d0ed0106446bfe78851e0ab3e7dd50e586c951","3bf1d8b8ef045df1787c703fb6874f01709597da","cc4296eecd15b83a3c9b9d3a84e91327bd14c7bd","6e3f2fe073ba009336c0f13636bb5d137ac276f6","764ecb30e4ab5b1ad0cebec96019d9753c0c7f62","afa5f554eba815dc34be4409a595a53f514cb0bc","0ad8d00860f3b65527a40377dd96b55564515167","961f6d5bb9b43833d7387e377b25d7a68118b23b","3d913755fcda4fde4e0a1365299429038e256c74","60abc9657a01a128c5503673609bfe8e7637c00b","6e1fd106a17d67078a1d0ab8d5ef4f64d0feec1e","098a170095b1e84b3a8eb673adfe6408fc0dcff5","e50c2e4b8f7df21763f47c8941aa560c75d133b1","011f7f9ba9e6f9bc7f05994271725bc0fc9c3b94","46018a894d533813d67322827ca51f78aed6d59e","90df2b8696ad2ff4db8178fded1d17303f917d92","9d4e4136353d6ed07cfb61565151e0e6e8fd7cf6","10eb7bfa7687f498268bdf74b2f60020a151bdc6","89677d5cb68a4c020f4604e99b9a72f6c4532581","51691435d646c5b9152162f328d29511755328ba","74225938608ccf6a825df576d4dfe2fca9726200","0bb7e5432be8ae6799eddf88d35e525b4b1744f7","db8c3cfaae04a14c1209d62953029b6fa53e23c7","946232a2fe2549a2d2f52612b09ed8d66a74f34b","2f87364ce9255e345b35d431b00ba7271329cba8","01ad77475dedda66e3961f707a19fe588ef48f39","a5eb54e8a30406ac723ba258111239e156e25523","40e6f827c72158e4b0a7dcc20f18448b4dd7e042","41f50d08e4c237d0f192f5c09f78d7e1d09d9cef","0dbac3fa87f3288454bb1c5a24e97c9fa4401cbe","190b27eb221eec46b97373d80c5b96d95c4153c3","3845947052d99e45f0f3e7789b66c6582d098e48","e39d1345a5aef8a5ee32c0a774de877b903de50c","6e3fbbeec1f3ad2a8efe9a7207ba92bee322b78e","124acb2ceccd1d1cd9b4715e00d24eab31099aa8","78b729049a0135dc75a021ce5bbc127902253fde","4893c08f1c76fe4eb7cad2bb496da526b0fa5dd3","6bf6a3fd2c4c17c4326b81424ce19aba0a4b9c42","0bddc08769e9ff5d42ed36336f69bb3b1f42e716","a3bd824b2d0077e1e2f97bb73942151d711cdf93","70fa7c2a8a12509da107fe82b04d5e422120984f","2a92f8fb49dcae62af89d0c1cb9a3001b1be7f5a","1e9e3694954822d720570c5c53136c8814b2c4be","08e5cce87b6af2ae0ee09b58b86d81aa229b46d0","3598b9eb49e226c1f49f123a058c1896459af339","4e649f746c007f858492a504ee2da156b3d25e36","0d43fe67820c2f2a2e8adda04be04579fa85582c","007d73c91a1bf90d72eb59fbdd8791a4b009f363","dc4a6027f4b868e544d2659181e52fa2e716a517","69de7827bb2d7508d0f15ce634d9594f8d0bb4b4","91ff05cf0512789727dd010270e95dfc02301318","e21a6a56e24f321286a50f2a1811ae66ef6245f2","3fd5307b2705971e543d16e7350651945b257f14","1683b929bd2c1fd5f326ee9501ef24c2b613b5ef","76765f06161c2f1b153056b86ce665102414ec73","907e35afd9633cd0819e0add854081bae738d320","022029f1211970dbf29824f6510fe6dc7718190e","3b98dc29eb5f95470d12d19ae528674129ca0411","e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","277fc66c995fe0c5b79e68b328259159cb412c85","00a28138c74869cfb8236a18a4dbe3a896f7a812","420a94968bd8671c8071d9c735a095ee475d0b20","1600e8c878a2f42c71c753a353b8554b37ecf093","0b753d3c9fb80145c59f987749ebbde80a0de475","25eb839f39507fe6983ad3e692b2f8d93a5cb0cc","14577a88dec08a9199ec4f7a878956a4e55c3dd5","6556cea7f91c1221dd2693aba0d3dd45d2e09e6d","368fe0571a5611c2de27379747adff2066245937","71898bd32548b98ceecf71bb30b9616e4be40b43","41a0c983e59c8fc5100b94c0802d74a64f86c741","16ce9e96e5cf0b8fd3aeee137d548ff903711b97","561008cb23d7a38a00806353ba3389c1b95395af","35e024714f0702f8b77c8c2a916dedb91931708a","3b118bbb7c33eb8ca4605170e1bff2d459488df6","ecdd236a1a31afcb2a3480a4c7dd5aa65b44551d","0a832c69048ca30d4f528d221ba9899590a45f34","0e18e9dcfceedabcf14e6a615862c53ce8c0293d","03f34688ef4ee4239464633784235387e9bff4bb","7184a8b7be5ae88bca900542b0ab2c27b019f0ce","7509931a7cafe9b0a61337d141d79d76eadaaeed","65ad0e876216ea034b7958f016456e32666bc5c6","5f4abcc6526ca79fb0d5df14527a9a5ce1c12621","44d6ffe6535f75ccfd4a71a72ac2982077afcb1c","1297074f0e7d6c663c55f7580ca603473842eec9","5d363ec087445a226c8f64db03dd7efaa132e854","043e91679d7def20ece119117757ae7178d9eec9","45923f515e07b0250643350f8e4c19a74593dc91","c05d7d5ad5b154e5118e8e996a26898b6854eb6e","5272c3e3c06481eb705241a6ed45f1d3905a1459","5fc69f93422b11c944b2d53e9d2f93295eca3d19","8c3679bab6b379a3f487c68f631f55bb18292bc0","1ec8bcda29ae8159b0beb989fef08f6c332e4993","fe994dc0a8d9a365c5dc34e573dbf7c481b978df","7b4cd55ce7d7558eecb8d39f25a05cfd01ce4715","9f5f5ecf0ee423f8493e3a600c6af9a7f8720d8e","af5a145735128e1aa4782f8eb8981ff5e3292b2a","54636e41ef8dc33f818aec7b9b02731eb75650d6","02537a4d8781435a40f768f479a68e7a613dd26f","620d6ebb2e7604f6dd838210a6847239bc2670dd","26cb14c9d22cf946314d685fe3541ef9f641e429","0d75052f1d7350fa035a35566555ce7b65d1cd2f","1d8bfe40212695fdd05fd1e297f9584a30f477c4","7584a7f5e9ace396f2d9725cd9d9072a27c1b5a6","683e5c3112c63aabaedafb7ba24f994baf07e0a4","81b3b3fe994a9eda6d3f9d2149aa4492d1933975","be15028db5dc7f50cbca96d9e3d264802f385873","d2ac9f33f16b421c3c5f38fddb8cdd8d30d7f886","180920cd652b27cd9207fb797107b19d46e1100a","0b4d3e59a0107f0dad22e74054bab1cf1ad9c32e","5394ab5bf4b66bfb52f111525d6141a3226ba883","bd4ddde7f9af802805fac652b44146698f0bda6e","3df87d901a0e3de3bd6a0c17c5683d753fbeaf64","7797d9e0e444246ba61d7e16cbbd654b07c117e8","0add4da8389bd54958221def3ea16575ee9a007f","bfe947a04e9716fb6a581d9609f448aa5daa50f1","0d60fce590359780f62017e472fd4523a5de3f5f","710822be03043849bc016acc987695a3b8b34ecf","400f6f4304b1c12efb22acf7e80a1784015cb23a","315cd76e4a34b8fe27e20345abcd4fc27c7ee1ab","4d8d303fd622cf3bd0899bfe532fbee41202e718","3c43d0c9a1d7864e7d819179940d857f8633d4ce","1828eaa8750ee188111b92deae5c7f67323e723f","9315cf87a57a346c416a98984b8bf82d3b85e679","046a1302079f56b94c81457bf7fd21c3417a9f72","032e9974cedb31f5c6e354626760e54e5ebf1e3c","f21fcb8a04f764eb482479823a4933b09534a9b2","b9392e6965137b2f81c81ff17addf3096faa4ffa","12244deb997152492d96c6246ec21b2b9804800d","94b0e8e97c19ad0977d26e3e355d3ae09ad49365","35c14fd2453b0dc02a02dcf538fbd775e4993e81","52d6d89cccb79fc5f8fa3c0e4e9c4021645b70f0","4def24ede97126c946c58e8af355a87a6775e268","9b1725d100e06bfd43ed5e996288e8c55c7d510b","54c22be7d4bdcb761c42f2670242c8689e29206f","3f43314011db0e92e629d6ee6cf6a918e1e258bb","65d994fb778a8d9e0f632659fb33a082949a50d3","771a8edc3a434af7e1af3b0c9424c7e199e4c646","0a22ce29e0fd1b0303ede3f5a94e1fe70764e455","e27d81521dc4e8b6ea93947c05ffccf06784f569","60fdfa95dee2dc3b125f1552aa5c080542707e8f","aad2c96c46064604c5ef729a491a47d12727ae9e","2a0cec7f0f8b63f182ea0c52cb935580acabafcc","ceef5d5b82b822308a07998f19c6cca7ce0a80a6","4b2283dd97a853812ca053547b4e067b2fa54512","654a3e53fb41d8168798ee0ee61dfab73739b1ed","dfd790fd02afea1de60cba77a3b6b98640bea491","5158991247802f372b1646ef8751db46c307a2c8","30e2c349636920f96e72882e2e2c1d22b4739127","0ed7adf241b11c34fe1354240a1b3cd9cd8a3705","3b2697d76f035304bfeb57f6a682224c87645065","91277a62af159aa44e03482711149658576cae36","4fadcd3a0cb63edd255383f41ed3e280495333b2","dd24d28c60d432cf95f5b8a64ba3dec75fcf93b7","18758118e78ca7f908021c55196fcba12dbb6283","1671a665c636bec7d2eaff137d74e9b7f074892f","05fd1da7b2e34f86ec7f010bef068717ae964332","d5b5db41929d791b4e911a5c7b0e29f60ee23253","1e9e87fc99430a82621810b3ce7db51e339be315","02227c94dd41fe0b439e050d377b0beb5d427cda","0a5eace1ead92c5b1c47ccc89d1daf7bb5c5acd8","75cbe27efe1c8b255102f641feba1871176c6c20"],"ComVis":["01a903739564f575b81c87f7a9e2cb7b609f7ada","115808104b2a9c3ab6e2e60582ab7e33b937b754","6701889c81ad460f53a4d84361cd3d37b4e02743","0c881ea63ff12d85bc3192ce61f37abf701fdf38","19150b001031cc6d964e83cd28553004f653cc24","098abe1eee6208b693daafe7183f017f9e71e409","38211dc39e41273c0007889202c69f841e02248a","f9c431f58565f874f76a024add2aa80717ec5cf5","2cb3bd0cff91c0afcbfb2cb10ce30313e0f70133"],"DB":["023db6fa5c87852885dcee9531a1c843063c2347","46cd50a49397e37e5d83cb536f4b92a40293a467","29360559cbb9d8d4012c94262f3a79ea259abe2f","43b39679a5a37585e31260fc426120f4fe00538d","4cc6eb1716cabda5a55a8ed103f8e503717ac434","3549ff6774d1dde5e6c80b2e97bc5b6ce578f677","0bfdae2b923805e2453b2affcb1ba05994889d77","1298f6dd80c11766a3ff8ae1a4c21e12eadb7c17","3df5b6b3b2b648f3d8224322e6a0f127850df017","1d6156b17ef02f510394f78c3c57b9a1d93af2b4","bd212586b54cb959c5d5058a4e16ee6b2cdae1cc","20dd934a841f775ed18e0d2f8e887f363cbb9099","988cbe6332ce9b202ed3d8c6d03c678e9f7c3e2f","58cfb95d06b6f4ccbf79ed1436c50187d1657252","700e1d1b7c54711e96645691438d7fd9fda229ef","0bbf348983a12f21c1607b8a963357c33f18f120","0e16487103cbe1d9bf2ce04ea5718c3fe9f08cf8","1780dab3ed9253b596cb68a10cf3c2d9b8e5015f","3f099575d82b0248c6be9b042c286b3d81f01e63","5cb40de2c796b9644346821c4cb600fd05e8e727","55c3597ebe9ce20c3f2374be4dda465726335403","245cd97fc34fad6d6e7efb373874e5a42576adfe","0c325c32039656541760b2d8f02be4636e026785","157852dd14a9b518bf62fa6511be0f02f5d04a79","d8c11242130cc02c9f9b7a9506c303ab05e2daea","2185c1d1e465dd443b7ccb47c9b2d520da65a319","31b58ddd4a558f7d9e41e19ef75993668ee69119","d0f04a8e3a21b0142f46067b98530ec833672e93","441a89d393eba5091afe76fbaa7c35cbac1c31dc","a9d41b228cd14532fcabb87d65739a8213f69299","704dfc1636911592c5ea02a198077d2586bb8d23","422955d1f193e70bebb546b33d53564421c5085a","124f6f240ba622cd74a9a0ea554ec2a5011eaadf","8e370946562e09f8895f8454a7719848cb176d79","1b4f5bb49dc95340a66c75e1c4c719f0f96439c8","136eefe33796c388a15d25ca03cb8d5077d14f37","33e46d74b066402f0ae912627dff5b700006f9b2","18c021c9cce95ed5615a060f590b8388b604e7c5","99126d196c0539e3014b61d91538a29ee2de8c54","412a9e54bbb31e12d008a9579994e009c5b40b46","64b05d23afb0bd5754fcb70d87233c9b4326eef2","160cb0d0c0930f2782d8caa212d892845fc02aaf","4d82a5741b03a0a134380400d42552c53c89ac83","1513c758ce244032348b751c40a100846e9aa321","3c7da5d87776db54d04978700f01f3c41156d3e2","323011da81c396825df6ec185ac17a21f2272ea0","23d1255f1a0453ba96c6e6ef054903086f3656df","0ec2c62121f8543864c84348835883df564aea14","1b74e7b2547373ff3df6fc293be7bed49075053d","4ca3c1040a4bc0d1ae200d26d1c18fbc6df0e95a","77ae592ec975dfb14d494667e0b019af19da9075","3d6e2d4c1cde661fea2c8b1c93528aa24bb87aff","2525c025f11aec60cff428271ca851381b92008f","265efd16c16dc942705246a57337d323722003cb","7fee2107608d10a494a469dbf01c807a9a95b09d","5d458a1bff91aa598fcc47711e5cfd7a6dfa559d","cb3e11ff1f5f03aa58e47e08640647ca133faab2","211e4b486a85f5250ef68aef8a4422811ec7a932","19a2c31570534fbdde8290413ac465b3831e9e99","374668781c1aa05391a900cc2841eb31c7bbce6a","260697ac804faf844241de404bab6e9460223c2f","bf1d7df019f8c5355758a65002dedc9690539819","16cfb80ff6321fa0e277e0316398742475dcdc84","0899c2c3c31cd555425008d38a1a671f3e37724a","87f34d2e08f9f079c189bdb1db1a4d750080a8cc","2009b3c05dd9084a0a1c609abcedd81713ed7150","8c8b44029fbdac1572ae47b8eaab3929c9987098","43fc0302e46e91ffcbae49a2aae1a32dd0e927d9","133eacaf0ad25b8364cb4510007d9363298e8adf","2dfbd860542f79240f8f58cc042db1dfa562053a","5a5d6b0a9a7036c5417a618925c8e03d786e7d74","7cc937645ddee844b7171b88b587a8d9af98c9bc","273121804df13771df2a6890f50e54e7ec419f75","019902292dff81eae20f3e87970dd7a1151d9405","411b6492957d5367ef0df2ac926793fec0ca74ed","0e6c65b8c366d6f925b8221e01aceddbf931ef19","73f31354cc9058ddc2e47a1c585b753e1592c1bf","25241d105a39e9933c03298579248fc0874352d5","70325f4d1f7502bd06fa54484535b36c8b0c1290","908f7db1b21067ee9f9442bccce4787d9ad5634f","266228767a0d4aed021754b17947c926f4f8881b","61845763cd9c8865870b17f6efa034e8fa862978","207def18c67fa8024741b7ae3cdc655b57f2053f","5578cabaef7b5dfc88443626e74d2e04951818f7","47ec99aed698b07b24fdec85e4dbf584c4d2e707","208edd765ff1fa2320dc87d2746b5b7460303bd8","57e979025374da67fae37fbb81bbadecee68cc08","b2bf34c0c0007145a389e014b7ddaa3daa76f332","40709c0e27d0fe6f9265b6c29847e63392f4001d","0f41cd1792db9ff879fdfffc746cf5a01adf207f","6edc18adf0b57400726b8fc76b3428173d118826","0c7650ed95c7d5fc90d769d5e34fd519313e8383","232d3305b2c4c68a57b357c2cdea7c754891ebcb","0aa8c4caef4ad23da2e1def1f22b74badd5be4e2","25a782a03c249d315b04a68cb1a3663ff8c893d5","2f207e27cad923b81253542a6f76439d49d87925","5b88525bca03b88dfffc6e478127ddd72518d813","16cd112b8f3283a5f915b57c3e56ad10ffb45b09","437b328a1a62eeb85aaa87514a13010a4df315ee","683422744d6a8f5e6bac38756f1f51a8ed16e2e3","3e4290774195402d31661d3c6b1f1da1bc55f5ae","1e4d40e94fa57419a1236a1866a0c3ef357f8b78"],"HCI":["71439278086b7fc84eedc8aeba377747e00b8403","87a937dff0ecffefd8a7e4ca4ce068aca9731a0a","5edf85b3068a0c95d5901623c7fd793ac1c92b38","7060f6062ba1cbe9502eeaaf13779aa1664224bb"]},"word_grid":[[[],[["Experiment",4],["Approximation algorithm",2],["Backpropagation",2],["Discriminative model",2],["Generative model",2],["Markov chain",2],["Markov decision process",2],["Minimax",2],["Multilayer perceptron",2],["Perceptron",2],["Test set",2],["Addressing scheme",2],["Feedforward neural network",2],["Memory address",2],["Memory cell (binary)",2],["Non-deterministic Turing machine",2],["Nonlinear system",2],["Turing machine",2],["Turing test",2],["X86",2],["Generative adversarial networks",1],["MNIST database",1],["Cloning Vectors",1],["Controllers",1],["Inference",1],["Natural language",1]],[],[],[]],[[],[["Algorithm",38],["Artificial neural network",29],["Machine learning",25],["Restricted Boltzmann machine",21],["Deep learning",20],["Experiment",19],["Unsupervised learning",17],["Gradient",14],["Boltzmann machine",14],["Sampling (signal processing)",14],["Recurrent neural network",11],["Encoder",11],["Autoencoder",10],["Bayesian network",10],["Neural Networks",9],["Slab allocation",9],["Noise reduction",9],["Generative model",9],["Gibbs sampling",9],["Nonlinear dimensionality reduction",8],["Test set",8],["Approximation algorithm",8],["Latent variable",8],["Sparse matrix",8],["algorithm",8],["Nonlinear system",8],["Program optimization",8],["Greedy algorithm",8],["Deep belief network",7],["Calculus of variations",7],["The Spike (1997)",7],["Feature learning",7],["Supervised learning",7],["Eisenstein's criterion",7],["Matrix regularization",7],["Cluster analysis",6],["Semi-supervised learning",6],["Jacobian matrix and determinant",6],["Initialization (programming)",6],["Maxima and minima",6],["Dropout (neural networks)",5],["Graphics processing unit",5],["Neural coding",5],["Loss function",5],["Markov chain",5],["Computation",5],["Decibel",5],["MNIST database",5],["Multidimensional scaling",4],["Spectral clustering",4],["Language model",4],["Mathematical optimization",4],["statistical cluster",4],["Kullback\u2013Leibler divergence",4],["Markov chain Monte Carlo",4],["Feature extraction",4],["Synthetic data",4],["Benchmark (computing)",4],["Computational complexity theory",4],["High- and low-level",4],["Spatial variability",3],["Activation function",3],["Convolutional neural network",3],["Architecture as Topic",3],["Inference",3],["manifold",3],["Computer vision",3],["Interaction",3],["Data point",3],["Support vector machine",3],["Image resolution",3],["Generalization (Psychology)",3],["Kernel",3],["Kernel principal component analysis",3],["Smoothing",3],["Decision tree",3],["Parallel tempering",3],["Feedforward neural network",3],["Outline of object recognition",3],["Partition function (mathematics)",3],["Pixel",3],["Statistical model",3],["Structured prediction",3],["Linear classifier",3],["Natural language",3],["Layer (electronics)",3],["Optimization problem",3],["Isomap",2],["Numerical analysis",2],["Emotion recognition",2],["K-means clustering",2],["Multimodal interaction",2],["Statistical classification",2],["Data (computing)",2],["Models, Statistical",2],["Action potential",2],["Focus stacking",2],["Long short-term memory",2],["Python",2],["NIPS",2],["Memory-level parallelism",2],["Simulation",2],["Anatomic Node",2],["Autoregressive model",2],["HIPPI",2],["Decision boundary",2],["Convolution",2],["Inpainting",2],["Embedding",2],["Overfitting",2],["Reinforcement learning",2],["Monte Carlo method",2],["Singular value decomposition",2],["Unit",2],["Extractor (mathematics)",2],["Man-Machine Systems",2],["Dbm",2],["EXPTIME",2],["Fixed point (mathematics)",2],["Variational method (quantum mechanics)",2],["Machine translation",2],["Generalized linear model",2],["Perturbation theory",2],["Convex optimization",2],["Gradient descent",2],["Domain adaptation",2],["Discriminant",2],["Network architecture",2],["Map",2],["Piecewise linear continuation",2],["Missing data",2],["Mixture model",2],["Edge detection",2],["Image gradient",2],["Natural language generation",2],["Neural Network Simulation",2],["Hessian",2],["Latent variable model",2],["Computer multitasking",2],["Connectionism",2],["Value (ethics)",2],["Curse of dimensionality",2],["Causality",2],["Kernel density estimation",2],["Microsoft Windows",2],["Stochastic process",2],["Ergodicity",2],["Algorithmic efficiency",2],["Data dependency",1],["Dimensionality reduction",1],["Unified Framework",1],["Catastrophic interference",1],["Hollywood",1],["Illumination (image)",1],["Modality (human\u2013computer interaction)",1],["Streaming media",1],["Video clip",1],["Electron Microscopy",1],["Euphorbia aaron-rossii",1],["Factor analysis",1],["explanation",1],["Grammatical Framework",1],["Top-down and bottom-up design",1],["Ability",1],["Academic degree",1],["Biological Neural Networks",1],["Dynamical system",1],["Multilayer perceptron",1],["Note (document)",1],["Rule (guideline)",1],["Speedup",1],["Binary prefix",1],["Characteristic function (convex analysis)",1],["Euclidean distance",1],["Expectation propagation",1],["Feature vector",1],["Formal proof",1],["K-nearest neighbors algorithm",1],["Kernel method",1],["Linear separability",1],["Nearest neighbor search",1],["Nearest neighbour algorithm",1],["Test point",1],["Hall effect",1],["Texture synthesis",1],["Effective method",1],["Interdependence",1],["Optical character recognition",1],["Laplacian matrix",1],["Natural mapping (interface design)",1],["Compiler",1],["TensorFlow",1],["Theano (software)",1],["Exploit (computer security)",1],["Monte Carlo",1],["Hilbert space",1],["Nautical chart",1],["Imagery",1],["Sampling - Surgical action",1],["International Conference on Machine Learning",1],["Extractors",1],["Mitobronitol",1],["Parkinson Disease",1],["Tracer",1],["Approximation",1],["Distortion",1],["Feedback",1],["Google Brain",1],["Learning rule",1],["PC Bruno",1],["N-gram",1],["Metropolis",1],["Metropolis\u2013Hastings algorithm",1],["Perlin noise",1],["Arc diagram",1],["Hearing Loss, High-Frequency",1],["Kernel (operating system)",1],["Linear IgA Bullous Dermatosis",1],["Principal Component Analysis",1],["Principal component analysis",1],["multidimensional scaling",1],["Image segmentation",1],["Information theory",1],["Network theory",1],["Newton",1],["Newton's method",1],["Quasi-Newton method",1],["Glossary of computer graphics",1],["Scheduling (computing)",1],["Treebank",1],["Data domain",1],["Similarity measure",1],["Rectifier",1],["Data mining",1],["Spanning Tree Protocol",1],["Spanning tree",1],["Combinatory logic",1],["Electronic filter topology",1],["Bridging (networking)",1],["Algorithmic inference",1],["Backpropagation",1],["Social network",1],["Stationary process",1],["BLEU",1],["Conditioning (Psychology)",1],["Discrepancy function",1],["Learning Disorders",1],["Security token",1],["Ensemble interpretation",1],["Deep packet inspection",1],["Coefficient",1],["Early stopping",1],["Morgan",1],["Objective-C",1],["Parametric model",1],["Point of View (computer hardware company)",1],["Sample Size",1],["Branch predictor",1],["Multi-task learning",1],["Autostereogram",1],["Dictionary",1],["Grammar-based code",1],["Sparse approximation",1],["Analysis of algorithms",1],["Baseline (configuration management)",1],["End-to-end principle",1],["Hidden Markov model",1],["Sequence labeling",1],["Speech recognition",1],["TIMIT",1],["Binary data",1],["Sampling in order",1],["Database",1],["Linear model",1],["Mean squared error",1],["Tails",1],["Weight function",1],["Artificial intelligence",1],["Gaussian (software)",1],["Journal of Machine Learning Research",1],["STL (file format)",1],["Whitening transformation",1],["Stack (abstract data type)",1],[".cda file",1],["Facial recognition system",1],["Mathematical morphology",1],["X\/Open",1],["Discriminative model",1],["Program animation",1],["Collaborative filtering",1],["Motion capture",1],["Multi-label classification",1],["Arabic numeral 0",1],["Binary code",1],["Binary number",1],["Citrus aurantium",1],["Decision",1],["Decision Trees",1],["Decision tree learning",1],["Error detection and correction",1],["Forests",1],["Gradient boosting",1],["Influence diagram",1],["Inspiration function",1],["Local hidden variable theory",1],["Reconstructive Surgical Procedures",1],["Representation (action)",1],["Time complexity",1],["Tree structure",1],["Trees (plant)",1],["Wood material",1],["anatomical layer",1],["exponential",1],["Graph (discrete mathematics)",1],["Iteration",1],["Iterative method",1],["Iterative refinement",1],["Contraction mapping",1],["Upsampling",1],["Hierarchical database model",1],["Overhead (computing)",1],["Span and div",1],["Digit - number character",1],["Random subspace method",1],["Assistive technology",1],["Performance",1],["Preprocessor",1],["Boundary scan",1],["Discrete mathematics",1],["Discriminator",1],["Generative adversarial networks",1],["ImageNet",1],["Text-based (computing)",1],["Online and offline",1],["Inference engine",1],["Approximation theory",1],["Artificial neuron",1],["Heuristic",1],["Multiplicative noise",1],["Neuron",1],["Sigmoid function",1],["Distribution (mathematics)",1],["Importance sampling",1],["Kalman filter",1],["Markov random field",1],["Multiprogram Research Facility",1]],[["Cluster analysis",3],["Experiment",3],["Mixture model",3],["Sampling (signal processing)",3],["Gibbs sampling",2],["Algorithm",2],["Bayesian network",1],["Dimensionality reduction",1],["K-means clustering",1],["Nonlinear system",1],["Similarity measure",1],["Spectral method",1],["Active learning (machine learning)",1],["Central processing unit",1],["Cosine similarity",1],["Selectivity (electronic)",1],["Tf\u2013idf",1],["Binary data",1],["Boltzmann machine",1],["Concentrate Dosage Form",1],["Data point",1],["Man-Machine Systems",1],["Restricted Boltzmann machine",1],["Gaussian elimination",1],["statistical cluster",1]],[["Algorithm",2],["Cluster analysis",1],["Heuristic",1],["Hierarchical clustering",1],["Hierarchical database model",1],["Ward's method",1],["Conditional entropy",1],["Data quality",1],["Experiment",1],["Functional dependency",1],["Method of analytic tableaux",1],["Sampling (signal processing)",1],["Synthetic data",1]],[["Algorithm",12],["Differential privacy",10],["Experiment",9],["Privacy",8],["Data anonymization",8],["Synthetic data",6],["Adversary (cryptography)",5],["Information privacy",5],["Information sensitivity",5],["Approximation algorithm",4],["Hoc (programming language)",4],["Value (ethics)",4],["Personally identifiable information",3],["Microdata Corporation",3],["Sparse matrix",3],["Online algorithm",3],["Count data",3],["Social network",3],["Denial-of-service attack",2],["Aggregate data",2],["Aggregate function",2],["Contingency table",2],["Entity",2],["Table (information)",2],["Data model",2],["Geographic coordinate system",2],["Provable prime",2],["Machine learning",2],["Anonymous web browsing",2],["Image stitching",2],["Single-instance storage",2],["Linear programming",2],["Loss function",2],["Mathematical optimization",2],["Program optimization",2],["Tame",2],["Whole Earth 'Lectronic Link",2],["Angular defect",1],["Bayesian network",1],["Curse of dimensionality",1],["Mutual information",1],["Service control point",1],["Surrogate key",1],["Private network",1],["Tree structure",1],["Video post-processing",1],["Virtual private network",1],["Distortion",1],["Wavelet",1],["Graph embedding",1],["Graph property",1],["Provable security",1],["Dominating set",1],["Mathematical model",1],["Information",1],["Stock and flow",1],["Context-sensitive help",1],["Eisenstein's criterion",1],["Information leakage",1],["Numerical analysis",1],["Spectral leakage",1],["Effective method",1],["Graph (abstract data type)",1],["Tree (data structure)",1],["Fast Fourier transform",1],["Software testing",1],["Data cube",1],["Data point",1],["Dummy variable (statistics)",1],["Keyboard shortcut",1],["Naivety",1],["Sampling (signal processing)",1],["Simplicial complex",1],["Sparse",1],["Social Networks",1],["Database",1],["Perturbation theory (quantum mechanics)",1],["Possible world",1],["Uncertain data",1],["Zero suppression",1],["Computation",1],["Crystal structure",1],["Data structure",1],["Stream (computing)",1],["Data modeling",1],["Deadlock",1],["End-to-end encryption",1],["Information Rules",1],["Requirement",1],["StumbleUpon",1],["Programming, Linear",1],["Tosylarginine Methyl Ester",1],["algorithm",1],["Graph (discrete mathematics)",1],["Interaction",1]]],[[["Machine translation",17],["Neural machine translation",15],["Artificial neural network",9],["Vocabulary",7],["Encoder",6],["Algorithm",5],["Statistical machine translation",5],["Baseline (configuration management)",3],["Natural language processing",3],["Experiment",3],["Text corpus",3],["Word embedding",3],["Language model",2],["Lexical function",2],["The Sentence",2],["Recurrent neural network",2],["BLEU",2],["Unsupervised learning",2],["Machine learning",2],["Network architecture",2],["Concatenation",2],["Display resolution",2],["High- and low-level",2],["N-gram",2],["Nonlinear system",2],["Parallel text",2],["Substring",2],["Download",1],["Mathematical model",1],["Web page",1],["Binary decoder",1],["Convolutional neural network",1],["Recursion",1],["Linear model",1],["Log-linear model",1],["Network model",1],["String (computer science)",1],["Importance sampling",1],["Sampling (signal processing)",1],["Crossword",1],["Dictionary",1],["Reverse engineering",1],["Categorization",1],["Feature vector",1],["Intermediate representation",1],["Sensor",1],["Spatial variability",1],["Batch processing",1],["Code",1],["Creational pattern",1],["Microsoft Windows",1],["Self-information",1],["Data pre-processing",1],["Language-independent specification",1],["Preprocessor",1],["Software deployment",1],["Test set",1],["Bag-of-words model",1],["Type system",1],["Evaluation of machine translation",1],["Freebase",1],["Knowledge base",1],["Natural language",1],["Neural Networks",1],["Question answering",1],["Supervised learning",1],["Heuristic",1],["Semantic similarity",1],["End-to-end principle",1],["Cluster analysis",1],["Distortion",1],["Emergence",1],["Feature learning",1],["K-means clustering",1]],[["Artificial neural network",29],["Algorithm",20],["Deep learning",18],["Recurrent neural network",15],["Machine learning",11],["Experiment",11],["Speech recognition",10],["Benchmark (computing)",9],["High- and low-level",7],["Convolutional neural network",7],["Language model",7],["End-to-end principle",6],["Graphics processing unit",6],["Sparse matrix",6],["Test set",6],["Natural language",5],["Matrix regularization",5],["Machine translation",5],["Question answering",4],["Neural Networks",4],["Unsupervised learning",4],["BLEU",4],["Gradient",4],["Encoder",4],["Hidden Markov model",4],["Computer vision",4],["Finite-state machine",4],["Loss function",4],["Telephone exchange",4],["Vocabulary",4],["Text corpus",4],["Bayesian network",3],["Natural language processing",3],["Pipeline (computing)",3],["Multilayer perceptron",3],["Perceptron",3],["Speech synthesis",3],["Pattern recognition",3],["Autoencoder",3],["Network architecture",3],["MNIST database",3],["Mathematical optimization",3],["TIMIT",3],["Statistical model",3],["N-gram",3],["Dialog system",3],["Robot",3],["Architecture as Topic",3],["Outline of object recognition",3],["Object detection",3],["Dictionary",3],["Nonlinear system",3],["Feedforward neural network",3],["Robotics",3],["Approximation algorithm",2],["Sampling (signal processing)",2],["Textual entailment",2],["International Conference on Machine Learning",2],["Feature vector",2],["Kinesiology",2],["Stochastic process",2],["Baseline (configuration management)",2],["Deep belief network",2],["Modality (human\u2013computer interaction)",2],["Restricted Boltzmann machine",2],["Computer cluster",2],["Debugging",2],["Gradient descent",2],["Program optimization",2],["Markov model",2],["Boltzmann machine",2],["Heuristic",2],["Modal logic",2],["Recursion",2],["Sentiment analysis",2],["Treebank",2],["Brain Neoplasms",2],["Neoplasms",2],["Neural Tube Defects",2],["Reinforcement learning",2],["Physical object",2],["Semantic similarity",2],["Dropout (neural networks)",2],["anatomical layer",2],["Creation",2],["Activation function",2],["Basis function",2],["Decoupling (electronics)",2],["Latent variable",2],["Acoustic model",2],["Recommender system",2],["Transcription (software)",2],["High-level programming language",2],["Extractor (mathematics)",2],["Word embedding",2],["Medical transcription",2],["Computation",2],["Natural language understanding",2],["Smartphone",2],["Greedy algorithm",1],["Pipelines",1],["Semantic role labeling",1],["Bidirectional recurrent neural networks",1],["ECML PKDD",1],["GPS navigation device",1],["Layer (electronics)",1],["Connectionism",1],["Feature engineering",1],["Categorization",1],["Intermediate representation",1],["Sensor",1],["Spatial variability",1],["Audio description",1],["Meteor",1],["Email",1],["Evaluation of machine translation",1],["IBM Research",1],["Natural language generation",1],["Off topic",1],["Perplexity",1],["Technical support",1],["Ubuntu",1],["Branch predictor",1],["Emotion recognition",1],["Image retrieval",1],["Video clip",1],["Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm",1],["Conjugate gradient method",1],["Distortion",1],["Limited-memory BFGS",1],["Line search",1],["Network model",1],["Stochastic gradient descent",1],["Input\/output",1],["Extrapolation",1],["Instability",1],["Biological Neural Networks",1],["Computer data storage",1],["Fibrosis of Extraocular Muscles, Congenital, with Synergistic Divergence",1],["Heuristics",1],["Inference",1],["Man-Machine Systems",1],["Memory Disorders",1],["Electronic Supplementary Materials",1],["Grams",1],["Quantity",1],["gram",1],["Cascade Device Component",1],["Glioblastoma",1],["Information source",1],["Scientific Publication",1],["Test data",1],["childhood brain stem glioma",1],["Archive",1],["Backlink",1],["Crostata",1],["Entity linking",1],["Knowledge base",1],["Star filler",1],["Wikipedia",1],["Markov decision process",1],["Speaker recognition",1],["Location awareness",1],["Importance sampling",1],["Information",1],["Variance reduction",1],["Imagery",1],["Isomap",1],["Leigh Disease",1],["Nonlinear dimensionality reduction",1],["Sammon mapping",1],["Subgroup",1],["T-distributed stochastic neighbor embedding",1],["Manuscripts",1],["Silo (dataset)",1],["Skeletal animation",1],["Unit",1],["research study",1],["Backpropagation",1],["Calculus of variations",1],["Black box",1],["Multimodal learning",1],["Caltech 101",1],["Feed forward (control)",1],["Neural coding",1],["Vector quantization",1],["Dynamic Bayesian network",1],["Noise (electronics)",1],["Commodity computing",1],["InfiniBand",1],["Best practice",1],["Bag-of-words model",1],["Collaborative filtering",1],["Latent Dirichlet allocation",1],["Product of experts",1],["State (computer science)",1],["Transformer",1],["Cluster analysis",1],["K-means clustering",1],["Teaching method",1],["Byte",1],["Byte pair encoding",1],["Data compression",1],["Intelligent agent",1],["Multi-agent system",1],["Multimodal interaction",1],["Open-source software",1],["Data center",1],["Dynamic dispatch",1],["End-to-end encryption",1],["Super Robot Monkey Team Hyperforce Go!",1],["Convolution",1],["Recursive neural network",1],["Kalman filter",1],["Quantum decoherence",1],["Tweaking",1],["LiveCode",1],["The Wall Street Journal",1],["Computational model",1],["Flow network",1],["Modulation",1],["Conditional random field",1],["Early stopping",1],["Overfitting",1],["Common Criteria",1],["Linear separability",1],["Non-negative matrix factorization",1],["Pitch detection algorithm",1],["Stellar classification",1],["Backoff",1],["Generative model",1],["Interaction",1],["Spatial\u2013temporal reasoning",1],["Testbed",1],["Coordinate descent",1],["Moses",1],["Multi-Environment Real-Time",1],["Powell's method",1],["Statistical machine translation",1],["Performance",1],["Random oracle",1],["Visual modeling",1],["Voice activity detection",1],["Application programming interface",1],["Extensibility",1],["Library",1],["Hessian",1],["Information geometry",1],["Matrix multiplication",1],["Newton",1],["Partition function (mathematics)",1],["Microsoft Word for Mac",1],["C++",1],["Compiler",1],["Linear algebra",1],["Theano (software)",1],["Autostereogram",1],["Synthetic data",1],["Convolutional Deep Belief Networks",1],["Focus stacking",1],["Statistical classification",1],["Causality",1],["Optimization problem",1],["Support vector machine",1],["Division by two",1],["Transduction (machine learning)",1],["CUPS",1],["Graphics",1],["Object type (object-oriented programming)",1],["Software versioning",1],["Feature learning",1],["Locality of reference",1],["Principle of locality",1],["Requirement",1],["STL (file format)",1],["Scalability",1],["Topography",1],["Artificial Intelligence",1],["Artificial intelligence",1],["Computation (action)",1],["Content-control software",1],["E-commerce",1],["Engineering",1],["Extractors",1],["Gene Expression",1],["Languages",1],["Mental Orientation",1],["Metrorrhagia",1],["Mutation",1],["Neural Network Simulation",1],["Particle Accelerators",1],["Pixel",1],["Power (Psychology)",1],["Relevance",1],["Sequence motif",1],["Social network",1],["Speech Disorders",1],["algorithm",1],["cell transformation",1],["Conditional (computer programming)",1],["Discriminative model",1],["Error analysis for the Global Positioning System",1],["Estimation theory",1],["Markov chain",1],["Maximum-entropy Markov model",1],["Part-of-speech tagging",1],["Point of sale",1],["Protologism",1],["Word sense",1],["Word-sense disambiguation",1],["Electron Microscopy",1],["Information extraction",1],["Labels (device)",1],["Microsoft Windows",1],["Named-entity recognition",1],["Beam search",1],["Lexicon",1],["Word error rate",1],["Noise reduction",1],["Solutions",1],["Cross entropy",1],["Edit distance",1],["Hinge loss",1],["Supervised learning",1],["Workaround",1],["Detectors",1],["Digital video",1],["Foremost",1],["Frame (physical object)",1],["MinEd",1],["Pattern matching",1],["Probability",1],["Triplet state",1],["Video content analysis",1],["X86",1],["Chroma feature",1],["Memory-level parallelism",1],["Kernel (operating system)",1],["Least squares",1],["Matching pursuit",1],["Mean squared error",1],["Radial basis function",1],["Weight function",1]],[["Machine learning",6],["Algorithm",5],["XML",5],["Artificial neural network",5],["Computer vision",4],["Restricted Boltzmann machine",3],["Support vector machine",3],["Test set",3],["Machine translation",3],["Database",3],["algorithm",3],["Crowdsourcing",3],["Unsupervised learning",3],["Boltzmann machine",3],["Convolutional neural network",3],["Multi-label classification",2],["Perceptron",2],["Natural language processing",2],["Text corpus",2],["Neural machine translation",2],["Query language",2],["SQL",2],["Access control",2],["Neural Network Simulation",2],["Arabic numeral 0",2],["Autonomous car",2],["Computation",2],["Reinforcement learning",2],["Optical character recognition",2],["Interaction",2],["Supervised learning",2],["Approximation algorithm",2],["Deep learning",2],["Synthetic data",2],["Email",2],["Benchmark (computing)",2],["Money",2],["Tracer",2],["Layer (electronics)",1],["Logistic regression",1],["Multilayer perceptron",1],["Preprocessor",1],["General number field sieve",1],["Multitier architecture",1],["Language model",1],["Accessibility",1],["Web accessibility",1],["Categories",1],["Decision tree",1],["Financial savings",1],["Generalized linear model",1],["Insurance Carriers",1],["Linear Models",1],["Linear model",1],["Medical algorithm",1],["Network model",1],["Nonlinear system",1],["Numerical analysis",1],["Performance",1],["Regression - mental defense mechanism",1],["Sigmoid function",1],["Support Vector Machine",1],["Trees (plant)",1],["benefit",1],["MapReduce",1],["Dimensionality reduction",1],["Epilepsy",1],["Hallermann's Syndrome",1],["JEAN",1],["Lactic acid",1],["Object Pascal",1],["Schizophrenia",1],["citation",1],["newton per square metre",1],["Cold start",1],["Concatenation",1],["Drug Discovery",1],["Generalization (Psychology)",1],["Linear classifier",1],["Model\u2013view\u2013controller",1],["Recommender system",1],["Bitwise operation",1],["CAPTCHA",1],["Sentiment analysis",1],["Speedup",1],["Browsing",1],["Usability",1],["Web application",1],["Web search engine",1],["Parser",1],["Parsing",1],["Shift-reduce parser",1],["emotional dependency",1],["second (number)",1],["Audio description",1],["Computational linguistics",1],["Empirical Methods in Natural Language Processing",1],["Jensen's inequality",1],["Kullback\u2013Leibler divergence",1],["Latent Dirichlet allocation",1],["Topic model",1],["Data pre-processing",1],["Digital signature",1],["Experiment",1],["Mortar methods",1],["Statistical classification",1],["Type signature",1],["Human-based computation",1],["Naivety",1],["Scalability",1],["Sparse matrix",1],["Time complexity",1],["Dynamic circuit network",1],["Gradient",1],["MNIST database",1],["Categorization",1],["Cluster analysis",1],["Spectral clustering",1],["Blochmannia endosymbiont of Camponotus (Colobopsis) leonardi",1],["Raman scattering",1],["Subhash Suri",1],["Yossi Matias",1],["Regular expression",1],["Application checkpointing",1],["Eventual consistency",1],["Failure rate",1],["Fault tolerance",1],["Serialization",1],["Stream processing",1],["Undo",1],["Computer multitasking",1],["Zero",1],["Emulator",1],["Latent variable",1],["Slab allocation",1],["The Spike (1997)",1],["Backtracking",1],["Calculus of variations",1],["Information",1],["RL (complexity)",1],["RL circuit",1],["Rewards",1],["Subgroup",1],["Termination analysis",1],["Trace theory",1],["Encoder",1],["Foreach loop",1],["Input\/output",1],["Recurrent neural network",1],["Speech recognition",1],["Video clip",1],["Discretization",1],["Electron mobility",1],["End-to-end principle",1],["GPS navigation device",1],["Information sensitivity",1],["Population",1],["Privacy",1],["Sampling (signal processing)",1],["Smartphone",1],["Trajectory optimization",1],["Trie",1],["Wearable technology",1],["Data (computing)",1],["Hidden Markov model",1],["Markov model",1],["Maximum-entropy Markov model",1],["Named entity",1],["Named-entity recognition",1],["Substring",1],["Test data",1],["Augmented reality",1],["Human factors and ergonomics",1],["Malignant neoplasm of skin",1],["Nevus",1],["Sound - physical agent",1],["melanoma",1],["Aclarubicin",1],["Chamaecyparis lawsoniana",1],["Edit distance",1],["Grams",1],["Inclusion Body Myositis (disorder)",1],["Intuition",1],["Mental Suffering",1],["Question (inquiry)",1],["Select (SQL)",1],["Structured Query Language",1],["gram",1],["Ground truth",1],["ImageNet",1],["Object detection",1],["Outline of object recognition",1],["Access control list",1],["Digital distribution",1],["Entity",1],["Enumerated type",1],["Fax",1],["Information model",1],["Lightweight Directory Access Protocol",1],["Prospective search",1],["Recursion",1],["Systems architecture",1],["Technical standard",1],["Unix",1],["X.500",1],["Image resolution",1],["Pixel",1],["Amazon Mechanical Turk",1],["Amazona",1],["COMEFROM",1],["Confusion",1],["Confusion matrix",1],["HLA-DQ Antigens",1],["Labels (device)",1],["Lb substance",1],["Natural language",1],["Ordinal data",1],["Predictive modelling",1],["Quantitation",1],["Silo (dataset)",1],["Spamming",1],["The Turk",1],["Video game bot",1],["Viral phylodynamics",1],["payment",1],["Digit structure",1],["Feature learning",1],["Google Street View",1],["Human reliability",1],["machine",1],["photograph",1]],[["Algorithm",9],["Computation",6],["Database",4],["World Wide Web",3],["Synthetic data",3],["Program optimization",2],["Sensor",2],["Scalability",2],["Time complexity",2],["Centralisation",2],["Join (SQL)",2],["Approximation algorithm",2],["benefit",2],["XML",2],["XSLT\/Muenchian grouping",2],["Autonomous car",1],["Hoc (programming language)",1],["Query optimization",1],["Terabyte",1],["EXPTIME",1],["Fixed point (mathematics)",1],["Forward declaration",1],["Microsoft Windows",1],["Polynomial",1],["Query language",1],["Network topology",1],["Relevance",1],["Testbed",1],["Workstation",1],["Information retrieval",1],["Linkage (software)",1],["Machine learning",1],["Coherence (physics)",1],["Data Sources",1],["Dictionaries as Topic",1],["Dictionary",1],["Discovery system",1],["Graph - visual representation",1],["Knowledge Graph",1],["Ontology (information science)",1],["Physical object",1],["Word embedding",1],["XML schema",1],["Holism",1],["XPath",1],["Amortized analysis",1],["Bandwidth (signal processing)",1],["Relational algebra",1],["Similarity measure",1],["Simulation",1],["Categories",1],["Data model",1],["Hierarchical database model",1],["Library classification",1],["Lightweight Directory Access Protocol",1],["Published Directory",1],["Relational database",1],["Relational model",1],["Web Services Discovery",1],["X.500",1],["XML database",1],["Data store",1],["Scientific literature",1],["Speculative execution",1],["Cloud computing",1],["Systems architecture",1],["Virtual world",1],["WEB",1],["Experiment",1],["Exploratory testing",1],["Greedy algorithm",1],["Heuristic",1],["Optimization problem",1],["Non-blocking algorithm",1],["Provable prime",1],["Space\u2013time tradeoff",1],["Functional dependency",1],["Blog",1],["Dense subgraph",1],["Entity",1],["Social media",1],["Social network",1],["User-generated content",1],["Artificial neural network",1],["Backpropagation",1],["Cache (computing)",1],["End-to-end principle",1],["Question answering",1],["Reinforcement learning",1],["Softmax function",1]],[["Algorithm",48],["Database",41],["XML",31],["Approximation algorithm",16],["Program optimization",15],["XML database",12],["Synthetic data",10],["Join (SQL)",10],["SQL",9],["Query optimization",9],["XPath",9],["Sputter cleaning",9],["Data model",8],["Experiment",8],["Substring",8],["Data quality",7],["Relational database",7],["String searching algorithm",7],["Computation",6],["Selectivity (electronic)",6],["Tf\u2013idf",6],["VLDB",6],["R-tree",6],["Rewriting",5],["Cosine similarity",5],["Cluster analysis",5],["Information retrieval",5],["Entity",4],["Pattern matching",4],["Question (inquiry)",4],["algorithm",4],["Online analytical processing",4],["Edit distance",4],["Scalability",4],["Time complexity",4],["Performance Evaluation",4],["Value (ethics)",4],["Best, worst and average case",4],["Central processing unit",4],["Data structure",4],["Lightweight Directory Access Protocol",4],["Overlap\u2013add method",4],["Relational algebra",4],["Twig",4],["R+ tree",4],["Information system",3],["Usability",3],["Document",3],["XML schema",3],["Query language",3],["Real life",3],["Benchmark (computing)",3],["Memory-mapped I\/O",3],["Matching (graph theory)",3],["Query string",3],["Extensible Markup Language",3],["Relational operator",3],["Type signature",3],["Tree (data structure)",3],["Twig (database)",3],["Preprocessor",3],["Holism",2],["Data integrity",2],["Linkage (software)",2],["Global information system",2],["Materialized view",2],["Mobile computing",2],["Attribute\u2013value pair",2],["Open road tolling",2],["Range query (data structures)",2],["Telephone number",2],["Dataspaces",2],["Graph edit distance",2],["Interchange circuit",2],["Sampling (signal processing)",2],["Color",2],["Entity\u2013relationship model",2],["Mobile data terminal",2],["Computer data storage",2],["Hoc (programming language)",2],["Semi-structured data",2],["Dynamic programming",2],["Maxima and minima",2],["Star schema",2],["Proximity search (text)",2],["Web search engine",2],["F-algebra",2],["Parsing",2],["Email",2],["Web service",2],["Denial-of-service attack",2],["Network traffic control",2],["Server (computing)",2],["Streaming media",2],["Utility",2],["Similarity measure",2],["Web application",2],["User-defined function",2],["Column (database)",2],["Aggregate function",2],["Relational database management system",2],["String (computer science)",2],["Transcription (software)",2],["Internet",2],["Randomized algorithm",2],["Declarative programming",2],["Binary tree",2],["B-tree",2],["Approximation",2],["Markov chain",2],["Radio-frequency identification",2],["Data (computing)",2],["Hierarchical database model",2],["IBM Notes",2],["Grams",2],["Text corpus",2],["Foreach loop",2],["Replay attack",2],["Network packet",1],["Optical Carrier transmission rates",1],["Centrality",1],["Chemical similarity",1],["Database design",1],["Insertion sort",1],["Typing",1],["Information Systems",1],["Outline (list)",1],["Reasoning",1],["Select (SQL)",1],["Single-access key",1],["Structured Query Language",1],["Histogram",1],["Published Database",1],["Scientific Queries Study Contact",1],["MIX",1],["Functional dependency",1],["Schema evolution",1],["Software verification and validation",1],["Table (database)",1],["Software framework",1],["Diagram",1],["Erd\u0151s\u2013R\u00e9nyi model",1],["Normal form (abstract rewriting)",1],["Serializability",1],["Software bug",1],["Book",1],["Out-of-core algorithm",1],["File sharing",1],["Network theory",1],["Peer-to-peer",1],["Peer-to-peer file sharing",1],["Router (computing)",1],["Routing",1],["Simulation",1],["IBM Tivoli Storage Productivity Center",1],["Google Search",1],["Graph labeling",1],["Information discovery",1],["Self-documenting code",1],["Structured analysis",1],["Cube",1],["Data cube",1],["Parse tree",1],["Aurora",1],["Computer science",1],["Data breach",1],["Exploratory testing",1],["Intrusion detection system",1],["Loose coupling",1],["Scheduling (computing)",1],["Microsoft Windows",1],["Stream processing",1],["Query plan",1],["Relevance",1],["Attribute-value system",1],["Discovery system",1],["Heuristic",1],["Missing data",1],["Semiconductor consolidation",1],["Fuzzy logic",1],["Internet backbone",1],["Object composition",1],["Petabyte",1],["Traffic analysis",1],["Web content",1],["Representation oligonucleotide microarray analysis",1],["Algorithmic efficiency",1],["Brute-force attack",1],["Dirty data",1],["NP (complexity)",1],["Social inequality",1],["Vertex (graph theory)",1],["Aggregate data",1],["Integer programming",1],["Loop-invariant code motion",1],["Mutual exclusion",1],["Uncertain data",1],["Autonomous car",1],["Identifier",1],["Microsoft SQL Server",1],["Planar separator theorem",1],["Text-based (computing)",1],["Approximate string matching",1],["Bing Maps",1],["Document retrieval",1],["Lazy evaluation",1],["Local search (optimization)",1],["Embedded system",1],["Suffix tree",1],["Efficient XML Interchange",1],["Path expression",1],["Unified Framework",1],["Schematic",1],["Input\/output",1],["Tree structure",1],["Analysis",1],["CPU (central processing unit of computer system)",1],["Merge",1],["Domain Name System Security Extensions",1],["Internal Market Information System",1],["Multilevel security",1],["Mutual information",1],["Decision support system",1],["Online aggregation",1],["Streaming algorithm",1],["Greedy algorithm",1],["Joint Task Force-Global Network Operations",1],["Precomputation",1],["Sparse matrix",1],["1:1 pixel mapping",1],["Database schema",1],["Discrepancy function",1],["Heterogeneous database system",1],["Information theory",1],["Eval",1],["Stack (abstract data type)",1],["Universal quantification",1],["Comparison of programming languages (string functions)",1],["Minimum bounding box",1],["Our World",1],["Archive",1],["Little Big Adventure",1],["Online and offline",1],["Theme (computing)",1],["Value restriction",1],["Experience",1],["Home page",1],["Whole Earth 'Lectronic Link",1],["R* tree",1],["Collision resistance",1],["Correctness (computer science)",1],["Cryptographic primitive",1],["Cryptography",1],["Hash function",1],["Load Shedding",1],["Outsourcing",1],["Programming paradigm",1],["Video synopsis",1],["Expressive power (computer science)",1],["Technical standard",1],["3D lookup table",1],["Coefficient",1],["Contingency table",1],["Strategic management",1],["Wavelet",1],["Loss function",1],["Mathematical optimization",1],["Online public access catalog",1],["Optimization problem",1],["Access control",1],["Accessibility",1],["Authentication",1],["Authorization",1],["Data item",1],["Locality of reference",1],["Lookup table",1],["Multi-user",1],["DSPACE",1],["N-gram",1],["Attribute domain",1],["Numerical analysis",1],["Scoring functions for docking",1],["User (computing)",1],["Medical transcription",1],["Auxiliary memory",1],["P (complexity)",1],["Demo (computer programming)",1],["The Mother of All Demos",1],["Hidden Markov model",1],["Language model",1],["Universal instantiation",1],["Data modeling",1],["Information exchange",1],["Logical data model",1],["Serialization",1],["XQuery",1]]],[[["BLEU",7],["Machine translation",6],["Multi-Environment Real-Time",4],["Meteor",4],["Supervised learning",3],["Unsupervised learning",3],["Textual entailment",2],["Generative model",2],["Hidden Markov model",2],["Information extraction",2],["Structured text",2],["Test set",2],["Discriminative model",2],["Distortion",2],["Java",2],["Open-source software",2],["Statistical machine translation",2],["Classification",2],["Log probability",1],["Natural language processing",1],["Type system",1],["Eisenstein's criterion",1],["Estimation theory",1],["Edit distance",1],["Evaluation of machine translation",1],["METEOR",1],["Word error rate",1],["Semi-supervised learning",1],["Java Programming Language",1],["PECR gene",1],["Silo (dataset)",1],["De Morgan's laws",1],["Linear classifier",1],["Log-linear model",1],["Preprocessor",1],["Latent semantic analysis",1],["Cross-validation (statistics)",1],["Pipeline (computing)",1],["Advertisements",1],["Coherence (physics)",1],["Databases, Bibliographic",1],["Document",1],["Solutions",1],["citation",1],["Description",1],["Digit structure",1],["Executable compression",1],["Extractors",1],["Feature extraction",1],["Initialization (programming)",1],["MNIST database",1],["Machine learning",1],["Neural coding",1],["Sparse",1],["Sparse approximation",1],["Sparse matrix",1],["Statistical classification",1],["Teh",1]],[["Parsing",20],["Algorithm",19],["Natural language",16],["Artificial neural network",15],["Treebank",10],["Natural language processing",10],["Recursion",9],["Machine learning",8],["Text corpus",8],["WordNet",8],["Test set",8],["Computer vision",7],["Recursive neural network",6],["Stochastic context-free grammar",6],["Baseline (configuration management)",6],["Language model",6],["Unsupervised learning",6],["Part-of-speech tagging",5],["Experiment",5],["Word embedding",5],["Neural coding",4],["Vocabulary",4],["Optical character recognition",4],["Linguistics",4],["Entity",4],["Phrase structure rules",4],["Conditional random field",4],["The Sentence",4],["Word-sense disambiguation",4],["The Wall Street Journal",4],["Context-free grammar",3],["Image retrieval",3],["Utility",3],["Point of sale",3],["Sentiment analysis",3],["Recurrent neural network",3],["emotional dependency",3],["sentence",3],["Question answering",3],["Grammar induction",3],["Trees (plant)",3],["Deep learning",3],["Speech recognition",3],["High- and low-level",3],["Parse tree",3],["End-to-end principle",3],["Textual entailment",3],["Synonym ring",3],["Amazon Mechanical Turk",3],["The Turk",3],["Word sense",3],["Generative model",3],["Chart parser",2],["Scene graph",2],["F1 score",2],["Super Robot Monkey Team Hyperforce Go!",2],["Machine translation",2],["Programming paradigm",2],["Autoencoder",2],["Lexicon",2],["Abductive reasoning",2],["Automated theorem proving",2],["Graph - visual representation",2],["Matching (graph theory)",2],["Phrases",2],["Well-formed formula",2],["Tree structure",2],["Knowledge base",2],["Similarity measure",2],["Markov chain",2],["Statistical model",2],["Convolutional neural network",2],["Statistical classification",2],["SemEval",2],["Mathematical morphology",2],["ImageNet",2],["Outline of object recognition",2],["Computational linguistics",2],["Languages",2],["Audio description",2],["Finite-state machine",2],["Regular expression",2],["Kernel method",2],["Visual Objects",2],["Mathematical induction",2],["Computational complexity theory",2],["Context-free language",2],["Program optimization",2],["Smoothing",2],["Feature learning",2],["Interaction",2],["Network architecture",2],["Bag-of-words model",2],["Computer",2],["Curse of dimensionality",2],["Cluster analysis",2],["Cubic function",1],["Strongly connected component",1],["Top-down and bottom-up design",1],["Logic programming",1],["Markov model",1],["Maximum-entropy Markov model",1],["Tag (metadata)",1],["BLEU",1],["Neural machine translation",1],["Multinomial logistic regression",1],["User story",1],["Bit error rate",1],["Encoder",1],["Errors-in-variables models",1],["Orthographic projection",1],["Redundancy (engineering)",1],["User-generated content",1],["Anatomic Node",1],["Graph (discrete mathematics)",1],["Inference",1],["Parser",1],["Robot",1],["Robotic mapping",1],["Semantic mapping (statistics)",1],["Simultaneous localization and mapping",1],["Body of uterus",1],["Decision",1],["Observable",1],["Freebase",1],["Tipu's Tiger",1],["Classification",1],["Graph (abstract data type)",1],["Pan\u2013tilt\u2013zoom camera",1],["Puromycin Aminonucleoside",1],["Error analysis (mathematics)",1],["Extensibility",1],["Frame language",1],["Immunoglobulin kappa-Chains",1],["Labels (device)",1],["Language Disorders",1],["Markov property",1],["Models, Statistical",1],["NL (complexity)",1],["Phrase structure grammar",1],["Sparse matrix",1],["Temporomandibular Joint Disorders",1],["algorithm",1],["Automatic Transmitter Identification System (television)",1],["Iterative method",1],["Autonomous car",1],["Real-time computing",1],["Brill tagger",1],["Poisson regression",1],["Tag (game)",1],["Net (polyhedron)",1],["Semantic analysis (compilers)",1],["Constant-voltage speaker system",1],["Pitch (music)",1],["Value (ethics)",1],["Document",1],["Document retrieval",1],["Image processing",1],["Image scanner",1],["Preprocessor",1],["Scanning",1],["GiST",1],["Android",1],["Dropout (neural networks)",1],["Feedforward neural network",1],["Neural Networks",1],["iOS",1],["Propositional calculus",1],["The Matrix",1],["Minimum bounding box",1],["Object detection",1],["Anomaly detection",1],["Modal logic",1],["Zero",1],["A* search algorithm",1],["Multiple sequence alignment",1],["Sequence alignment",1],["Statistical parsing",1],["Transducer",1],["Google Questions and Answers",1],["Human reliability",1],["Link relation",1],["Linear classifier",1],["Annotation",1],["Choose (action)",1],["Educational workshop",1],["Equilibrium",1],["Estimation theory",1],["Generative grammar",1],["Head",1],["Head-driven phrase structure grammar",1],["Killzone: Liberation",1],["Mac OS X 10.4 Tiger",1],["Order by",1],["Reside",1],["Benchmark (computing)",1],["European Conference on Computer Vision",1],["ICCV",1],["Parallel text",1],["Video clip",1],["Interruption science",1],["Speech processing",1],["Latent semantic analysis",1],["Speech corpus",1],["The Times",1],["Descriptive Video Service",1],["Digital media",1],["Display resolution",1],["Temporal logic",1],["Overfitting",1],["Text segmentation",1],["Downstream (software development)",1],["Long tail",1],["On the fly",1],["Zipf's law",1],["Pixel",1],["Semi-supervised learning",1],["The New York Times",1],["Approximation error",1],["Knowledge Search",1],["World Wide Web",1],["Gigabyte",1],["Interpolation",1],["Java",1],["LR parser",1],["Naivety",1],["Phrasal template",1],["Rewrite (programming)",1],["Rewriting",1],["Silent Hill: Downpour",1],["Directed graph",1],["Tf\u2013idf",1],["Error detection and correction",1],["Principle of maximum entropy",1],["Google Street View",1],["International Conference on Document Analysis and Recognition",1],["Content-based image retrieval",1],["Visual Basic[.NET]",1],["Biological Neural Networks",1],["Categories",1],["Etoposide",1],["Neural Network Simulation",1],["Neural Tube Defects",1],["Backpropagation",1],["Black box",1],["Feature engineering",1],["Interpretation (logic)",1],["Mathematical optimization",1],["Microsoft Windows",1],["Named-entity recognition",1],["Numerical analysis",1],["Coherence (physics)",1],["Flickr",1],["Image segmentation",1],["Patch (computing)",1],["Relevance",1],["Visual modeling",1],["Cognition",1],["Crowdsourcing",1],["Whole genome sequencing",1],["Generalization (Psychology)",1],["Oscillator representation",1],["Protologism",1],["Brown Corpus",1],["Initialization (programming)",1],["Viterbi algorithm",1],["Crostata",1],["Database",1],["Information extraction",1],["Named entity",1],["Wikipedia",1],["B-tree",1],["Internet backbone",1],["Population",1],["Link analysis",1],["Markov decision process",1],["Stationary process",1],["End system",1],["Long short-term memory",1],["Network topology",1],["Semantic similarity",1],["Human-based computation game",1],["Concatenation",1],["Corpus linguistics",1],["Grams",1],["N-gram",1],["Carroll Morgan (computer scientist)",1],["Entity\u2013relationship model",1],["Equivalence (formal languages)",1],["Plausibility structure",1],["Stage level 1",1],["Steve Omohundro",1],["Eisenstein's criterion",1],["Support vector machine",1],["Pattern recognition",1],["Supervised learning",1],["Pascal",1],["Scalability",1],["Dependency grammar",1],["FITS",1],["Is-a",1],["Taxonomy (general)",1]],[["Database",2],["Algorithm",2],["Big data",2],["Information system",1],["VLDB",1],["Handwriting recognition",1],["Journal of Machine Learning Research",1],["Languages",1],["Optical character recognition",1],["Approximation",1],["Burn-in",1],["Computation",1],["Computational physics",1],["Computer",1],["Computer hardware",1],["D-Wave Two",1],["Machine learning",1],["Markov chain",1],["Markov chain Monte Carlo",1],["Physical computing",1],["Restricted Boltzmann machine",1],["Sampling (signal processing)",1],["Simulation",1],["Mathematical optimization",1],["Ethernet hub",1],["Query plan",1],["Cross Reactions",1],["Distributed Interactive Simulation",1],["IBM Notes",1],["Multitier architecture",1],["Naive Bayes classifier",1],["Naivety",1],["Natural language",1],["Nipponia nippon",1],["Principle of maximum entropy",1],["Stanford University centers and institutes",1],["Status Epilepticus",1],["Tor Messenger",1],["Trees (plant)",1],["Word sense",1],["Word-sense disambiguation",1],["algorithm",1],["funding grant",1],["C-Store",1],["CIDR",1],["Column-oriented DBMS",1],["Local Interconnect Network",1],["Stan",1],["Stan Franklin",1],["GeForce 700 series",1],["General-purpose computing on graphics processing units",1],["Next-generation network",1],["SIMD",1],["Single instruction, multiple threads",1],["Single-core",1],["Systems design",1],["Xeon Phi",1]],[["Database",19],["Data model",8],["SQL",7],["Big data",7],["Database engine",6],["Computation",5],["Computer data storage",5],["Online transaction processing",5],["Transaction processing",5],["Hoc (programming language)",5],["User-generated content",5],["Sputter cleaning",5],["Benchmark (computing)",5],["Application programming interface",4],["H-Store",3],["Open-source software",3],["Throughput",3],["Scalability",3],["Data science",3],["SciDB",3],["MIMIC Simulator",3],["Algorithm",3],["Structured Query Language",3],["Relational database",3],["ACID",2],["In-memory database",2],["Stream processing",2],["Central processing unit",2],["Computer",2],["Lock (computer science)",2],["Parallel computing",2],["Published Database",2],["database management software",2],["Experience",2],["Workflow engine",2],["Debugging",2],["Human-readable medium",2],["Interactivity",2],["Question (inquiry)",2],["Programming language",2],["Program optimization",2],["Relational database management system",2],["Software transactional memory",2],["Transaction processing system",2],["Downstream (software development)",2],["Sentiment analysis",2],["Middleware",2],["Span and div",2],["Reference implementation",2],["End-to-end encryption",2],["Pipeline (computing)",2],["Data hub",2],["Time series",2],["Apache Storm",1],["App Store",1],["Block cipher mode of operation",1],["Correctness (computer science)",1],["Esper_(software)",1],["Vertical bar",1],["Architecture as Topic",1],["Automatic parallelization",1],["Buffers",1],["CPU (central processing unit of computer system)",1],["Computers",1],["Hardware acceleration",1],["Multiprocessing",1],["Oracle Database",1],["Proprietary hardware",1],["Requirement",1],["Shared memory",1],["Shared nothing architecture",1],["Supercomputer",1],["Symmetric multiprocessing",1],["Terabyte",1],["corporation",1],["End-to-end principle",1],["Federation (information technology)",1],["Grunt",1],["Linkage (software)",1],["Merck Index",1],["Apache Hadoop",1],["Array DBMS",1],["Coprocessor",1],["Management system",1],["Singular value decomposition",1],["Web analytics",1],["Blog",1],["Information extraction",1],["Social media",1],["Video clip",1],["Wiki",1],["Classless Inter-Domain Routing",1],["Reading (activity)",1],["Apache Accumulo",1],["Heterogeneous database system",1],["PostgreSQL",1],["Software release life cycle",1],["FITS",1],["Query language",1],["Query optimization",1],["Operating environment",1],["Stored procedure",1],["Browsing",1],["Event chain methodology",1],["Geolocation",1],["Streaming algorithm",1],["Data Sources",1],["Interface Device Component",1],["TimeLine Fluoride Releasing Resin",1],["Algorithms for Recovery and Isolation Exploiting Semantics",1],["Application checkpointing",1],["Downtime",1],["High availability",1],["High-throughput computing",1],["Overhead projector",1],["Run time (program lifecycle phase)",1],["VoltDB",1],["Write-ahead logging",1],["Microsoft Windows",1],["Naivety",1],["NewSQL",1],["Scope (computer science)",1],["State management",1],["Velocity",1],["Body of uterus",1],["Text corpus",1],["Data lineage",1],["Input\/output",1],["Locality of reference",1],["B+ tree",1],["B-tree",1],["Concurrency (computer science)",1],["Concurrency control",1],["IBM Tivoli Storage Productivity Center",1],["Project Looking Glass",1],["Thread (computing)",1],["Academic Medical Centers",1],["Astronomy",1],["Earth Sciences",1],["Image analysis",1],["Medical image computing",1],["Medical imaging",1],["Oceanography",1],["Pixel",1],["User-defined function",1],["Workload",1],["Control flow",1],["MapReduce",1],["Performance",1],["Data visualization",1],["International Components for Unicode",1],["Discovery system",1],["E-commerce",1],["Semiconductor consolidation",1],["Tandem Mass Spectrometry Scoring Engine",1],["Visual Accommodation",1],["Cache (computing)",1],["Image stitching",1],["meeting",1],["Care-of address",1],["Graphical user interface",1],["Programmer",1],["Usability",1],["User interface",1],["Access time",1],["Array programming",1],["Control system",1],["Data compression",1],["Database storage structures",1],["Delta encoding",1],["Map",1],["OpenStreetMap",1],["Software versioning",1],["Storage model",1],["Version control",1],["Metagenomics",1],["Graph - visual representation",1]],[["Database",2],["Online transaction processing",2],["Requirement",2],["Stream processing",2],["Aurora",1],["Illustra",1],["Inbound marketing",1],["Outbound laptop",1],["Real-time computing",1],["Time series",1],["Transaction processing",1],["Benchmark (computing)",1],["FITS",1],["H-Store",1],["IBM Tivoli Storage Productivity Center",1],["Legacy code",1],["Relational database management system",1],["Rewrite (programming)",1]]],[[],[["Parsing",4],["Treebank",4],["Automatic Transmitter Identification System (television)",2],["Distributional semantics",2],["Grammar induction",2],["Natural language",2],["Part-of-speech tagging",2],["Phrases",2],["Sentiment analysis",2],["Trees (plant)",2],["sentence",2],["Robotics",2],["Context-free grammar",1],["Context-free language",1],["Pattern recognition",1],["Statistical model",1],["Unsupervised learning",1],["Recursion",1],["Deep learning",1],["Natural language processing",1]],[["Algorithm",13],["Autonomous car",7],["Parsing",5],["Stochastic context-free grammar",5],["Chart parser",4],["Table (information)",4],["Reinforcement learning",3],["Context-free grammar",3],["Context-free language",3],["Correctness (computer science)",3],["License",3],["Parser",3],["algorithm",3],["Robotics",2],["Apprenticeship learning",2],["Instability",2],["In-place algorithm",2],["Differential dynamic programming",2],["Dynamic programming",2],["Remote control",1],["State space",1],["Control system",1],["Experience",1],["Extreme programming",1],["Nonlinear system",1],["Bundle adjustment",1],["Camera resectioning",1],["Experiment",1],["Global Positioning System",1],["Positioning system",1],["Structure from motion",1],["Dijkstra's algorithm",1],["Natural deduction",1],["Dynamical system",1],["Simulation",1],["Spatial database",1],["Stationary process",1],["Ada",1],["Heuristic",1],["Microsoft Windows",1],["NP-completeness",1],["Network traffic control",1],["Outbound laptop",1],["Scheduling (computing)",1],["Systems architecture",1],["Legal expert system",1],["Machine learning",1],["A* search algorithm",1],["Best, worst and average case",1],["Cubic function",1],["Precomputation",1],["Speedup",1],["Time complexity",1],["Treebank",1],["Viterbi algorithm",1],["Loss function",1],["Optimal control",1]],[["Amazon Mechanical Turk",2],["Crowdsourcing",2],["Requirement",2],["The Turk",2]],[]]],"size":{"minx":-1.2035683809,"maxx":1.2640926411,"miny":-1.2779606838,"maxy":1.4013600653,"width":2.467661022,"height":2.6793207491,"node_weights":{"min":1,"max":162,"range":161},"edge_weights":{"min":1,"max":68,"range":67},"word_grid":{"cols":5,"rows":5,"cell_width":0.4935322044,"cell_height":0.5358641498}}}